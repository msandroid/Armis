# Qwen2.5-Omni 3B Integration

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Armisãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«Qwen2.5-Omni-3B-GGUFãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIæ¨è«–ã¨éŸ³å£°ç”Ÿæˆã‚’è¡Œã†æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## æ¦‚è¦

Qwen2.5-Omni 3Bã¯ã€AlibabaãŒé–‹ç™ºã—ãŸã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ã‚’ç†è§£ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¨è‡ªç„¶ãªéŸ³å£°ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚7Bãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦è»½é‡ã§é«˜é€Ÿãªå‡¦ç†ãŒå¯èƒ½ã§ã™ã€‚

### ä¸»è¦ç‰¹å¾´

- ğŸ¯ **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ**: ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ã®çµ±åˆç†è§£
- ğŸ¤ **éŸ³å£°ç”Ÿæˆ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å‡ºåŠ›ï¼ˆChelsie/Ethanï¼‰
- ğŸ—ï¸ **é©æ–°çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Thinker-Talkerè¨­è¨ˆ
- âš¡ **é«˜é€Ÿå‡¦ç†**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
- ğŸ”’ **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼**: ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–
- ğŸš€ **è»½é‡**: 3.4Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜é€Ÿå‹•ä½œ

## å‰ææ¡ä»¶

### 1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```bash
npm install node-llama-cpp@3
```

### 2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«

Qwen2.5-Omni 3B GGUFãƒ¢ãƒ‡ãƒ«ã‚’`./models/unsloth/Qwen2.5-Omni-3B-GGUF/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

æ¨å¥¨é‡å­åŒ–ãƒ¬ãƒ™ãƒ«:
- `Qwen2.5-Omni-3B-Q4_K_M.gguf` (æ¨å¥¨) - ãƒãƒ©ãƒ³ã‚¹å‹
- `Qwen2.5-Omni-3B-Q5_K_M.gguf` - é«˜å“è³ª
- `Qwen2.5-Omni-3B-Q8_0.gguf` - æœ€é«˜å“è³ª

### 3. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

- **ãƒ¡ãƒ¢ãƒª**: 4GBä»¥ä¸Šæ¨å¥¨
- **CPU**: ãƒãƒ«ãƒã‚³ã‚¢CPUæ¨å¥¨
- **GPU**: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆCUDA/OpenCLå¯¾å¿œï¼‰
- **OS**: Windows, macOS, Linux

## åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

### 1. ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–

```typescript
import { Qwen2_5OmniService } from '@/services/llm/qwen2.5-omni-service'

const qwenOmniService = new Qwen2_5OmniService({
  modelPath: './models/unsloth/Qwen2.5-Omni-3B-GGUF/Qwen2.5-Omni-3B-Q4_K_M.gguf',
  temperature: 0.7,
  maxTokens: 2048,
  contextSize: 32768,
  threads: 4,
  gpuLayers: 0,
  enableAudioOutput: false,
  speaker: 'Chelsie',
  useAudioInVideo: true
})

await qwenOmniService.initialize()
```

### 2. åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

```typescript
const response = await qwenOmniService.generateResponse(
  "Hello! I am Qwen2.5-Omni 3B. How can I help you today?"
)

console.log('Response:', response.text)
console.log('Duration:', response.duration, 'ms')
console.log('Tokens:', response.tokens)
```

### 3. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ä¼šè©±

```typescript
const conversation = [
  {
    role: 'system',
    content: [
      {
        type: 'text',
        text: 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'
      }
    ]
  },
  {
    role: 'user',
    content: [
      {
        type: 'text',
        text: 'What can you see in this image?'
      },
      {
        type: 'image',
        image: '/path/to/image.jpg'
      }
    ]
  }
]

const response = await qwenOmniService.generateMultimodalResponse(conversation)
console.log('Multimodal response:', response.text)
```

### 4. éŸ³å£°ç”Ÿæˆï¼ˆå°†æ¥å®Ÿè£…äºˆå®šï¼‰

```typescript
// éŸ³å£°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–
await qwenOmniService.enableAudioOutput()

// ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚’è¨­å®š
qwenOmniService.setSpeaker('Ethan')

// éŸ³å£°ä»˜ãã§å¿œç­”ç”Ÿæˆ
const response = await qwenOmniService.generateResponse(
  "Hello! This is a test of audio generation.",
  { returnAudio: true, speaker: 'Chelsie' }
)

if (response.audio) {
  // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
  console.log('Audio generated:', response.audio.byteLength, 'bytes')
}
```

## è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

### ãƒ¢ãƒ‡ãƒ«è¨­å®š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----------|------|
| `modelPath` | `./models/unsloth/Qwen2.5-Omni-3B-GGUF/Qwen2.5-Omni-3B-Q4_K_M.gguf` | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| `temperature` | 0.7 | å‰µé€ æ€§ã®èª¿æ•´ï¼ˆ0.0-2.0ï¼‰ |
| `maxTokens` | 2048 | æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•° |
| `contextSize` | 32768 | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º |
| `threads` | -1 | CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆ-1ã§è‡ªå‹•ï¼‰ |
| `gpuLayers` | 99 | GPUä½¿ç”¨ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° |

### éŸ³å£°è¨­å®š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----------|------|
| `enableAudioOutput` | false | éŸ³å£°å‡ºåŠ›ã®æœ‰åŠ¹åŒ– |
| `speaker` | 'Chelsie' | éŸ³å£°ã‚¿ã‚¤ãƒ—ï¼ˆ'Chelsie'/'Ethan'ï¼‰ |
| `useAudioInVideo` | true | å‹•ç”»å†…éŸ³å£°ã®ä½¿ç”¨ |

## éŸ³å£°ã‚¿ã‚¤ãƒ—

### Chelsie (å¥³æ€§)
- æ¸©ã‹ã¿ã®ã‚ã‚‹æ˜ç­ãªå£°
- å„ªã—ãè¦ªã—ã¿ã‚„ã™ã„ãƒˆãƒ¼ãƒ³
- ä¸€èˆ¬çš„ãªç”¨é€”ã«æœ€é©

### Ethan (ç”·æ€§)
- æ˜ã‚‹ãè¦ªã—ã¿ã‚„ã™ã„å£°
- ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ã§é­…åŠ›çš„
- ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªä¼šè©±ã«æœ€é©

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

| é‡å­åŒ–ãƒ¬ãƒ™ãƒ« | ã‚µã‚¤ã‚º | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |
|-------------|--------|-------------|
| Q4_K_M | 2.1 GB | ~3 GB |
| Q5_K_M | 2.44 GB | ~4 GB |
| Q8_0 | 3.62 GB | ~5 GB |

### æ¨å¥¨è¨­å®š

#### è»½é‡ç’°å¢ƒï¼ˆ4GB RAMï¼‰
```typescript
{
  threads: 2,
  gpuLayers: 0,
  contextSize: 16384
}
```

#### æ¨™æº–ç’°å¢ƒï¼ˆ8GB RAMï¼‰
```typescript
{
  threads: 4,
  gpuLayers: 10,
  contextSize: 32768
}
```

#### é«˜æ€§èƒ½ç’°å¢ƒï¼ˆ16GB+ RAMï¼‰
```typescript
{
  threads: 8,
  gpuLayers: 99,
  contextSize: 65536
}
```

## 7Bãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ

| ç‰¹å¾´ | 3Bãƒ¢ãƒ‡ãƒ« | 7Bãƒ¢ãƒ‡ãƒ« |
|------|----------|----------|
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | 3.4B | 7.62B |
| ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º | 2.1-3.6 GB | 4.7-8.1 GB |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 3-5 GB | 6-10 GB |
| å‡¦ç†é€Ÿåº¦ | é«˜é€Ÿ | ä¸­é€Ÿ |
| å“è³ª | è‰¯å¥½ | é«˜å“è³ª |
| æ¨å¥¨ç”¨é€” | è»½é‡ç’°å¢ƒ | æ¨™æº–ç’°å¢ƒ |

## ä½¿ç”¨ä¾‹

### 1. ç”»åƒåˆ†æ

```typescript
const imageAnalysisConversation = [
  {
    role: 'system',
    content: 'You are Qwen2.5-Omni 3B, an AI assistant with visual understanding capabilities.'
  },
  {
    role: 'user',
    content: [
      { type: 'text', text: 'Describe what you see in this image.' },
      { type: 'image', image: '/path/to/image.jpg' }
    ]
  }
]

const analysis = await qwenOmniService.generateMultimodalResponse(imageAnalysisConversation)
```

### 2. éŸ³å£°ç†è§£

```typescript
const audioUnderstandingConversation = [
  {
    role: 'system',
    content: 'You are Qwen2.5-Omni 3B, capable of understanding audio content.'
  },
  {
    role: 'user',
    content: [
      { type: 'text', text: 'What did you hear in this audio?' },
      { type: 'audio', audio: '/path/to/audio.wav' }
    ]
  }
]

const understanding = await qwenOmniService.generateMultimodalResponse(audioUnderstandingConversation)
```

### 3. å‹•ç”»åˆ†æ

```typescript
const videoAnalysisConversation = [
  {
    role: 'system',
    content: 'You are Qwen2.5-Omni 3B, capable of analyzing video content with audio.'
  },
  {
    role: 'user',
    content: [
      { type: 'text', text: 'What is happening in this video?' },
      { type: 'video', video: '/path/to/video.mp4' }
    ]
  }
]

const analysis = await qwenOmniService.generateMultimodalResponse(videoAnalysisConversation)
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„
```
Error: Qwen2.5-Omni 3B model file not found
```
**è§£æ±ºæ–¹æ³•**: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ã„ãƒ‘ã‚¹ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

#### 2. ãƒ¡ãƒ¢ãƒªä¸è¶³
```
Error: Out of memory
```
**è§£æ±ºæ–¹æ³•**: ã‚ˆã‚Šè»½é‡ãªé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ¡ãƒ¢ãƒªã‚’å¢—è¨­ã—ã¦ãã ã•ã„ã€‚

#### 3. éŸ³å£°ç”ŸæˆãŒå‹•ä½œã—ãªã„
```
Warning: Audio generation is not yet implemented
```
**è§£æ±ºæ–¹æ³•**: éŸ³å£°ç”Ÿæˆæ©Ÿèƒ½ã¯å°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å®Ÿè£…äºˆå®šã§ã™ã€‚

### ãƒ‡ãƒãƒƒã‚°

```typescript
// è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
const qwenOmniService = new Qwen2_5OmniService({
  verbose: true,
  // ... ãã®ä»–ã®è¨­å®š
})

// è¨­å®šã‚’ç¢ºèª
const config = qwenOmniService.getConfig()
console.log('Current config:', config)

// æº–å‚™çŠ¶æ…‹ã‚’ç¢ºèª
console.log('Service ready:', qwenOmniService.isReady())
```

## ä»Šå¾Œã®å®Ÿè£…äºˆå®š

- [ ] å®Œå…¨ãªéŸ³å£°ç”Ÿæˆæ©Ÿèƒ½
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- [ ] ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®å®Œå…¨ã‚µãƒãƒ¼ãƒˆ
- [ ] éŸ³å£°å“è³ªã®æœ€é©åŒ–
- [ ] ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½

## å‚è€ƒãƒªãƒ³ã‚¯

- [Qwen2.5-Omni 3B Hugging Face](https://huggingface.co/unsloth/Qwen2.5-Omni-3B-GGUF)
- [Qwen2.5-Omni è«–æ–‡](https://arxiv.org/abs/2503.20215)
- [node-llama-cpp ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/withcatai/node-llama-cpp)
