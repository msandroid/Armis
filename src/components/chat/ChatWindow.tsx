import React, { useState, useEffect, useRef } from 'react'
import { ScrollArea } from '@/components/ui/scroll-area'
import { Button } from '@/components/ui/button'
import { ChatMessage, ChatMessageProps } from './ChatMessage'
import { PromptInputBox } from './PromptInputBox'

import { EnhancedFilePreview } from './EnhancedFilePreview'
import { ArmisSettings } from '@/components/settings/ArmisSettings'
import { createTTSManager } from '@/services/tts'
import { AISDKService } from '@/services/llm/ai-sdk-service'
import { GeminiFileService } from '@/services/llm/gemini-file-service'
import { AIProviderConfig, ModelSettings, AVAILABLE_PROVIDERS } from '@/types/ai-sdk'
import { cn } from '@/lib/utils'
import { FactCheckingService, FactCheckResult } from '@/services/llm/fact-checking-service'
import { 
  Settings, 
  Brain, 
  X, 
  Paperclip,
  FileText,
  Image as ImageIcon,
  Video,
  Music,
  File,
  AlertCircle,
  Wrench,
  Bot,
  Zap,
  Terminal as TerminalIcon
} from 'lucide-react'
import { motion, AnimatePresence } from 'framer-motion'
import { JumpingDots } from '@/components/ui/jumping-dots'
import { CircleSpinner } from '@/components/ui/circle-spinner'
import { ShimmerText } from '@/components/ui/shimmer-text'
import { SequentialThinkingPanel } from './SequentialThinkingPanel'
import { SequentialThinkingPlan } from '@/types/llm'
import { LLMManager } from '@/services/llm/llm-manager'
import { InputAnalyzer, InputAnalysis } from '@/services/agent/input-analyzer'
import { AgentType } from '@/types/llm'
import { AgentInfoDisplay } from './AgentInfoDisplay'
import { browserWebSearchService, WebSearchResult } from '@/services/web-search/browser-web-search'
import { webSearchManager } from '@/services/web-search/web-search-manager'
import { EmbeddedCLI } from './EmbeddedCLI'
import { TaskShimmerDisplay } from '@/components/ui/task-shimmer-display'
import { useTaskExecution } from '@/hooks/useTaskExecution'
import { FileCreationLoader } from '@/components/ui/file-creation-loader'
import { useFileCreation } from '@/hooks/useFileCreation'
import { ModelDownloadShimmer } from '@/components/ui/model-download-shimmer'
import { GeminiImageService } from '@/services/llm/gemini-image-service'
import { checkGoogleAIConfig, logConfigStatus } from '@/utils/env-checker'
import { VeoService } from '@/services/video/veo-service'

import { useTheme } from '@/components/theme-provider'
import { STTSettingsService } from '@/services/stt/stt-settings-service'

interface ChatWindowProps {
  className?: string
  useEnhancedPreview?: boolean // æ‹¡å¼µãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
}

// AI Elements helper functions
const createAIElementsMessage = (
  content: string,
  sources?: Array<{ id: string; href: string; title?: string; description?: string }>,
  tasks?: Array<{
    id: string;
    title: string;
    status: 'pending' | 'in_progress' | 'completed' | 'error';
    items?: Array<{
      id: string;
      text: string;
      status: 'pending' | 'in_progress' | 'completed' | 'error';
      file?: { name: string; icon?: React.ReactNode };
    }>;
  }>,
  citations?: Array<{
    id: string;
    text: string;
    sources: Array<{
      id: string;
      url: string;
      title?: string;
      description?: string;
      quote?: string;
    }>;
  }>,
  tools?: Array<{
    id: string;
    type: string;
    state: 'input-streaming' | 'input-available' | 'output-available' | 'output-error';
    input: any;
    output?: any;
    errorText?: string;
  }>,
  webPreviews?: Array<{
    id: string;
    url: string;
    title?: string;
    consoleLogs?: Array<{ level: 'log' | 'warn' | 'error'; message: string; timestamp: Date }>;
  }>
): ChatMessageProps => ({
  id: Date.now().toString(),
  content,
  role: 'assistant',
  useAIElements: true,
  sources,
  tasks,
  citations,
  tools,
  webPreviews,
  isStreaming: false
})

// Sample AI Elements messages for demonstration
const sampleAIElementsMessages: ChatMessageProps[] = [
  createAIElementsMessage(
    `# AI SDK Response Component

This is a **markdown** response that demonstrates various features:

## Features

- **Bold text** and *italic text*
- \`inline code\` and code blocks
- Lists and numbered lists
- Links and images
- Math equations
- Tables

### Code Example

\`\`\`typescript
function greet(name: string): string {
  return \`Hello, \${name}!\`;
}

console.log(greet('World'));
\`\`\`

### Math Equation

Inline math: $E = mc^2$

Block math:
$$
\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}
$$

### Table Example

| Feature | Support | Description |
|---------|---------|-------------|
| Markdown | âœ… | Full markdown support |
| Code Highlighting | âœ… | Syntax highlighting |
| Math Equations | âœ… | KaTeX integration |
| Copy Button | âœ… | One-click code copying |

### Task List

- [x] Implement basic markdown rendering
- [x] Add code syntax highlighting
- [x] Integrate math equation support
- [ ] Add custom components support
- [ ] Implement streaming support

Visit [AI SDK Documentation](https://ai-sdk.dev) for more information.

> This is a blockquote that demonstrates the styling of quoted text.`,
    [
      {
        id: '1',
        href: 'https://ai-sdk.dev/elements/components/response',
        title: 'AI SDK Response Component',
        description: 'Official documentation for the Response component'
      },
      {
        id: '2',
        href: 'https://vercel.com/ai',
        title: 'Vercel AI',
        description: 'Build AI applications with Vercel'
      },
      {
        id: '3',
        href: 'https://radix-ui.com/primitives/docs/components/collapsible',
        title: 'Radix UI Collapsible',
        description: 'Unstyled, accessible collapsible component'
      }
    ],
    [
      {
        id: '1',
        title: 'Project Setup',
        status: 'completed',
        items: [
          { id: '1-1', text: 'Initialize project with npm', status: 'completed' },
          { id: '1-2', text: 'Install dependencies', status: 'completed' },
          { id: '1-3', text: 'Configure build tools', status: 'completed' }
        ]
      },
      {
        id: '2',
        title: 'Component Development',
        status: 'in_progress',
        items: [
          { id: '2-1', text: 'Create component structure', status: 'completed' },
          { id: '2-2', text: 'Implement core functionality', status: 'in_progress' },
          { id: '2-3', text: 'Add styling and animations', status: 'pending' },
          { id: '2-4', text: 'Write unit tests', status: 'pending' }
        ]
      },
      {
        id: '3',
        title: 'Documentation',
        status: 'pending',
        items: [
          { id: '3-1', text: 'Write API documentation', status: 'pending' },
          { id: '3-2', text: 'Create usage examples', status: 'pending' }
        ]
      }
    ]
  )
]

export const ChatWindow: React.FC<ChatWindowProps> = ({ 
  className,
  useEnhancedPreview = true // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ‹¡å¼µãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨
}) => {
  const [messages, setMessages] = useState<ChatMessageProps[]>([])
  const [streamingResponse, setStreamingResponse] = useState('')
  const [isTyping, setIsTyping] = useState(false)
  const [isGenerating, setIsGenerating] = useState(false)
  const [selectedFiles, setSelectedFiles] = useState<File[]>([])
  const [showSettings, setShowSettings] = useState(false)
  const [showSequentialThinking, setShowSequentialThinking] = useState(false)
  const [sequentialThinkingPlan, setSequentialThinkingPlan] = useState<SequentialThinkingPlan | null>(null)
  const [showAgentInfo, setShowAgentInfo] = useState(false)
  const [agentType, setAgentType] = useState<AgentType>('general')
  const [showEmbeddedCLI, setShowEmbeddedCLI] = useState(false)
  const [isWebSearching, setIsWebSearching] = useState(false)
  const [webSearchResults, setWebSearchResults] = useState<WebSearchResult[]>([])
  const [isVideoGenerating, setIsVideoGenerating] = useState(false)
  const [videoGenerationProgress, setVideoGenerationProgress] = useState(0)
  const [isImageGenerating, setIsImageGenerating] = useState(false)
  const [imageGenerationProgress, setImageGenerationProgress] = useState(0)
  const [isAudioGenerating, setIsAudioGenerating] = useState(false)
  const [audioGenerationProgress, setAudioGenerationProgress] = useState(0)
  const [isSTTTranscribing, setIsSTTTranscribing] = useState(false)
  const [sttFileName, setSttFileName] = useState<string>('')
  const [isDragging, setIsDragging] = useState(false)
  
  // Theme management
  const { theme, setTheme } = useTheme()
  
  // isGeneratingã®çŠ¶æ…‹å¤‰æ›´ã‚’ãƒ­ã‚°å‡ºåŠ›
  useEffect(() => {
    console.log('ğŸ”„ ChatWindow isGenerating changed:', isGenerating)
  }, [isGenerating])
  const [isTTSProcessing, setIsTTSProcessing] = useState(false) // TTSå‡¦ç†ä¸­çŠ¶æ…‹
  
  // ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹
  const [modelDownloadState, setModelDownloadState] = useState<{
    isDownloading: boolean;
    modelName: string;
    progress: number;
    status: 'starting' | 'downloading' | 'verifying' | 'completed' | 'error';
    downloadedBytes?: number;
    totalBytes?: number;
  }>({
    isDownloading: false,
    modelName: '',
    progress: 0,
    status: 'starting',
    downloadedBytes: undefined,
    totalBytes: undefined
  })

  // ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã®ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
  useEffect(() => {
    console.log('ğŸ”„ modelDownloadState changed:', modelDownloadState)
  }, [modelDownloadState])

  const [currentPlan, setCurrentPlan] = useState<SequentialThinkingPlan | null>(null)
  const [currentProviderConfig, setCurrentProviderConfig] = useState<AIProviderConfig | null>(null)
  const [webSearchEnabled, setWebSearchEnabled] = useState(false)
  
  // Fact checking settings state
  const [factCheckingSettings, setFactCheckingSettings] = useState(() => {
    try {
      const saved = localStorage.getItem('armis_fact_checking_settings')
      return saved ? JSON.parse(saved) : {
        enabled: false,
        model: 'ollama',
        temperature: 0.1,
        maxTokens: 1000,
        includeSources: true,
        strictMode: false,
        autoCheck: false,
        checkThreshold: 0.7
      }
    } catch (error) {
      console.error('Failed to load fact checking settings from localStorage:', error)
      return {
        enabled: false,
        model: 'ollama',
        temperature: 0.1,
        maxTokens: 1000,
        includeSources: true,
        strictMode: false,
        autoCheck: false,
        checkThreshold: 0.7
      }
    }
  })

  // Generation settings state
  const [generationSettings, setGenerationSettings] = useState(() => {
    // ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_generation_settings')
      return saved ? JSON.parse(saved) : {
        image: {
          enabled: false, // ç”»åƒç”Ÿæˆæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–
          provider: 'google',
          models: {
            'gemini-2.0-flash-preview-image-generation': { enabled: false, priority: 1 },
            'imagen-3.0-generate-002': { enabled: false, priority: 2 },
            'imagen-3.0-generate-001': { enabled: false, priority: 3 },
            'imagen-3.0-fast-generate-001': { enabled: false, priority: 4 },
            'imagen-4.0-generate-001': { enabled: false, priority: 5 },
            'imagen-4.0-fast-generate-001': { enabled: false, priority: 6 },
            'imagen-4.0-ultra-generate-001': { enabled: false, priority: 7 }
          }
        },
        audio: {
          enabled: false,
          provider: 'google',
          models: {
            'gemini-2.5-flash-preview-tts': { enabled: false, priority: 1 },
            'gemini-2.5-pro-preview-tts': { enabled: false, priority: 2 }
          },
          tts: {
            defaultVoice: 'Kore',
            defaultLanguage: 'ja-JP',
            defaultStyle: '',
            apiKey: ''
          }
        },
        video: {
          enabled: false,
          provider: 'openai',
          models: {}
        },
        code: {
          enabled: false,
          provider: 'openai',
          models: {}
        }
      }
    } catch (error) {
      console.error('Failed to load generation settings from localStorage:', error)
      return {
        image: {
          enabled: false, // ç”»åƒç”Ÿæˆæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–
          provider: 'google',
          models: {
            'gemini-2.0-flash-preview-image-generation': { enabled: false, priority: 1 },
            'imagen-3.0-generate-002': { enabled: false, priority: 2 },
            'imagen-3.0-generate-001': { enabled: false, priority: 3 },
            'imagen-3.0-fast-generate-001': { enabled: false, priority: 4 },
            'imagen-4.0-generate-001': { enabled: false, priority: 5 },
            'imagen-4.0-fast-generate-001': { enabled: false, priority: 6 },
            'imagen-4.0-ultra-generate-001': { enabled: false, priority: 7 }
          }
        },
        audio: {
          enabled: false,
          provider: 'google',
          models: {
            'gemini-2.5-flash-preview-tts': { enabled: false, priority: 1 },
            'gemini-2.5-pro-preview-tts': { enabled: false, priority: 2 }
          },
          tts: {
            defaultVoice: 'Kore',
            defaultLanguage: 'ja-JP',
            defaultStyle: '',
            apiKey: ''
          }
        },
        video: {
          enabled: false,
          provider: 'openai',
          models: {}
        },
        code: {
          enabled: false,
          provider: 'openai',
          models: {}
        }
      }
    }
  })
  
  const [createImageEnabled, setCreateImageEnabled] = useState(false)
  const [audioEnabled, setAudioEnabled] = useState(false)
  const [videoEnabled, setVideoEnabled] = useState(false)
  const [isGeneratingVideo, setIsGeneratingVideo] = useState(false)
  const [videoProgress, setVideoProgress] = useState(0)
  const [currentTTSModel, setCurrentTTSModel] = useState<string>(() => {
    // ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰TTSãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_tts_model')
      return saved || 'auto'
    } catch (error) {
      console.error('Failed to load TTS model from localStorage:', error)
      return 'auto'
    }
  })

  const [currentSTTModel, setCurrentSTTModel] = useState<string>(() => {
    // ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰STTãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_stt_model')
      return saved || 'auto'
    } catch (error) {
      console.error('Failed to load STT model from localStorage:', error)
      return 'auto'
    }
  })

  // TTSãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¨­å®šç”»é¢ã®Audioè¨­å®šã¨åŒæœŸ
  useEffect(() => {
    if (generationSettings.audio.enabled && generationSettings.audio.models) {
      // æœ‰åŠ¹ãªTTSãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
      const enabledTTSModels = Object.entries(generationSettings.audio.models)
        .filter(([_, config]: [string, any]) => config.enabled)
        .sort(([_, a]: [string, any], [__, b]: [string, any]) => (a.priority || 999) - (b.priority || 999))
      
      // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹TTSãƒ¢ãƒ‡ãƒ«ãŒæœ‰åŠ¹ã§ãªã„å ´åˆã€æœ€é«˜å„ªå…ˆåº¦ã®ãƒ¢ãƒ‡ãƒ«ã«è¨­å®š
      if (currentTTSModel !== 'auto' && !enabledTTSModels.find(([model, _]) => model === currentTTSModel)) {
        const defaultModel = enabledTTSModels.length > 0 ? enabledTTSModels[0][0] : 'auto'
        setCurrentTTSModel(defaultModel)
        localStorage.setItem('armis_tts_model', defaultModel)
        console.log('TTS model synchronized with settings:', defaultModel)
      }
    }
  }, [generationSettings.audio.enabled, generationSettings.audio.models, currentTTSModel])

  // STTè¨­å®šã‚µãƒ¼ãƒ“ã‚¹
  const [sttSettingsService] = useState(() => STTSettingsService.getInstance())
  
  // Create Imageãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’Generationè¨­å®šã¨åŒæœŸ
  useEffect(() => {
    // åˆæœŸåŒ–æ™‚ã¯å¿…ãšfalseã«è¨­å®š
    if (generationSettings.image.enabled === false) {
      setCreateImageEnabled(false)
      console.log('Create Image enabled: false (initialized)')
    } else {
      setCreateImageEnabled(generationSettings.image.enabled)
      console.log('Create Image enabled:', generationSettings.image.enabled)
    }
  }, [generationSettings.image.enabled])

  // Audioãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’Generationè¨­å®šã¨åŒæœŸ
  useEffect(() => {
    // åˆæœŸåŒ–æ™‚ã¯å¿…ãšfalseã«è¨­å®š
    if (generationSettings.audio.enabled === false) {
      setAudioEnabled(false)
      console.log('Audio enabled: false (initialized)')
    } else {
      setAudioEnabled(generationSettings.audio.enabled)
      console.log('Audio enabled:', generationSettings.audio.enabled)
    }
  }, [generationSettings.audio.enabled])

  // Videoãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’Generationè¨­å®šã¨åŒæœŸ
  useEffect(() => {
    // åˆæœŸåŒ–æ™‚ã¯å¿…ãšfalseã«è¨­å®š
    if (generationSettings.video.enabled === false) {
      setVideoEnabled(false)
      console.log('Video enabled: false (initialized)')
    } else {
      setVideoEnabled(generationSettings.video.enabled)
      console.log('Video enabled:', generationSettings.video.enabled)
    }
  }, [generationSettings.video.enabled])

  // åˆæœŸåŒ–æ™‚ã«CreateImageã€Audioã€Videoã‚’ç¢ºå®Ÿã«falseã«è¨­å®š
  useEffect(() => {
    setCreateImageEnabled(false)
    setAudioEnabled(false)
    setVideoEnabled(false)
    console.log('Initialized CreateImage, Audio, and Video to false')
  }, [])

  // Generationè¨­å®šã®æ°¸ç¶šåŒ–
  useEffect(() => {
    localStorage.setItem('armis_generation_settings', JSON.stringify(generationSettings))
    console.log('ğŸ›ï¸ Generation settings updated:', {
      audioProvider: generationSettings.audio.provider,
      audioEnabled: generationSettings.audio.enabled,
      audioModels: generationSettings.audio.models,
      videoProvider: generationSettings.video.provider,
      videoEnabled: generationSettings.video.enabled
    })
  }, [generationSettings])

  // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¨­å®šã®æ°¸ç¶šåŒ–
  useEffect(() => {
    localStorage.setItem('armis_fact_checking_settings', JSON.stringify(factCheckingSettings))
    console.log('ğŸ” Fact checking settings updated:', {
      enabled: factCheckingSettings.enabled,
      model: factCheckingSettings.model,
      autoCheck: factCheckingSettings.autoCheck
    })
  }, [factCheckingSettings])

  // åˆå›èµ·å‹•æ™‚ã«localStorageã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ã—ã„è¨­å®šã§åˆæœŸåŒ–
  useEffect(() => {
    // å¼·åˆ¶çš„ã«localStorageã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ã—ã„è¨­å®šã§åˆæœŸåŒ–
    localStorage.removeItem('armis_generation_settings')
    localStorage.setItem('armis_first_run', 'true')
    console.log('Generation settings reset to default (false)')
    
    // å¼·åˆ¶çš„ã«è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆé–‹ç™ºç”¨ï¼‰
    const forceReset = localStorage.getItem('armis_force_reset_generation')
    if (forceReset === 'true') {
      localStorage.removeItem('armis_generation_settings')
      localStorage.removeItem('armis_force_reset_generation')
      console.log('Force reset generation settings')
    }
  }, [])
  
  // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ç®¡ç†
  const [loadingState, setLoadingState] = useState<'idle' | 'text' | 'media'>('idle')
  const [modelSettings, setModelSettings] = useState<ModelSettings>(() => {
    // localStorageã‹ã‚‰ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_model_settings')
      return saved ? JSON.parse(saved) : {
        enabledModels: [
          { providerId: 'openai', modelId: 'gpt-4o', enabled: true, priority: 1 },
          { providerId: 'anthropic', modelId: 'claude-opus-4.1', enabled: true, priority: 3 },
          { providerId: 'google', modelId: 'gemini-2.5-flash-lite', enabled: true, priority: 4 },
          { providerId: 'ollama', modelId: 'gemma3:1b', enabled: true, priority: 2 }
        ],
        defaultModel: 'ollama:gemma3:1b',
        autoSwitch: true
      }
    } catch (error) {
      console.error('Failed to load model settings from localStorage:', error)
      return {
        enabledModels: [
          { providerId: 'openai', modelId: 'gpt-4o', enabled: true, priority: 1 },
          { providerId: 'anthropic', modelId: 'claude-opus-4.1', enabled: true, priority: 3 },
          { providerId: 'google', modelId: 'gemini-2.5-flash-lite', enabled: true, priority: 4 },
          { providerId: 'ollama', modelId: 'gemma3:1b', enabled: true, priority: 2 }
        ],
        defaultModel: 'ollama:gemma3:1b',
        autoSwitch: true
      }
    }
  })
  
  // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã®API Keyç®¡ç†
  const [providerApiKeys, setProviderApiKeys] = useState<Record<string, string>>(() => {
    // localStorageã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_provider_api_keys')
      const apiKeys = saved ? JSON.parse(saved) : {}
      
      // Web Search Managerã«APIã‚­ãƒ¼ã‚’è¨­å®š
      webSearchManager.setProviderApiKeys(apiKeys)
      
      return apiKeys
    } catch (error) {
      console.error('Failed to load API keys from localStorage:', error)
      return {}
    }
  })
  const [aiSDKService] = useState(() => new AISDKService())
  const [geminiFileService] = useState(() => new GeminiFileService())

  // Router Agent Systemé–¢é€£ã®çŠ¶æ…‹
  const [llmManager, setLlmManager] = useState<LLMManager | null>(null)
  const [inputAnalyzer] = useState(() => new InputAnalyzer())
  const [currentAnalysis, setCurrentAnalysis] = useState<InputAnalysis | null>(null)
  const [agentInfo, setAgentInfo] = useState<{
    type: AgentType | null
    confidence: number
    reasoning: string
  } | null>(null)

  // ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£ã®çŠ¶æ…‹
  const [downloadProgress, setDownloadProgress] = useState<any>(null)
  const [isDownloading, setIsDownloading] = useState(false)

  // CLIé–¢é€£ã®çŠ¶æ…‹
  const [showCLI, setShowCLI] = useState(false)
  const [isCLIMinimized, setIsCLIMinimized] = useState(false)
  
  // CLIãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã®çŠ¶æ…‹
  const [cliFontSettings, setCliFontSettings] = useState(() => {
    // localStorageã‹ã‚‰ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿
    try {
      const saved = localStorage.getItem('armis_cli_font_settings')
      return saved ? JSON.parse(saved) : {
        fontFamily: 'Cascadia Code',
        fontSize: 12,
        fontLigatures: true
      }
    } catch (error) {
      console.error('Failed to load CLI font settings from localStorage:', error)
      return {
        fontFamily: 'Cascadia Code',
        fontSize: 12,
        fontLigatures: true
      }
    }
  })

  // APIã‚­ãƒ¼ã‚’localStorageã«ä¿å­˜ã™ã‚‹é–¢æ•°
  const saveApiKeysToStorage = (apiKeys: Record<string, string>) => {
    try {
      localStorage.setItem('armis_provider_api_keys', JSON.stringify(apiKeys))
    } catch (error) {
      console.error('Failed to save API keys to localStorage:', error)
    }
  }

  // CLIãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’localStorageã«ä¿å­˜ã™ã‚‹é–¢æ•°
  const saveCliFontSettingsToStorage = (settings: { fontFamily: string; fontSize: number; fontLigatures: boolean }) => {
    try {
      localStorage.setItem('armis_cli_font_settings', JSON.stringify(settings))
    } catch (error) {
      console.error('Failed to save CLI font settings to localStorage:', error)
    }
  }

  // ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’localStorageã«ä¿å­˜ã™ã‚‹é–¢æ•°
  const saveModelSettingsToStorage = (settings: ModelSettings) => {
    try {
      localStorage.setItem('armis_model_settings', JSON.stringify(settings))
    } catch (error) {
      console.error('Failed to save model settings to localStorage:', error)
    }
  }

  // APIã‚­ãƒ¼æ›´æ–°æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleProviderApiKeysChange = (newApiKeys: Record<string, string>) => {
    setProviderApiKeys(newApiKeys)
    saveApiKeysToStorage(newApiKeys)
    
    // Web Search Managerã«APIã‚­ãƒ¼ã‚’è¨­å®š
    webSearchManager.setProviderApiKeys(newApiKeys)
  }

  // CLIãƒ•ã‚©ãƒ³ãƒˆè¨­å®šæ›´æ–°æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleCliFontSettingsChange = (settings: { fontFamily: string; fontSize: number; fontLigatures: boolean }) => {
    setCliFontSettings(settings)
    saveCliFontSettingsToStorage(settings)
  }

  // ãƒ¢ãƒ‡ãƒ«è¨­å®šæ›´æ–°æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleModelSettingsChange = (settings: ModelSettings) => {
    setModelSettings(settings)
    saveModelSettingsToStorage(settings)
  }

  // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¨­å®šã‚’localStorageã«ä¿å­˜ã™ã‚‹é–¢æ•°
  const saveFactCheckingSettingsToStorage = (settings: any) => {
    try {
      localStorage.setItem('armis_fact_checking_settings', JSON.stringify(settings))
    } catch (error) {
      console.error('Failed to save fact checking settings to localStorage:', error)
    }
  }

  // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¨­å®šæ›´æ–°æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleFactCheckingSettingsChange = (settings: any) => {
    setFactCheckingSettings(settings)
    saveFactCheckingSettingsToStorage(settings)
  }

  // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œé–¢æ•°
  const performFactCheck = async (text: string): Promise<FactCheckResult | null> => {
    if (!factCheckingSettings.enabled) {
      return null
    }

    try {
      // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
      let factCheckModel = factCheckingSettings.model as any
      let ollamaModel = 'gemma3:1b'
      let ollamaBaseUrl = 'http://localhost:11434'

      // ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ç¢ºèª
      if (currentSelectedModel) {
        if (currentSelectedModel.startsWith('ollama:')) {
          factCheckModel = 'ollama'
          ollamaModel = currentSelectedModel.replace('ollama:', '')
        } else if (currentSelectedModel.startsWith('llama-cpp:')) {
          factCheckModel = 'ollama'
          ollamaModel = currentSelectedModel.replace('llama-cpp:', '')
        }
      }

      const factChecker = new FactCheckingService({
        model: factCheckModel,
        temperature: factCheckingSettings.temperature,
        maxTokens: factCheckingSettings.maxTokens,
        includeSources: factCheckingSettings.includeSources,
        strictMode: factCheckingSettings.strictMode,
        ollamaModel: ollamaModel,
        ollamaBaseUrl: ollamaBaseUrl
      })

      const result = await factChecker.checkFacts(text)
      return result
    } catch (error) {
      console.error('Fact checking failed:', error)
      return null
    }
  }

  // ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºå®Ÿè¡Œé–¢æ•°
  const detectHallucinations = async (text: string): Promise<any | null> => {
    if (!factCheckingSettings.enabled) {
      return null
    }

    try {
      const factChecker = new FactCheckingService({
        model: factCheckingSettings.model as any,
        temperature: factCheckingSettings.temperature,
        maxTokens: factCheckingSettings.maxTokens,
        includeSources: factCheckingSettings.includeSources,
        strictMode: factCheckingSettings.strictMode
      })

      const result = await factChecker.detectHallucinations(text)
      return result
    } catch (error) {
      console.error('Hallucination detection failed:', error)
      return null
    }
  }
  const [lastError, setLastError] = useState<string | null>(null)

  // ã‚¿ã‚¹ã‚¯å®Ÿè¡Œç®¡ç†
  const {
    activeTasks,
    addTask,
    updateTask,
    removeTask,
    clearCompletedTasks,
    getTask,
    isTaskActive
  } = useTaskExecution()

  // ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆç®¡ç†
  const {
    activeFileCreations,
    addFileCreation,
    updateFileCreation,
    removeFileCreation,
    clearCompletedFileCreations,
    getFileCreation,
    isFileCreationActive
  } = useFileCreation()

  // LLM ManageråˆæœŸåŒ–
  useEffect(() => {
    const initializeLLMManager = async () => {
      try {
        // ç’°å¢ƒå¤‰æ•°ã®è¨­å®šçŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        logConfigStatus()
        
        // æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
        if (llmManager) {
          return
        }
        
        const config: any = { // LLMManagerã®å‹å®šç¾©ã‚’ä¿®æ­£
          useOllama: true,
          useLlamaCpp: false,
          defaultModel: 'gemma3:1b',
          vectorDBConfig: {
            type: 'in-memory'
          }
        }
        
        const manager = new LLMManager(config)
        
        // åˆæœŸåŒ–æ™‚ã«Ollamaã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã—ã€gemma3:1bã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try {
          await manager.initialize()
          
          // OllamaãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã€gemma3:1bãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
          if (manager.isUsingOllama()) {
            const models = await manager.listOllamaModels()
            const gemmaExists = models.some(m => m.name === 'gemma3:1b')
            
            if (!gemmaExists) {
              await manager.pullOllamaModelWithProgress(
                'gemma3:1b',
                (progress) => {
                  // ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ­ã‚°ã‚’éè¡¨ç¤º
                }
              )
            }
          }
        } catch (error) {
          console.warn('âš ï¸ Ollama initialization failed, using fallback:', error)
        }
        
        setLlmManager(manager)
      } catch (error) {
        console.error('âŒ Failed to initialize LLM Manager:', error)
        setLastError('Router Agent Systemã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ')
      }
    }

    initializeLLMManager()
  }, [])
  const [currentToolCalls, setCurrentToolCalls] = useState<any[]>([])
  const [isAgentMode, setIsAgentMode] = useState(false)
  const scrollAreaRef = useRef<HTMLDivElement>(null)

  // Auto-scroll
  useEffect(() => {
    if (scrollAreaRef.current) {
      scrollAreaRef.current.scrollTop = scrollAreaRef.current.scrollHeight
    }
  }, [messages, streamingResponse])

  // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¤‰æ›´ã‚’ç›£è¦–
  useEffect(() => {
    console.log('ğŸ“ Messages state changed:', {
      messageCount: messages.length,
      messages: messages.map(msg => ({
        id: msg.id,
        content: msg.content?.substring(0, 30) + (msg.content && msg.content.length > 30 ? '...' : ''),
        contentLength: msg.content?.length,
        role: msg.role
      }))
    })

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¿½åŠ ã•ã‚ŒãŸå¾Œã«isTypingã‚’ãƒªã‚»ãƒƒãƒˆ
    if (messages.length > 0 && isTyping) {
      const lastMessage = messages[messages.length - 1]
      if (lastMessage.role === 'assistant' && lastMessage.content && lastMessage.content.trim() !== '') {
        console.log('ğŸ”„ Resetting isTyping after assistant message added')
        setIsTyping(false)
        setLoadingState('idle')
      }
    }
  }, [messages, isTyping])

  // isTypingã®çŠ¶æ…‹ã‚’ç›£è¦–ã—ã¦ã€é•·æ™‚é–“trueã®ã¾ã¾ã®å ´åˆã¯ãƒªã‚»ãƒƒãƒˆ
  useEffect(() => {
    if (isTyping) {
      const timeout = setTimeout(() => {
        console.log('âš ï¸ isTyping timeout - resetting state')
        setIsTyping(false)
        setLoadingState('idle')
      }, 30000) // 30ç§’å¾Œã«ãƒªã‚»ãƒƒãƒˆ

      return () => clearTimeout(timeout)
    }
  }, [isTyping])

  // ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      // Cmd+T (Mac) ã¾ãŸã¯ Ctrl+T (Windows/Linux) ã§TTSãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ
      if ((e.metaKey || e.ctrlKey) && e.key === 't') {
        e.preventDefault()
        // TTSãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«è¿½åŠ 
        console.log('TTS model switch shortcut triggered')
      }
    }

    document.addEventListener('keydown', handleKeyDown)
    return () => document.removeEventListener('keydown', handleKeyDown)
  }, [])

  // Auto-configure provider on mount
  useEffect(() => {
    autoConfigureProvider()
  }, [])

  // ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
  const handleFileUpload = (files: FileList | File[]) => {
    const newFiles = Array.from(files)
    setSelectedFiles(prev => [...prev, ...newFiles])
  }

  const removeAttachment = (index: number) => {
    setSelectedFiles(prev => prev.filter((_, i) => i !== index))
  }

  // ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å‡¦ç†
  const handleDragOver = (e: React.DragEvent) => {
    e.preventDefault()
    setIsDragging(true)
  }

  const handleDragLeave = (e: React.DragEvent) => {
    e.preventDefault()
    setIsDragging(false)
  }

  const handleDrop = (e: React.DragEvent) => {
    e.preventDefault()
    setIsDragging(false)
    const files = e.dataTransfer.files
    if (files.length > 0) {
      handleFileUpload(files)
    }
  }

  // åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠã™ã‚‹é–¢æ•°
  const autoConfigureProvider = async () => {
    // ä¿å­˜ã•ã‚ŒãŸAPIã‚­ãƒ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    const savedApiKeys = Object.entries(providerApiKeys).filter(([_, apiKey]) => apiKey && apiKey.trim() !== '')

    if (savedApiKeys.length === 0) {
      console.log('No API keys available for auto-configuration')
      return
    }

    // æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
    const enabledModels = modelSettings.enabledModels
      .filter(model => model.enabled)
      .sort((a, b) => (a.priority || 999) - (b.priority || 999))

    // åˆ©ç”¨å¯èƒ½ãªAPIã‚­ãƒ¼ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    const availableModels = enabledModels.filter(model => {
      const hasApiKey = savedApiKeys.some(([providerId, _]) => providerId === model.providerId)
      const provider = AVAILABLE_PROVIDERS.find((p: any) => p.id === model.providerId)
      const modelExists = provider?.models.some((m: any) => m.id === model.modelId)
      return hasApiKey && modelExists
    })

    if (availableModels.length === 0) {
      console.log('No available models with valid API keys')
      return
    }

    // æœ€é«˜å„ªå…ˆåº¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    const selectedModel = availableModels[0]
    const apiKey = savedApiKeys.find(([providerId, _]) => providerId === selectedModel.providerId)?.[1]

    if (!apiKey) {
      console.error('API key not found for selected model')
      return
    }

    const config: AIProviderConfig = {
      providerId: selectedModel.providerId,
      modelId: selectedModel.modelId,
      apiKey
    }

    try {
      await handleProviderSelect(config)
      console.log(`Auto-selected model: ${selectedModel.providerId}:${selectedModel.modelId} (priority: ${selectedModel.priority})`)
    } catch (error) {
      console.error('Failed to auto-configure provider:', error)
      
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¬¡ã®å„ªå…ˆåº¦ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
      if (availableModels.length > 1) {
        const fallbackModel = availableModels[1]
        const fallbackApiKey = savedApiKeys.find(([providerId, _]) => providerId === fallbackModel.providerId)?.[1]
        
        if (fallbackApiKey) {
          const fallbackConfig: AIProviderConfig = {
            providerId: fallbackModel.providerId,
            modelId: fallbackModel.modelId,
            apiKey: fallbackApiKey
          }
          
          try {
            await handleProviderSelect(fallbackConfig)
            console.log(`Fallback to model: ${fallbackModel.providerId}:${fallbackModel.modelId}`)
          } catch (fallbackError) {
            console.error('Failed to configure fallback provider:', fallbackError)
          }
        }
      }
    }
  }

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleMessageEdit = (messageId: string, newContent: string) => {
    setMessages(prev => 
      prev.map(msg => 
        msg.id === messageId 
          ? { ...msg, content: newContent }
          : msg
      )
    )
    
    // ç·¨é›†ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å†é€ä¿¡
    handleSendMessage(newContent)
  }

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·¨é›†ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleMessageEditCancel = (messageId: string) => {
    // ç·¨é›†ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã®å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
    console.log('Message edit cancelled for:', messageId)
  }

  // åœæ­¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleStopGeneration = () => {
    console.log('ğŸ›‘ Stopping AI generation...')
    console.log('ğŸ”„ Current isGenerating state before stop:', isGenerating)
    
    // AI SDK Serviceã®åœæ­¢å‡¦ç†ã‚’å®Ÿè¡Œ
    aiSDKService.stopGeneration()
    
    // çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    setIsGenerating(false)
    setIsTyping(false)
    setLoadingState('idle')
    setStreamingResponse('')
    
    // æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã®å ´åˆã¯å‰Šé™¤
    setMessages(prev => {
      const lastMessage = prev[prev.length - 1]
      if (lastMessage && lastMessage.role === 'assistant' && (!lastMessage.content || lastMessage.content.trim() === '')) {
        return prev.slice(0, -1)
      }
      return prev
    })
    
    console.log('âœ… AI generation stopped successfully')
    console.log('ğŸ”„ isGenerating state after stop: false')
  }

  // WebSearchåˆ‡ã‚Šæ›¿ãˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleWebSearchToggle = (enabled: boolean) => {
    setWebSearchEnabled(enabled)
    console.log('WebSearch enabled:', enabled)
  }

  // TTS Modelé¸æŠãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleTTSModelSelect = (modelId: string) => {
    setCurrentTTSModel(modelId)
    // ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜
    try {
      localStorage.setItem('armis_tts_model', modelId)
    } catch (error) {
      console.error('Failed to save TTS model to localStorage:', error)
    }
    console.log('TTS Model selected:', modelId)
  }

  // Create Imageåˆ‡ã‚Šæ›¿ãˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleCreateImageToggle = (enabled: boolean) => {
    setCreateImageEnabled(enabled)
    console.log('Create Image enabled:', enabled)
    
    // Generationè¨­å®šã¨é€£æº
    if (enabled && !generationSettings.image.enabled) {
      setGenerationSettings((prev: any) => ({
        ...prev,
        image: {
          ...prev.image,
          enabled: true
        }
      }))
    }
  }

  // Audioåˆ‡ã‚Šæ›¿ãˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleAudioToggle = (enabled: boolean) => {
    setAudioEnabled(enabled)
    console.log('Audio enabled:', enabled)
    
    // Generationè¨­å®šã¨é€£æº
    if (enabled && !generationSettings.audio.enabled) {
      setGenerationSettings((prev: any) => ({
        ...prev,
        audio: {
          ...prev.audio,
          enabled: true
        }
      }))
    }
  }

  // Videoåˆ‡ã‚Šæ›¿ãˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleVideoToggle = (enabled: boolean) => {
    setVideoEnabled(enabled)
    console.log('Video enabled:', enabled)
    
    // Generationè¨­å®šã¨é€£æº
    if (enabled && !generationSettings.video.enabled) {
      setGenerationSettings((prev: any) => ({
        ...prev,
        video: {
          ...prev.video,
          enabled: true
        }
      }))
    }
  }

  // ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹é–¢æ•°
  const getCurrentMessage = () => {
    const textarea = document.querySelector('textarea[placeholder*="Plan, search, build"]') as HTMLTextAreaElement
    return textarea ? textarea.value : ''
  }

  // å‹•ç”»ç”Ÿæˆé–‹å§‹
  const startVideoGeneration = async (videoPrompt: string) => {
    setIsGeneratingVideo(true)
    setVideoProgress(0)
    
    // å‹•ç”»ç”Ÿæˆä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    const loadingMessageId = (Date.now() + 1).toString()
    const loadingMessage: ChatMessageProps = {
      id: loadingMessageId,
      content: 'Video generating...',
      role: 'assistant',
      isVideoLoading: true,
      videoProgress: 0
    }
    
    setMessages(prev => [...prev, loadingMessage])

    try {
      const veoService = new VeoService(providerApiKeys['google'] || '')
      
      // ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
      const progressInterval = setInterval(() => {
        setVideoProgress(prev => {
          const newProgress = Math.min(prev + Math.random() * 15, 90)
          setMessages(prevMessages => 
            prevMessages.map(msg => 
              msg.id === loadingMessageId 
                ? { ...msg, videoProgress: newProgress }
                : msg
            )
          )
          return newProgress
        })
      }, 500)

      // å‹•ç”»ç”Ÿæˆã®å®Ÿè¡Œ
      const result = await veoService.generateVideo({
        prompt: videoPrompt,
        duration: 8,
        aspectRatio: '16:9',
        quality: 'high'
      })
      
      let finalResult = result
      
      if (result.status === 'processing') {
        console.log('ğŸ”„ Starting video polling for ID:', result.id)
        finalResult = await veoService.pollVideoStatus(result.id, (status) => {
          console.log('Video generation status:', status)
        })
        console.log('âœ… Video polling completed:', finalResult)
      }

      clearInterval(progressInterval)
      setVideoProgress(100)

      // ç”Ÿæˆå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç½®ãæ›ãˆ
      const isSimulation = finalResult.isSimulation || finalResult.id?.startsWith('veo_sim_')
      const completedMessage: ChatMessageProps = {
        id: loadingMessageId,
        content: isSimulation 
          ? 'Video generation completed! (Simulation mode - API quota exceeded)'
          : 'Video generation completed!',
        role: 'assistant',
        videoUrl: finalResult.videoUrl,
        videoDetails: {
          duration: finalResult.duration || 8,
          aspectRatio: '16:9',
          quality: 'high',
          provider: 'google',
          model: 'veo-3.0-generate-preview',
          isSimulation: isSimulation
        }
      }
      
      console.log('ğŸ¬ Final video message:', {
        id: completedMessage.id,
        content: completedMessage.content,
        videoUrl: completedMessage.videoUrl,
        videoDetails: completedMessage.videoDetails
      })

      setMessages(prev => {
        console.log('ğŸ”„ Updating messages with video:', {
          loadingMessageId,
          completedMessage,
          prevMessages: prev.map(m => ({ id: m.id, content: m.content?.substring(0, 50), videoUrl: m.videoUrl }))
        })
        
        const updated = prev.map(msg => 
          msg.id === loadingMessageId ? completedMessage : msg
        )
        
        console.log('âœ… Updated messages:', updated.map(m => ({ id: m.id, content: m.content?.substring(0, 50), videoUrl: m.videoUrl })))
        
        return updated
      })

    } catch (error) {
      console.error('Video generation error:', error)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
      let errorContent = 'Video generation failed: Unknown error'
      
      if (error instanceof Error) {
        if (error.message.includes('429') || error.message.includes('quota') || error.message.includes('RESOURCE_EXHAUSTED')) {
          errorContent = `Video generation failed: API quota exceeded. 

To resolve this issue:
1. Check your Google Cloud billing status
2. Verify your API quota limits at https://ai.google.dev/gemini-api/docs/rate-limits
3. Consider upgrading your billing plan
4. Wait for quota reset (usually daily)

Currently using simulation mode for demonstration.`
        } else if (error.message.includes('API quota exceeded')) {
          errorContent = `Video generation failed: ${error.message}`
        } else {
          errorContent = `Video generation failed: ${error.message}`
        }
      }
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç½®ãæ›ãˆ
      const errorMessage: ChatMessageProps = {
        id: loadingMessageId,
        content: errorContent,
        role: 'assistant'
      }

      setMessages(prev => 
        prev.map(msg => 
          msg.id === loadingMessageId ? errorMessage : msg
        )
      )
    } finally {
      setIsGeneratingVideo(false)
      setIsGenerating(false)
      setVideoProgress(0)
      // Videoãƒœã‚¿ãƒ³ã¯ç„¡åŠ¹åŒ–ã—ãªã„ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•ã§åˆ‡ã‚Šæ›¿ãˆã‚‹ï¼‰
    }
  }

  // TTSè¦æ±‚ã‚’æ¤œå‡ºã™ã‚‹é–¢æ•°ï¼ˆåŸºæœ¬çš„ãªæ¤œå‡ºï¼‰
  const detectTTSRequest = (text: string): boolean => {
    const ttsKeywords = [
      'éŸ³å£°ã‚’ä½œæˆ', 'éŸ³å£°ã§', 'éŸ³å£°åŒ–', 'TTS', 'éŸ³å£°åˆæˆ',
      'éŸ³å£°ã‚’ç”Ÿæˆ', 'éŸ³å£°ã«å¤‰æ›', 'éŸ³å£°ã§èª­ã¿ä¸Šã’',
      'audio', 'voice', 'speech', 'tts', 'éŸ³å£°'
    ]
    
    const lowerText = text.toLowerCase()
    return ttsKeywords.some(keyword => lowerText.includes(keyword.toLowerCase()))
  }

  // TTSå‡¦ç†ã‚’è¡Œã†é–¢æ•°ï¼ˆæ–°ã—ã„é«˜åº¦ãªè§£ææ©Ÿèƒ½ä»˜ãï¼‰
  const processTTS = async (userInput: string, aiResponse: string) => {
    console.log('ğŸµ TTS processing started:', { audioEnabled, generationSettingsAudioEnabled: generationSettings.audio.enabled, userInput: userInput.substring(0, 50) + '...', aiResponse: aiResponse.substring(0, 50) + '...' })
    
    if (!audioEnabled || !generationSettings.audio.enabled) {
      console.log('âŒ TTS processing cancelled: Audio not enabled')
      return
    }

    // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ã€Router Agentã®åˆ¤æ–­ã«é–¢ä¿‚ãªãå¸¸ã«TTSå‡¦ç†ã‚’å®Ÿè¡Œ
    const shouldProcessTTS = audioEnabled && generationSettings.audio.enabled
    console.log('âœ… TTS processing will proceed:', shouldProcessTTS)
    

    // TTSå‡¦ç†é–‹å§‹æ™‚ã«ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    const loadingMessageId = (Date.now() + 1).toString()
    const loadingMessage: ChatMessageProps = {
      id: loadingMessageId,
      content: 'audio generating...',
      role: 'assistant',
      isTTSLoading: true, // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°
      ttsInfo: {
        provider: 'loading',
        model: 'loading',
        voice: 'loading',
        language: 'loading',
        text: 'audio generating...'
      }
    }
    
    setMessages(prev => [...prev, loadingMessage])

    try {
      // APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆå„ªå…ˆé †ä½: Settings API Keys > TTS Settings > ç’°å¢ƒå¤‰æ•°ï¼‰
      const geminiApiKey = providerApiKeys['google'] || 
                           generationSettings.audio.tts?.apiKey || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_GOOGLE_GENAI_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.GOOGLE_GENAI_API_KEY : undefined)

      // OpenAI APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆTTSè¦æ±‚è§£æç”¨ï¼‰
      const openaiApiKey = providerApiKeys['openai'] || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_OPENAI_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.OPENAI_API_KEY : undefined)

      if (!geminiApiKey && !openaiApiKey) {
        console.warn('TTS: No API keys available. Please configure Google or OpenAI API key in Settings â†’ API Keys or set environment variables.')
        return
      }

      // Hugging Face APIã‚­ãƒ¼ã‚’å–å¾—
      const huggingfaceApiKey = providerApiKeys['huggingface'] || 
                                (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_HUGGINGFACE_API_KEY : undefined) ||
                                (typeof process !== 'undefined' ? process.env.HUGGINGFACE_API_KEY : undefined)

      // è¨­å®šã‹ã‚‰é¸æŠã•ã‚ŒãŸTTSãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
      const selectedTTSProvider = generationSettings.audio.provider || 'google'
      const selectedTTSModels = generationSettings.audio.models || {}
      
      console.log('ğŸµ TTS Provider Selection Debug:', {
        generationSettingsAudioProvider: generationSettings.audio.provider,
        selectedTTSProvider,
        generationSettingsAudio: generationSettings.audio,
        currentTTSModel,
        audioEnabled
      })
      
      // æœ‰åŠ¹ãªTTSãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
      const enabledTTSModels = Object.entries(selectedTTSModels)
        .filter(([_, config]: [string, any]) => config.enabled)
        .sort(([_, a]: [string, any], [__, b]: [string, any]) => (a.priority || 999) - (b.priority || 999))
      
      // é¸æŠã•ã‚ŒãŸTTSãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯æœ€é«˜å„ªå…ˆåº¦ã®TTSãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆè¨­å®šç”»é¢ã¨å®Œå…¨ã«åŒæœŸï¼‰
      let primaryTTSModel = currentTTSModel !== 'auto' ? currentTTSModel : 
                           enabledTTSModels.length > 0 ? enabledTTSModels[0][0] : 'gemini-2.5-flash-preview-tts'
      
      // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
      if (selectedTTSProvider === 'openai') {
        // OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã€OpenAIãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
        const openaiModel = enabledTTSModels.find(([model, config]: [string, any]) => model === 'gpt-4o-mini-tts')
        if (openaiModel) {
          primaryTTSModel = 'gpt-4o-mini-tts'
        }
      } else if (selectedTTSProvider === 'google') {
        // Googleãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã€Googleãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
        const googleModel = enabledTTSModels.find(([model, config]: [string, any]) => 
          model === 'gemini-2.5-flash-preview-tts' || model === 'gemini-2.5-pro-preview-tts'
        )
        if (googleModel) {
          primaryTTSModel = googleModel[0]
        }
      }
      
      console.log('TTS Settings:', {
        provider: selectedTTSProvider,
        selectedModel: currentTTSModel,
        primaryModel: primaryTTSModel,
        enabledModels: enabledTTSModels.map(([model, config]: [string, any]) => ({ model, priority: config.priority }))
      })



      // Inworld AI APIã‚­ãƒ¼ã‚’å–å¾—
      const inworldApiKey = providerApiKeys?.['inworld'] || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_INWORLD_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.INWORLD_API_KEY : undefined)

      console.log('ğŸ¤ Inworld API Key Debug:', {
        fromProviderApiKeys: !!providerApiKeys?.['inworld'],
        fromEnv: !!(typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_INWORLD_API_KEY : undefined),
        fromProcess: !!(typeof process !== 'undefined' ? process.env.INWORLD_API_KEY : undefined),
        finalApiKey: !!inworldApiKey,
        apiKeyLength: inworldApiKey?.length || 0
      })

      // ãƒ­ãƒ¼ã‚«ãƒ«Inworldè¨­å®š
      const localInworldConfig = {
        pythonPath: process.env.PYTHON_PATH || 'python3',
        modelsDir: './models/inworld-tts-local',
        autoSetup: true
      }

      // TTSãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆï¼ˆè¨­å®šã‹ã‚‰é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼‰
      const primaryService = (() => {
        switch (selectedTTSProvider) {
          case 'google':
            return 'gemini'
          case 'openai':
            return 'openai'
          case 'inworld':
            return 'inworld'
          case 'local-inworld':
            return 'local-inworld'
          default:
            return 'gemini'
        }
      })()

      console.log('ğŸµ TTS Manager Configuration:', {
        selectedTTSProvider,
        primaryService,
        hasGeminiApiKey: !!geminiApiKey,
        hasOpenaiApiKey: !!openaiApiKey,
        hasInworldApiKey: !!inworldApiKey,
        hasHuggingfaceApiKey: !!huggingfaceApiKey,
        inworldApiKeyLength: inworldApiKey?.length || 0,
        providerApiKeysKeys: Object.keys(providerApiKeys || {}),
        inworldInProviderApiKeys: !!providerApiKeys?.['inworld']
      })

      const ttsManager = createTTSManager({
        primaryService,
        enableFallback: true, // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æœ‰åŠ¹åŒ–ã—ã¦åˆ©ç”¨å¯èƒ½ãªã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
        geminiApiKey: geminiApiKey,
        openaiApiKey: openaiApiKey,
        inworldApiKey: inworldApiKey,
        huggingfaceApiKey: huggingfaceApiKey,
        localInworldConfig: localInworldConfig,
        enableAdvancedAnalysis: true,
        enableTextExtraction: true // ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
      })

      // LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’TTSãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«è¨­å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºç”¨ï¼‰
      if (llmManager) {
        ttsManager.setLLMManager(llmManager)
      }

      if (!ttsManager.isAnyServiceAvailable()) {
        console.warn('No TTS services are available')
        
        // Local Inworld TTSãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç‰¹åˆ¥ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if (selectedTTSProvider === 'local-inworld') {
          console.warn('âš ï¸  Local Inworld TTS requires Electron environment. Please use the Electron version of the application.')
          // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
          if (typeof window !== 'undefined' && !window.electronAPI) {
            alert('Local Inworld TTS requires Electron environment. Please use the Electron version of the application for local TTS functionality.')
          }
        }
        return
      }

      // TTSå‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®šï¼ˆè¨­å®šç”»é¢ã¨å®Œå…¨ã«åŒæœŸï¼‰
      const ttsOptions = {
        speaker: {
          voiceName: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'openai':
                return generationSettings.audio.tts?.defaultVoice || 'alloy'
              case 'inworld':
                return generationSettings.audio.tts?.defaultVoice || 'Ashley'
              case 'local-inworld':
                return generationSettings.audio.tts?.defaultVoice || 'tts-1'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultVoice || 'Kore'
            }
          })(),
          language: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨€èªã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'openai':
                return generationSettings.audio.tts?.defaultLanguage || 'en-US'
              case 'inworld':
                return generationSettings.audio.tts?.defaultLanguage || 'en'
              case 'local-inworld':
                return generationSettings.audio.tts?.defaultLanguage || 'en-US'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultLanguage || 'ja-JP'
            }
          })(),
          style: generationSettings.audio.tts?.defaultStyle
        },
        model: primaryTTSModel // é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
      }

      console.log('ğŸµ TTS Processing Options:', {
        provider: selectedTTSProvider,
        selectedModel: currentTTSModel,
        model: primaryTTSModel,
        voice: ttsOptions.speaker.voiceName,
        language: ttsOptions.speaker.language,
        style: ttsOptions.speaker.style,
        localInworldConfig: selectedTTSProvider === 'local-inworld' ? localInworldConfig : undefined,
        isElectron: typeof window !== 'undefined' && !!window.electronAPI,
        windowElectronAPI: typeof window !== 'undefined' ? !!window.electronAPI : 'window undefined'
      })

      console.log('ğŸµ TTS Voice Selection Debug (1st):', {
        selectedTTSProvider,
        defaultVoiceFromSettings: generationSettings.audio.tts?.defaultVoice,
        selectedVoiceName: ttsOptions.speaker.voiceName,
        inworldCase: selectedTTSProvider === 'inworld' ? 'Ashley' : 'not inworld'
      })

      // TTSã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’è©³ç´°ã«ãƒã‚§ãƒƒã‚¯
      console.log('ğŸ” TTS Service Availability Check:', {
        selectedProvider: selectedTTSProvider,
        isLocalInworld: selectedTTSProvider === 'local-inworld',
        electronEnvironment: typeof window !== 'undefined' && !!window.electronAPI,
        ttsManagerAvailable: ttsManager.isAnyServiceAvailable()
      })

      // AudioãŒã‚ªãƒ³ã®å ´åˆã¯è‡ªå‹•çš„ã«TTSå‡¦ç†ã‚’å®Ÿè¡Œã€ãã†ã§ãªã„å ´åˆã¯TTSè¦æ±‚ã‚’æ¤œå‡º
      let result
      console.log('ğŸµ Starting TTS synthesis with options:', ttsOptions)
      
      if (shouldProcessTTS) {
        // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ã€TTSè¦æ±‚ã®æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
        console.log('ğŸµ Direct TTS synthesis (Audio enabled)')
        result = await ttsManager.synthesize(aiResponse, ttsOptions)
        console.log('ğŸµ TTS synthesis completed:', result ? 'Success' : 'Failed')
      } else {
        // AudioãŒã‚ªãƒ•ã®å ´åˆã¯ã€TTSè¦æ±‚ã‚’æ¤œå‡ºã—ã¦ã‹ã‚‰éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
        console.log('ğŸµ TTS request analysis (Audio disabled)')
        result = await ttsManager.processTTSRequest(userInput, aiResponse, ttsOptions)

        // TTSè¦æ±‚ã§ãªã„å ´åˆã¯å‡¦ç†ã‚’çµ‚äº†
        if (!result) {
          console.log('No TTS request detected or processing cancelled')
          return
        }
      }

      // TTSResultã‹ã‚‰audioUrlã‚’ä½¿ç”¨ï¼ˆæ—¢ã«WAVãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¿½åŠ æ¸ˆã¿ï¼‰
      const audioUrl = result.audioUrl || (() => {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: WAVãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã§å†ç”Ÿå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        const wavData = addWavHeader(result.audioData, 24000, 1, 16)
        const blob = new Blob([wavData], { type: 'audio/wav' })
        return URL.createObjectURL(blob)
      })()

      // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Ÿéš›ã®TTSçµæœã«ç½®ãæ›ãˆ
      const ttsMessage: ChatMessageProps = {
        id: loadingMessageId, // åŒã˜IDã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç½®ãæ›ãˆ
        content: `Audio has been generated using ${selectedTTSProvider} (${primaryTTSModel})!`,
        role: 'assistant',
        audioUrl: audioUrl,
        audioData: result.audioData,
        isTTSLoading: false, // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†
        ttsInfo: {
          provider: selectedTTSProvider,
          model: primaryTTSModel,
          selectedModel: currentTTSModel,
          voice: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'vibevoice':
                return generationSettings.audio.tts?.defaultVoice || 'vibevoice-japanese'
              case 'openai':
                return generationSettings.audio.tts?.defaultVoice || 'alloy'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultVoice || 'Kore'
            }
          })(),
          language: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨€èªã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'vibevoice':
                return generationSettings.audio.tts?.defaultLanguage || 'ja-JP'
              case 'openai':
                return generationSettings.audio.tts?.defaultLanguage || 'en-US'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultLanguage || 'ja-JP'
            }
          })(),
          style: generationSettings.audio.tts?.defaultStyle,
          text: aiResponse.substring(0, 100) + (aiResponse.length > 100 ? '...' : '')
        }
      }
      
      setMessages(prev => prev.map(msg => msg.id === loadingMessageId ? ttsMessage : msg))
      
      console.log('TTS processing completed for:', aiResponse.substring(0, 50) + '...')
    } catch (error) {
      console.error('TTS processing failed:', error)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤º
      const errorMessage = error instanceof Error ? error.message : 'Unknown TTS error'
      
      // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç½®ãæ›ãˆ
      const errorMsg: ChatMessageProps = {
        id: loadingMessageId, // åŒã˜IDã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç½®ãæ›ãˆ
        content: errorMessage.includes('quota exceeded') || errorMessage.includes('429') 
          ? `âš ï¸ **TTS Quota Exceeded**\n\nGemini TTSã®1æ—¥ã®åˆ©ç”¨åˆ¶é™ï¼ˆ15å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚\n\n**è§£æ±ºæ–¹æ³•:**\nâ€¢ 24æ™‚é–“å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„\nâ€¢ è¨­å®šã§Web Speech APIã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„\nâ€¢ æœ‰æ–™ãƒ—ãƒ©ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„`
          : `âŒ **TTS Error**\n\n${errorMessage}\n\n**è§£æ±ºæ–¹æ³•:**\nâ€¢ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ è¨­å®šã§TTSã‚µãƒ¼ãƒ“ã‚¹ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„`,
        role: 'assistant',
        isError: true,
        isTTSLoading: false // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†
      }
      
      setMessages(prev => prev.map(msg => msg.id === loadingMessageId ? errorMsg : msg))
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ä»˜ãTTSå‡¦ç†ã‚’è¡Œã†é–¢æ•°
  const processTTSWithTextExtraction = async (userInput: string) => {
    console.log('ğŸµ TTS Text Extraction processing started:', { audioEnabled, generationSettingsAudioEnabled: generationSettings.audio.enabled, userInput: userInput.substring(0, 50) + '...' })
    
    if (!audioEnabled || !generationSettings.audio.enabled) {
      console.log('âŒ TTS Text Extraction processing cancelled: Audio not enabled')
      return
    }

    // TTSå‡¦ç†é–‹å§‹æ™‚ã«ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    const loadingMessageId = (Date.now() + 1).toString()
    const loadingMessage: ChatMessageProps = {
      id: loadingMessageId,
      content: 'audio generating...',
      role: 'assistant',
      isTTSLoading: true, // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°
      ttsInfo: {
        provider: 'loading',
        model: 'loading',
        voice: 'loading',
        language: 'loading',
        text: 'audio generating...'
      }
    }
    
    setMessages(prev => [...prev, loadingMessage])

    try {
      // APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆå„ªå…ˆé †ä½: Settings API Keys > TTS Settings > ç’°å¢ƒå¤‰æ•°ï¼‰
      const geminiApiKey = providerApiKeys['google'] || 
                           generationSettings.audio.tts?.apiKey || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_GOOGLE_GENAI_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.GOOGLE_GENAI_API_KEY : undefined)

      // OpenAI APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆTTSè¦æ±‚è§£æç”¨ï¼‰
      const openaiApiKey = providerApiKeys['openai'] || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_OPENAI_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.OPENAI_API_KEY : undefined)

      if (!geminiApiKey && !openaiApiKey) {
        console.warn('TTS: No API keys available. Please configure Google or OpenAI API key in Settings â†’ API Keys or set environment variables.')
        return
      }

      // Hugging Face APIã‚­ãƒ¼ã‚’å–å¾—
      const huggingfaceApiKey = providerApiKeys['huggingface'] || 
                                (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_HUGGINGFACE_API_KEY : undefined) ||
                                (typeof process !== 'undefined' ? process.env.HUGGINGFACE_API_KEY : undefined)

      // è¨­å®šã‹ã‚‰é¸æŠã•ã‚ŒãŸTTSãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
      const selectedTTSProvider = generationSettings.audio.provider || 'google'
      const selectedTTSModels = generationSettings.audio.models || {}
      
      // æœ‰åŠ¹ãªTTSãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
      const enabledTTSModels = Object.entries(selectedTTSModels)
        .filter(([_, config]: [string, any]) => config.enabled)
        .sort(([_, a]: [string, any], [__, b]: [string, any]) => (a.priority || 999) - (b.priority || 999))
      
      // é¸æŠã•ã‚ŒãŸTTSãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯æœ€é«˜å„ªå…ˆåº¦ã®TTSãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆè¨­å®šç”»é¢ã¨å®Œå…¨ã«åŒæœŸï¼‰
      let primaryTTSModel = currentTTSModel !== 'auto' ? currentTTSModel : 
                           enabledTTSModels.length > 0 ? enabledTTSModels[0][0] : 'gemini-2.5-flash-preview-tts'
      
      // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
      if (selectedTTSProvider === 'openai') {
        // OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã€OpenAIãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
        const openaiModel = enabledTTSModels.find(([model, config]: [string, any]) => model === 'gpt-4o-mini-tts')
        if (openaiModel) {
          primaryTTSModel = 'gpt-4o-mini-tts'
        }
      } else if (selectedTTSProvider === 'google') {
        // Googleãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã€Googleãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
        const googleModel = enabledTTSModels.find(([model, config]: [string, any]) => 
          model === 'gemini-2.5-flash-preview-tts' || model === 'gemini-2.5-pro-preview-tts'
        )
        if (googleModel) {
          primaryTTSModel = googleModel[0]
        }
      }

      // Inworld AI APIã‚­ãƒ¼ã‚’å–å¾—
      const inworldApiKey = providerApiKeys['inworld'] || 
                           (typeof import.meta !== 'undefined' ? (import.meta as any).env?.VITE_INWORLD_API_KEY : undefined) ||
                           (typeof process !== 'undefined' ? process.env.INWORLD_API_KEY : undefined)

      // ãƒ­ãƒ¼ã‚«ãƒ«Inworldè¨­å®š
      const localInworldConfig = {
        pythonPath: process.env.PYTHON_PATH || 'python3',
        modelsDir: './models/inworld-tts-local',
        autoSetup: true
      }

      // TTSãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆï¼ˆè¨­å®šã‹ã‚‰é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼‰
      const primaryService = (() => {
        switch (selectedTTSProvider) {
          case 'google':
            return 'gemini'
          case 'openai':
            return 'openai'
          case 'inworld':
            return 'inworld'
          case 'local-inworld':
            return 'local-inworld'
          default:
            return 'gemini'
        }
      })()

      const ttsManager = createTTSManager({
        primaryService,
        enableFallback: true, // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æœ‰åŠ¹åŒ–ã—ã¦åˆ©ç”¨å¯èƒ½ãªã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
        geminiApiKey: geminiApiKey,
        openaiApiKey: openaiApiKey,
        inworldApiKey: inworldApiKey,
        huggingfaceApiKey: huggingfaceApiKey,
        localInworldConfig: localInworldConfig,
        enableAdvancedAnalysis: true,
        enableTextExtraction: true // ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
      })

      // LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’TTSãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«è¨­å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºç”¨ï¼‰
      if (llmManager) {
        ttsManager.setLLMManager(llmManager)
      }

      if (!ttsManager.isAnyServiceAvailable()) {
        console.warn('No TTS services are available')
        return
      }

      // TTSå‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®šï¼ˆè¨­å®šç”»é¢ã¨å®Œå…¨ã«åŒæœŸï¼‰
      const ttsOptions = {
        speaker: {
          voiceName: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'openai':
                return generationSettings.audio.tts?.defaultVoice || 'alloy'
              case 'inworld':
                return generationSettings.audio.tts?.defaultVoice || 'Ashley'
              case 'local-inworld':
                return generationSettings.audio.tts?.defaultVoice || 'tts-1'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultVoice || 'Kore'
            }
          })(),
          language: (() => {
            // è¨­å®šç”»é¢ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨€èªã‚’é¸æŠ
            switch (selectedTTSProvider) {
              case 'openai':
                return generationSettings.audio.tts?.defaultLanguage || 'en-US'
              case 'inworld':
                return generationSettings.audio.tts?.defaultLanguage || 'en'
              case 'local-inworld':
                return generationSettings.audio.tts?.defaultLanguage || 'en-US'
              case 'google':
              default:
                return generationSettings.audio.tts?.defaultLanguage || 'ja-JP'
            }
          })(),
          style: generationSettings.audio.tts?.defaultStyle
        },
        model: primaryTTSModel // é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
      }

      console.log('ğŸµ TTS Text Extraction: Starting synthesis with options:', ttsOptions)
      
      console.log('ğŸµ TTS Voice Selection Debug (2nd):', {
        selectedTTSProvider,
        defaultVoiceFromSettings: generationSettings.audio.tts?.defaultVoice,
        selectedVoiceName: ttsOptions.speaker.voiceName,
        inworldCase: selectedTTSProvider === 'inworld' ? 'Ashley' : 'not inworld'
      })
      
      // ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ä»˜ãTTSå‡¦ç†ã‚’å®Ÿè¡Œ
      const result = await ttsManager.processTTSWithTextExtraction(userInput, ttsOptions)
      
      if (!result) {
        console.log('ğŸµ TTS Text Extraction: No result returned, skipping audio generation')
        // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
        setMessages(prev => prev.filter(msg => msg.id !== loadingMessageId))
        return
      }

      // TTSResultã‹ã‚‰audioUrlã‚’ä½¿ç”¨ï¼ˆæ—¢ã«WAVãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¿½åŠ æ¸ˆã¿ï¼‰
      const audioUrl = result.audioUrl || (() => {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: WAVãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã§å†ç”Ÿå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        const wavData = addWavHeader(result.audioData, 24000, 1, 16)
        const audioBlob = new Blob([wavData], { type: 'audio/wav' })
        return URL.createObjectURL(audioBlob)
      })()

      // TTSæƒ…å ±ã‚’è¨­å®š
      const ttsInfo = {
        provider: selectedTTSProvider,
        model: primaryTTSModel,
        voice: ttsOptions.speaker.voiceName,
        language: ttsOptions.speaker.language,
        style: ttsOptions.speaker.style,
        text: userInput
      }

      // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’TTSçµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç½®ãæ›ãˆ
      const ttsMessage: ChatMessageProps = {
        id: loadingMessageId, // åŒã˜IDã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç½®ãæ›ãˆ
        content: `Audio has been generated using ${selectedTTSProvider} (${primaryTTSModel})!`,
        role: 'assistant',
        audioUrl: audioUrl,
        audioData: result.audioData,
        ttsInfo: ttsInfo,
        isTTSLoading: false // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†
      }
      
      setMessages(prev => prev.map(msg => msg.id === loadingMessageId ? ttsMessage : msg))
      
      console.log('ğŸµ TTS Text Extraction processing completed for:', userInput.substring(0, 50) + '...')
    } catch (error) {
      console.error('ğŸµ TTS Text Extraction processing failed:', error)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤º
      const errorMessage = error instanceof Error ? error.message : 'Unknown TTS error'
      
      // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç½®ãæ›ãˆ
      const errorMsg: ChatMessageProps = {
        id: loadingMessageId, // åŒã˜IDã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç½®ãæ›ãˆ
        content: errorMessage.includes('quota exceeded') || errorMessage.includes('429') 
          ? `âš ï¸ **TTS Quota Exceeded**\n\nGemini TTSã®1æ—¥ã®åˆ©ç”¨åˆ¶é™ï¼ˆ15å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚\n\n**è§£æ±ºæ–¹æ³•:**\nâ€¢ 24æ™‚é–“å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„\nâ€¢ è¨­å®šã§Web Speech APIã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„\nâ€¢ æœ‰æ–™ãƒ—ãƒ©ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„`
          : `âŒ **TTS Error**\n\n${errorMessage}\n\n**è§£æ±ºæ–¹æ³•:**\nâ€¢ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„\nâ€¢ è¨­å®šã§TTSã‚µãƒ¼ãƒ“ã‚¹ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„`,
        role: 'assistant',
        isError: true,
        isTTSLoading: false // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†
      }
      
      setMessages(prev => prev.map(msg => msg.id === loadingMessageId ? errorMsg : msg))
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // WAVãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  function addWavHeader(pcmData: ArrayBuffer, sampleRate: number, channels: number, bitsPerSample: number): ArrayBuffer {
    const dataLength = pcmData.byteLength
    const headerLength = 44
    const totalLength = headerLength + dataLength
    
    const buffer = new ArrayBuffer(totalLength)
    const view = new DataView(buffer)
    
    // WAVãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    // RIFFãƒ˜ãƒƒãƒ€ãƒ¼
    writeString(0, 'RIFF')
    view.setUint32(4, totalLength - 8, true) // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º - 8
    writeString(8, 'WAVE')
    
    // fmt ãƒãƒ£ãƒ³ã‚¯
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true) // fmtãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
    view.setUint16(20, 1, true) // PCMå½¢å¼
    view.setUint16(22, channels, true) // ãƒãƒ£ãƒ³ãƒãƒ«æ•°
    view.setUint32(24, sampleRate, true) // ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
    view.setUint32(28, sampleRate * channels * bitsPerSample / 8, true) // ãƒã‚¤ãƒˆãƒ¬ãƒ¼ãƒˆ
    view.setUint16(32, channels * bitsPerSample / 8, true) // ãƒ–ãƒ­ãƒƒã‚¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
    view.setUint16(34, bitsPerSample, true) // ãƒ“ãƒƒãƒˆæ·±åº¦
    
    // data ãƒãƒ£ãƒ³ã‚¯
    writeString(36, 'data')
    view.setUint32(40, dataLength, true) // ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
    
    // PCMãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
    const pcmView = new Uint8Array(pcmData)
    const bufferView = new Uint8Array(buffer)
    bufferView.set(pcmView, headerLength)
    
    return buffer
  }

  // ç”»åƒç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
  const handleImageGenerationRequest = async (content: string) => {
    let taskId: string | undefined
    try {
      // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
      if (audioEnabled && generationSettings.audio.enabled) {
        const audioMessage: ChatMessageProps = {
          id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          content: '',
          role: 'assistant',
          metadata: {
            title: 'Audio Mode Active'
          }
        }
        setMessages(prev => [...prev, audioMessage])
        return
      }

      console.log('ğŸ¨ Starting image generation process...')
      
      // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
      const imagePrompt = content
        .replace(/ç”»åƒã‚’ç”Ÿæˆ|create image|generate image|draw|paint|ç”»åƒã‚’ä½œæˆ|çµµã‚’æã„ã¦/gi, '')
        .trim()
      
      if (!imagePrompt) {
        const errorMessage = 'ç”»åƒç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹: "ç¾ã—ã„å¤•æ—¥ã‚’ç”»åƒã§ç”Ÿæˆ"'
        setMessages(prev => [...prev, {
          id: (Date.now() + 1).toString(),
          content: errorMessage,
          role: 'assistant'
        }])
        return
      }

      // ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’é–‹å§‹
      taskId = await startTask({
        taskType: 'generation',
        title: 'Image Generation',
        description: `ç”»åƒç”Ÿæˆä¸­: ${imagePrompt.substring(0, 50)}${imagePrompt.length > 50 ? '...' : ''}`,
        estimatedDuration: 15000,
        metadata: {
          complexity: 'medium',
          priority: 'medium',
          resources: ['AI Image Model']
        }
      })

      // ç’°å¢ƒå¤‰æ•°ã®è¨­å®šçŠ¶æ³ã‚’ç¢ºèª
      const config = await checkGoogleAIConfig()
      
      if (!config.isConfigured) {
        // ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        const setupMessage = `## ğŸ¨ ç”»åƒç”Ÿæˆã®è¨­å®šãŒå¿…è¦ã§ã™

ç”»åƒç”Ÿæˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€Google AI APIã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚

### ğŸ“‹ è¨­å®šæ‰‹é †:

1. **Google Cloud Console**ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
2. **Vertex AI API**ã‚’æœ‰åŠ¹åŒ–
3. **API Key**ã‚’ä½œæˆ
4. **Project ID**ã‚’å–å¾—

### ğŸ”§ ç’°å¢ƒå¤‰æ•°ã®è¨­å®š:

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«\`.env\`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š

\`\`\`bash
# Google AI API Key for Gemini File Upload and Image Generation
VITE_GOOGLE_API_KEY=your_google_api_key_here

# Google Cloud Project ID for Vertex AI Image Generation
VITE_GOOGLE_PROJECT_ID=your_google_cloud_project_id_here

# Google Cloud Location for Vertex AI (default: us-central1)
VITE_GOOGLE_LOCATION=us-central1
\`\`\`

### ğŸ“– è©³ç´°ãªè¨­å®šæ–¹æ³•:

è©³ç´°ãªè¨­å®šæ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[README.md](./README.md)ã®ã€ŒGemini Image Generation Integrationã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### ğŸ”„ è¨­å®šå¾Œã®å†èµ·å‹•:

ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ãŸå¾Œã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ã€‚

---
**ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** ${imagePrompt}
**çŠ¶æ…‹:** è¨­å®šå¾…ã¡`

        setMessages(prev => [...prev, {
          id: (Date.now() + 1).toString(),
          content: setupMessage,
          role: 'assistant'
        }])
        
        // ã‚¿ã‚¹ã‚¯å®Œäº†
        if (taskId) {
          completeTask(taskId, false)
        }
        return
      }

      // Gemini Image Serviceã‚’ä½¿ç”¨ã—ã¦ç”»åƒç”Ÿæˆ
      const geminiImageService = new GeminiImageService()

      // LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè£œå®Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ï¼‰
      let llmService = null
      try {
        if (llmManager) {
          // ç¾åœ¨ã®LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—
          llmService = llmManager.getLLMService()
        }
      } catch (error) {
        console.warn('LLMã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè£œå®Œæ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™:', error)
      }

      // Gemini Image Serviceã‚’è¨­å®šï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè£œå®Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä»˜ãï¼‰
      await geminiImageService.configure(
        config.googleApiKey!, 
        config.googleProjectId!, 
        config.googleLocation,
        llmService || undefined,
        true, // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè£œå®Œæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        true  // APIã‚­ãƒ¼æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç”»åƒç”Ÿæˆæ™‚ã«å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã†ï¼‰
      )

      // ç”»åƒç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
      const imageRequest = {
        prompt: imagePrompt,
        model: 'gemini-2.0-flash-preview-image-generation',
        aspectRatio: '1:1' as const,
        quality: 'standard' as const,
        style: 'photorealistic' as const,
        safetyFilter: 'block_some' as const,
        personGeneration: 'dont_allow' as const
      }

      console.log('ğŸ”„ Generating image with prompt:', imagePrompt)
      
      // ç”»åƒç”Ÿæˆã‚’å®Ÿè¡Œ
      const imageResponse = await geminiImageService.generateImage(imageRequest)
      
      console.log('âœ… Image generation completed:', imageResponse)

      // ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
      const assistantMessage: ChatMessageProps = {
        id: (Date.now() + 1).toString(),
        content: `image generated!`,
        role: 'assistant',
        images: imageResponse.images // ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
      }
      
      setMessages(prev => [...prev, assistantMessage])
      
      // ã‚¿ã‚¹ã‚¯å®Œäº†
      if (taskId) {
        completeTask(taskId, true)
      }

    } catch (error) {
      console.error('âŒ Image generation failed:', error)
      
      const errorMessage = `ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : 'Unknown error'}`
      
      setMessages(prev => [...prev, {
        id: (Date.now() + 1).toString(),
        content: errorMessage,
        role: 'assistant'
      }])
      
      // ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼
      if (taskId) {
        completeTask(taskId, false)
      }
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
  const performWebSearch = async (query: string): Promise<string | null> => {
    if (!webSearchEnabled) {
      return null
    }

    try {
      console.log('Performing web search for:', query)
      const searchResult = await webSearchManager.performWebSearch(query, 'google', 'auto')
      
      // æ¤œç´¢çµæœã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
      const searchContext = webSearchManager.formatSearchResultAsMarkdown(searchResult)
      
      return searchContext
    } catch (error) {
      console.error('Web search failed:', error)
      return `**ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚¨ãƒ©ãƒ¼:** ${error instanceof Error ? error.message : 'æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ'}\n`
    }
  }

  const handleSendMessage = async (content: string, contexts?: Array<{id: string, type: string, name: string, value?: string}>) => {
    if (!content.trim() && selectedFiles.length === 0) return

    setIsTyping(true)
    console.log('ğŸ”„ Setting isGenerating to true in handleSendMessage')
    setIsGenerating(true)
    setLoadingState('text')

    // ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
    let searchContext = ''
    if (webSearchEnabled) {
      try {
        const searchResult = await performWebSearch(content.trim())
        if (searchResult) {
          searchContext = searchResult + '\n\n'
        }
      } catch (error) {
        console.error('Web search error during message send:', error)
      }
    }

    // YouTube URLã‚’æ¤œå‡º
    const youtubeRegex = /^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.be)\/.+/
    const isYouTubeUrl = youtubeRegex.test(content.trim())

    // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãï¼‰
    const userMessage: ChatMessageProps = {
      id: Date.now().toString(),
      content: content.trim(),
      role: 'user',
      files: selectedFiles.length > 0 ? selectedFiles : undefined
    }
    console.log('â• Adding user message:', userMessage)
    setMessages(prev => [...prev, userMessage])

    // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜ï¼ˆAPIå‘¼ã³å‡ºã—å¾Œã«ã‚¯ãƒªã‚¢ï¼‰
    const filesToSend = [...selectedFiles]
    setSelectedFiles([])

    try {
      // VideoãŒæœ‰åŠ¹ãªå ´åˆã¯å‹•ç”»ç”Ÿæˆã‚’å®Ÿè¡Œ
      if (videoEnabled) {
        await startVideoGeneration(content.trim())
        return
      }

      // ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
      const currentConfig = aiSDKService.getCurrentConfig()
      
      // Ollamaã¾ãŸã¯LlamaCppãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
      if (currentConfig && (currentConfig.providerId === 'ollama' || currentConfig.providerId === 'llama-cpp') && llmManager) {
        try {
          setIsDownloading(true)
          setDownloadProgress({ status: 'checking', message: 'Checking model availability...' })

          // ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          await llmManager.ensureModelAvailable(currentConfig.modelId, (progress) => {
            // ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé–‹å§‹ã•ã‚ŒãŸã‚‰ã€ŒChecking model availability...ã€ã‚’éè¡¨ç¤ºã«ã™ã‚‹
            if (progress && progress.status === 'downloading') {
              setDownloadProgress(null)
            }
            // é€²è¡ŒçŠ¶æ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿æ›´æ–°ï¼ˆcheckingçŠ¶æ…‹ã®é‡è¤‡ã‚’é˜²ãï¼‰
            if (progress && (progress.status === 'starting' || progress.status === 'downloading' || progress.status === 'verifying' || progress.status === 'completed' || progress.status === 'error')) {
              setDownloadProgress(progress)
            }
            console.log('Model availability check progress:', progress)
            console.log('Progress status:', progress?.status)
            console.log('Progress message:', progress?.message)
          })

          setDownloadProgress({ status: 'completed', message: 'Model ready!' })
        } catch (error) {
          console.error('Failed to ensure model availability:', error)
          setLastError(`Model not available: ${error}`)
          setIsTyping(false)
          setIsGenerating(false)
          setLoadingState('idle')
          return
        } finally {
          setIsDownloading(false)
          setDownloadProgress(null)
        }
      }

      if (content.toLowerCase().includes('sequential') || content.toLowerCase().includes('step by step') || content.toLowerCase().includes('plan')) {
        await handleSequentialThinkingRequest(content, contexts)
      } else if (isYouTubeUrl && selectedFiles.length === 0) {
        // YouTube URLã®å ´åˆã¯å‹•ç”»åˆ†æã‚’å®Ÿè¡Œ
        await handleYouTubeVideoRequest(content, contexts)
      } else {
        await handleRegularChatRequest(content, contexts, filesToSend, searchContext)
      }
    } catch (error) {
      console.error('Error handling message:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred'
      setLastError(errorMessage)
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  const startTask = async (taskInfo: {
    taskType: 'analysis' | 'generation' | 'processing' | 'search' | 'computation' | 'optimization' | 'learning' | 'custom'
    title: string
    description: string
    estimatedDuration?: number
    metadata?: {
      model?: string
      complexity?: string
      priority?: 'low' | 'medium' | 'high'
      resources?: string[]
    }
  }) => {
    // Chat Response Generationã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
    if (taskInfo.title && taskInfo.title.includes('Chat Response Generation')) {
      return `excluded_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    }

    const taskId = await addTask({
      ...taskInfo,
      status: 'pending',
      progress: 0
    })

    // ã™ãã«å®Ÿè¡Œä¸­ã«å¤‰æ›´
    setTimeout(() => {
      updateTask(taskId, { status: 'running', progress: 10 })
    }, 100)

    return taskId
  }

  const updateTaskProgress = (taskId: string, progress: number, step?: string, stepIndex?: number) => {
    updateTask(taskId, { 
      progress, 
      currentStep: step,
      currentStepIndex: stepIndex
    })
  }

  const completeTask = (taskId: string, success: boolean = true) => {
    updateTask(taskId, { 
      status: success ? 'completed' : 'error',
      progress: 100
    })
  }

  // ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  const startFileCreation = (fileInfo: {
    fileName: string
    fileType: 'text' | 'image' | 'video' | 'audio' | 'code' | 'archive' | 'document' | 'other'
    estimatedDuration?: number
    metadata?: {
      size?: number
      format?: string
      quality?: string
      compression?: string
    }
  }) => {
    const fileCreationId = addFileCreation({
      ...fileInfo,
      status: 'creating',
      progress: 0
    })

    // ã™ãã«å‡¦ç†ä¸­ã«å¤‰æ›´
    setTimeout(() => {
      updateFileCreation(fileCreationId, { status: 'processing', progress: 10 })
    }, 100)

    return fileCreationId
  }

  const updateFileCreationProgress = (fileCreationId: string, progress: number, step?: string, stepIndex?: number) => {
    updateFileCreation(fileCreationId, { 
      progress, 
      currentStep: step,
      currentStepIndex: stepIndex
    })
  }

  const completeFileCreation = (fileCreationId: string, success: boolean = true) => {
    updateFileCreation(fileCreationId, { 
      status: success ? 'completed' : 'error',
      progress: 100
    })
  }

  const handleSequentialThinkingRequest = async (content: string, contexts?: Array<{id: string, type: string, name: string, value?: string}>) => {
    let taskId: string | undefined
    try {
      // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
      if (audioEnabled && generationSettings.audio.enabled) {
        // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¿½åŠ ã—ãªã„
        return
      }

      setIsAgentMode(true)
      setShowSequentialThinking(true)

      // ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’é–‹å§‹
      taskId = await startTask({
        taskType: 'analysis',
        title: 'Sequential Thinking Analysis',
        description: `è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã€Œ${content}ã€ã®æ®µéšçš„æ€è€ƒåˆ†æã‚’å®Ÿè¡Œä¸­`,
        estimatedDuration: 30000, // 30ç§’
        metadata: {
          complexity: 'high',
          priority: 'high',
          resources: ['AI Model', 'Planning Tools']
        }
      })

      if (aiSDKService.isProviderConfigured()) {
        // ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’å–å¾—
        const currentConfig = aiSDKService.getCurrentConfig()
        if (!currentConfig) {
          throw new Error('No provider configured')
        }

        // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®API Keyã‚’å–å¾—
        const apiKey = providerApiKeys[currentConfig.providerId]
        if (!apiKey) {
          throw new Error(`API Key not found for provider: ${currentConfig.providerId}`)
        }

        // API Keyã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å†è¨­å®š
        await aiSDKService.configureProvider({
          ...currentConfig,
          apiKey: apiKey
        })

        // ã‚¿ã‚¹ã‚¯é€²æ—ã‚’æ›´æ–°
        if (taskId) {
          updateTaskProgress(taskId, 30, 'AIãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†', 1)
        }

        // Use AI SDK with tools for Sequential Thinking
        let fullResponse = ''
        const toolCalls: any[] = []
        
        if (taskId) {
          updateTaskProgress(taskId, 50, 'AIåˆ†æå®Ÿè¡Œä¸­', 2)
        }
        
        setIsTyping(true) // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹æ™‚ã«isTypingã‚’trueã«è¨­å®š
        let lastFullResponse = '' // é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        await aiSDKService.streamResponse(
          `You are an AI assistant specialized in video production and media editing. The user wants to: ${content}. 
           Use the available tools to accomplish this task step by step. Think through the process and use tools when needed.
           ${contexts ? `Context: ${contexts.map(ctx => `@${ctx.name}`).join(', ')}` : ''}`,
          (chunk: string) => {
            fullResponse += chunk
            // é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if (fullResponse !== lastFullResponse) {
              setStreamingResponse(fullResponse)
              lastFullResponse = fullResponse
            }
          }
        )

        if (taskId) {
          updateTaskProgress(taskId, 80, 'çµæœç”Ÿæˆä¸­', 3)
        }

        // Create Sequential Thinking plan
        const plan: SequentialThinkingPlan = {
          id: Date.now().toString(),
          steps: [
            {
              id: '1',
              type: 'thinking',
              content: `Analyzing user request: "${content}". This appears to be a request for ${content.toLowerCase().includes('video') ? 'video creation' : 'content processing'}.`,
              timestamp: new Date()
            },
            ...toolCalls.map((toolCall, index) => ({
              id: (index + 2).toString(),
              type: 'tool_call' as const,
              content: '',
              toolCall: {
                name: toolCall.name,
                arguments: toolCall.arguments
              },
              timestamp: new Date()
            }))
          ],
          status: 'completed',
          createdAt: new Date(),
          updatedAt: new Date()
        }

        setCurrentPlan(plan)

        // Add assistant message
        const assistantMessage: ChatMessageProps = {
          id: (Date.now() + 1).toString(),
          content: fullResponse,
          role: 'assistant',
        }
        setMessages(prev => [...prev, assistantMessage])
        setStreamingResponse('')
        setIsTyping(false) // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã«isTypingã‚’falseã«è¨­å®š
        setIsGenerating(false)
      } else {
        // Fallback to mock Sequential Thinking
        const mockPlan: SequentialThinkingPlan = {
          id: Date.now().toString(),
          steps: [
            {
              id: '1',
              type: 'thinking',
              content: `Analyzing user request: "${content}". This appears to be a request for ${content.toLowerCase().includes('video') ? 'video creation' : 'content processing'}.`,
              timestamp: new Date()
            },
            {
              id: '2',
              type: 'tool_call',
              content: '',
              toolCall: {
                name: content.toLowerCase().includes('url') ? 'web_scraper' : 'file_processor',
                arguments: content.toLowerCase().includes('url') ? { url: 'https://example.com' } : { filePath: 'input.txt', type: 'text' }
              },
              timestamp: new Date()
            },
            {
              id: '3',
              type: 'result',
              content: '',
              result: { success: true, processedData: 'Sample processed data' },
              timestamp: new Date()
            }
          ],
          status: 'completed',
          createdAt: new Date(),
          updatedAt: new Date()
        }

        setCurrentPlan(mockPlan)

        // ã‚¿ã‚¹ã‚¯å®Œäº†
        if (taskId) {
          completeTask(taskId, true)
        }

        setTimeout(() => {
          const assistantMessage: ChatMessageProps = {
            id: (Date.now() + 1).toString(),
            content: `I've processed your request using Sequential Thinking! Here's what I did:\n\n1. Analyzed your request\n2. Used appropriate tools to process the content\n3. Generated the results\n\n${content.toLowerCase().includes('video') ? 'Your video is ready!' : 'Your content has been processed successfully!'}`,
            role: 'assistant',
          }
          setMessages(prev => [...prev, assistantMessage])
          setIsTyping(false)
          setIsGenerating(false)
        }, 3000)
      }
    } catch (error) {
      console.error('Error in Sequential Thinking:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error'
      setLastError(errorMessage)
      
      // ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼
      if (taskId) {
        completeTask(taskId, false)
      }
      
      const assistantMessage: ChatMessageProps = {
        id: (Date.now() + 1).toString(),
        content: `âŒ Sequential Thinking failed: ${errorMessage}`,
        role: 'assistant',
      }
      setMessages(prev => [...prev, assistantMessage])
      setIsGenerating(false)
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
      setIsAgentMode(false)
    }
  }

  const handleRegularChatRequest = async (content: string, contexts?: Array<{id: string, type: string, name: string, value?: string}>, files?: File[], searchContext?: string) => {
    let taskId: string | undefined
    try {

      // ç”»åƒç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œå‡ºï¼‰
      const isImageGenerationRequest = createImageEnabled && (
        content.toLowerCase().includes('ç”»åƒã‚’ç”Ÿæˆ') || 
        content.toLowerCase().includes('create image') ||
        content.toLowerCase().includes('generate image') ||
        content.toLowerCase().includes('draw') ||
        content.toLowerCase().includes('paint') ||
        content.toLowerCase().includes('ç”»åƒã‚’ä½œæˆ') ||
        content.toLowerCase().includes('çµµã‚’æã„ã¦') ||
        content.toLowerCase().includes('ç”»åƒ') && content.toLowerCase().includes('ä½œã£ã¦') ||
        content.toLowerCase().includes('picture') && content.toLowerCase().includes('create') ||
        content.toLowerCase().includes('image') && content.toLowerCase().includes('generate')
      )

      if (isImageGenerationRequest) {
        console.log('ğŸ–¼ï¸ Image generation request detected')
        console.log('ğŸ“Š Image generation settings:', {
          createImageEnabled,
          generationSettings: generationSettings.image,
          content: content.substring(0, 100) + '...'
        })
        await handleImageGenerationRequest(content)
        return
      }

      // æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦contentã‚’æ‹¡å¼µ
      let enhancedContent = content
      if (searchContext) {
        enhancedContent = `${searchContext}**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•:**\n${content}`
      }
      
      // å…¥åŠ›åˆ†æã‚’å®Ÿè¡Œï¼ˆAudioã®çŠ¶æ…‹ã‚’å«ã‚ã‚‹ï¼‰
      const analysis = inputAnalyzer.analyzeInput(enhancedContent, { 
        files,
        audioEnabled: audioEnabled && generationSettings.audio.enabled
      })
      setCurrentAnalysis(analysis)
      
      console.log('Input Analysis:', analysis)

      // ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’é–‹å§‹
      taskId = await startTask({
        taskType: analysis.complexity === 'complex' ? 'analysis' : 'generation',
        title: analysis.suggestedAgent && analysis.suggestedAgent !== 'file_processor' ? `${analysis.suggestedAgent} Agent Execution` : 'Chat Response Generation',
        description: `ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€Œ${content.substring(0, 50)}${content.length > 50 ? '...' : ''}ã€ã‚’å‡¦ç†ä¸­`,
        estimatedDuration: analysis.complexity === 'complex' ? 15000 : 8000,
        metadata: {
          complexity: analysis.complexity,
          priority: analysis.complexity === 'complex' ? 'high' : 'medium',
          resources: files && files.length > 0 ? ['File Processing', 'AI Model'] : ['AI Model']
        }
      })
      
      // Router Agent SystemãŒåˆ©ç”¨å¯èƒ½ã§ã€Router AgentãŒå¿…è¦ãªå ´åˆ
      if (llmManager && analysis.needsRouterAgent) {
        // ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã‚’è¨­å®šï¼ˆreasoningã¯éè¡¨ç¤ºï¼‰
        setAgentInfo({
          type: analysis.suggestedAgent || null,
          confidence: analysis.confidence,
          reasoning: ''
        })
        
        // Router Agent Systemã§å‡¦ç†
        const response = await llmManager.routeAndExecute(enhancedContent, {
          files,
          contexts,
          analysis
        })
        
        // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’éè¡¨ç¤ºã«ã—ã¦TTSå‡¦ç†ã®ã¿ã‚’å®Ÿè¡Œ
        const shouldHideInAudioMode = audioEnabled && generationSettings.audio.enabled
        
        // ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ãªã„
        if (response.content && 
            response.content.trim() !== '' && 
            response.content !== '{}' && 
            response.content !== '[]' && 
            response.content !== 'null' && 
            response.content !== 'undefined' &&
            response.content !== '[object Object]' &&
            response.content !== '[object Array]' &&
            !/^\s*\{\s*\}\s*$/.test(response.content) &&
            !/^\s*\[\s*\]\s*$/.test(response.content) &&
            !response.content.includes('[object Object]') &&
            !response.content.includes('[object Array]') &&
            !response.content.includes('{}') &&
            !response.content.includes('[]') &&
            !shouldHideInAudioMode) {
          
          // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
          const assistantMessage: ChatMessageProps = {
            id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
            content: response.content,
            role: 'assistant',
            images: response.images, // ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            metadata: response.agentType && response.agentType !== 'file_processor' ? {
              agentType: response.agentType,
              confidence: response.confidence || analysis.confidence,
              executionTime: response.executionTime,
              complexity: analysis.complexity,
              title: `${response.agentType} Agent Execution`
            } : {
              title: 'Chat Response Generation'
            }
          }
          
          console.log('âœ… Adding Router Agent message:', assistantMessage)
          setMessages(prev => [...prev, assistantMessage])
        } else {
          if (shouldHideInAudioMode) {
            console.log('ğŸµ Audio mode active - hiding text response for TTS processing')
          } else {
            console.log('âŒ Skipping empty Router Agent response:', response.content)
          }
        }
        
        // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ä»˜ãTTSå‡¦ç†ã‚’å®Ÿè¡Œ
        if (audioEnabled && generationSettings.audio.enabled) {
          console.log('ğŸµ Audio enabled, performing TTS processing with text extraction')
          await processTTSWithTextExtraction(content)
        }
        
        return
      }
      
      // é€šå¸¸ã®LLMå‡¦ç†ï¼ˆRouter AgentãŒä¸è¦ãªå ´åˆï¼‰
      setAgentInfo(null)
      
      // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã—ã€é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚’é¸æŠ
      if (currentSelectedModel && currentSelectedModel !== 'auto') {
        const [selectedProviderId, selectedModelId] = currentSelectedModel.split(':')
        
        // Ollamaã¾ãŸã¯LlamaCppãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€LLM Managerã‚’ä½¿ç”¨
        if (selectedProviderId === 'ollama' || selectedProviderId === 'llama-cpp') {
          if (llmManager) {
            // LLM Managerã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå‡¦ç†
            const response = await llmManager.processUserRequestLegacy(enhancedContent, {
              files,
              contexts,
              analysis
            })
            
            // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å†…å®¹ã‚’å–å¾—
            let responseContent = ''
            if (typeof response === 'string') {
              responseContent = response
            } else if (response && typeof response === 'object') {
              // å„ªå…ˆé †ä½: content > response > text
              responseContent = response.content || response.response || response.text || ''
              
              // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã®å ´åˆã®ãƒ­ã‚°
              if (!responseContent || responseContent.trim() === '') {
                console.warn('Empty response content detected:', {
                  content: response.content,
                  response: response.response,
                  text: response.text,
                  fullResponse: response
                })
              }
            } else {
              responseContent = 'Error: Invalid response format'
            }
            
            // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            console.log('ğŸ” Response content check:', {
              responseContent: responseContent?.substring(0, 50) + (responseContent && responseContent.length > 50 ? '...' : ''),
              isEmpty: !responseContent || responseContent.trim() === '',
              isObject: responseContent === '{}',
              isArray: responseContent === '[]',
              isNull: responseContent === 'null',
              isUndefined: responseContent === 'undefined',
              hasWhitespaceOnly: /^\s*$/.test(responseContent),
              hasWhitespaceObject: /^\s*\{\s*\}\s*$/.test(responseContent),
              hasWhitespaceArray: /^\s*\[\s*\]\s*$/.test(responseContent)
            })
            
            // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’éè¡¨ç¤ºã«ã—ã¦TTSå‡¦ç†ã®ã¿ã‚’å®Ÿè¡Œ
            const shouldHideInAudioMode = audioEnabled && generationSettings.audio.enabled
            
            // ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ãªã„
            if (responseContent && 
                responseContent.trim() !== '' && 
                responseContent !== '{}' && 
                responseContent !== '[]' && 
                responseContent !== 'null' && 
                responseContent !== 'undefined' &&
                responseContent !== '[object Object]' &&
                responseContent !== '[object Array]' &&
                !/^\s*\{\s*\}\s*$/.test(responseContent) &&
                !/^\s*\[\s*\]\s*$/.test(responseContent) &&
                !responseContent.includes('[object Object]') &&
                !responseContent.includes('[object Array]') &&
                !responseContent.includes('{}') &&
                !responseContent.includes('[]') &&
                !shouldHideInAudioMode) {
              const assistantMessage: ChatMessageProps = {
                id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
                content: responseContent,
                role: 'assistant',
                metadata: {
                  title: analysis.suggestedAgent && analysis.suggestedAgent !== 'file_processor' ? `${analysis.suggestedAgent} Agent Execution` : 'Chat Response Generation'
                }
              }
              
              console.log('âœ… Adding assistant message:', assistantMessage)
              setMessages(prev => [...prev, assistantMessage])
            } else {
              if (shouldHideInAudioMode) {
                console.log('ğŸµ Audio mode active - hiding text response for TTS processing')
              } else {
                console.log('âŒ Skipping empty/invalid response:', responseContent)
              }
            }
            
            // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯TTSå‡¦ç†ã‚’å®Ÿè¡Œï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ï¼‰
            if (audioEnabled && generationSettings.audio.enabled) {
              console.log('ğŸµ Audio enabled, performing TTS processing for user input')
              await processTTS(content, content)
            }
            
            return
          }
        } else {
          // é€šå¸¸ã®APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆã€AI SDK Serviceã‚’ä½¿ç”¨
          const apiKey = providerApiKeys[selectedProviderId]
          if (apiKey) {
            await aiSDKService.configureProvider({
              providerId: selectedProviderId,
              modelId: selectedModelId,
              apiKey
            })
          }
        }
      }
      
      // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒOllamaã¾ãŸã¯LlamaCppã§ãªã„å ´åˆã®ã¿ã€AI SDK Serviceã‚’ä½¿ç”¨
      const shouldUseAISDK = !currentSelectedModel || 
        (!currentSelectedModel.startsWith('ollama:') && !currentSelectedModel.startsWith('llama-cpp:'))
      
      if (shouldUseAISDK && aiSDKService.isProviderConfigured()) {
        // ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’å–å¾—
        const currentConfig = aiSDKService.getCurrentConfig()
        if (!currentConfig) {
          throw new Error('No provider configured')
        }

        // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®API Keyã‚’å–å¾—
        const apiKey = providerApiKeys[currentConfig.providerId]
        if (!apiKey) {
          throw new Error(`API Key not found for provider: ${currentConfig.providerId}`)
        }

        // API Keyã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å†è¨­å®š
        await aiSDKService.configureProvider({
          ...currentConfig,
          apiKey: apiKey
        })

        // ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯Gemini APIã‚’ä½¿ç”¨
        if (files && files.length > 0 && currentConfig.providerId === 'google') {
          await handleFileChatRequest(enhancedContent, contexts, files, apiKey)
          return
        }

        // é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒƒãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰
        const chatHistory = messages.map(msg => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: msg.content
        }))

        // æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        const updatedHistory = [
          ...chatHistory,
          { role: 'user' as const, content: enhancedContent }
        ]

        // ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆï¼‰
        if (contexts && contexts.length > 0) {
          const systemMessage = {
            role: 'system' as const,
            content: `Context: ${contexts.map(ctx => `@${ctx.name}`).join(', ')}`
          }
          updatedHistory.unshift(systemMessage)
        }

        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        const assistantMessageId = (Date.now() + 1).toString()
        const assistantMessage: ChatMessageProps = {
          id: assistantMessageId,
          content: '', // ç©ºæ–‡å­—åˆ—ã«è¨­å®š
          role: 'assistant',
          metadata: {
            title: analysis.suggestedAgent && analysis.suggestedAgent !== 'file_processor' ? `${analysis.suggestedAgent} Agent Execution` : 'Chat Response Generation'
          }
        }
        
        setStreamingResponse('')

                // ã‚¿ã‚¹ã‚¯é€²æ—ã‚’æ›´æ–°
        if (taskId) {
          updateTaskProgress(taskId, 50, 'AIãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†', 1)
        }

        // AI SDK UIã®readUIMessageStreamã‚’ä½¿ç”¨ã—ãŸæ–°ã—ã„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ©Ÿèƒ½
        console.log('ğŸš€ Starting AI streaming...')
        setIsTyping(true) // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹æ™‚ã«isTypingã‚’trueã«è¨­å®š
        
        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹æ™‚ã«ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ãªã„ï¼ˆç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        // ä»£ã‚ã‚Šã«ã€æœ€åˆã®æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ãŒåˆ°ç€ã—ãŸæ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹
        
        let lastFullResponse = '' // é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        let hasAddedMessage = false // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ ãƒ•ãƒ©ã‚°
        let chunkCount = 0 // ãƒãƒ£ãƒ³ã‚¯ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        
        console.log('ğŸš€ Gemini 2.5 Flash Lite ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹')
        
        await aiSDKService.streamUIMessageResponse(
          updatedHistory,
          (chunk: string) => {
            chunkCount++
            
            // ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã‚„ç„¡åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (!chunk || chunk.trim() === '' || chunk === '{}' || chunk === '[]' || chunk === 'null' || chunk === 'undefined') {
              console.log(`ğŸ”„ ç„¡åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ— (ãƒãƒ£ãƒ³ã‚¯ ${chunkCount}):`, chunk)
              return
            }
            
            // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
            setStreamingResponse(prev => {
              const newResponse = prev + chunk
              
              // é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼šå‰å›ã¨åŒã˜ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
              if (newResponse === lastFullResponse) {
                console.log(`ğŸ”„ é‡è¤‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ— (ãƒãƒ£ãƒ³ã‚¯ ${chunkCount})`)
                return prev
              }
              lastFullResponse = newResponse
              
              console.log(`ğŸ“ ãƒãƒ£ãƒ³ã‚¯ ${chunkCount} å‡¦ç†:`, {
                chunkLength: chunk.length,
                totalLength: newResponse.length,
                chunkPreview: chunk.substring(0, 30) + (chunk.length > 30 ? '...' : '')
              })
              
              // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
              console.log('ğŸ” Streaming chunk check:', {
                chunk: chunk?.substring(0, 30) + (chunk && chunk.length > 30 ? '...' : ''),
                newResponse: newResponse?.substring(0, 50) + (newResponse && newResponse.length > 50 ? '...' : ''),
                isEmpty: !newResponse || newResponse.trim() === '',
                isObject: newResponse === '{}',
                isArray: newResponse === '[]',
                isNull: newResponse === 'null',
                isUndefined: newResponse === 'undefined',
                hasWhitespaceOnly: /^\s*$/.test(newResponse),
                hasWhitespaceObject: /^\s*\{\s*\}\s*$/.test(newResponse),
                hasWhitespaceArray: /^\s*\[\s*\]\s*$/.test(newResponse)
              })
              
              // AudioãŒæœ‰åŠ¹ã§generalã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§moderateè¤‡é›‘åº¦ã®å ´åˆã¯ã€TTSå‡¦ç†ä¸­ã¯UIã«è¡¨ç¤ºã—ãªã„
              const shouldHideInAudioMode = audioEnabled && generationSettings.audio.enabled
              
              // ç©ºã§ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã®ã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°ã¾ãŸã¯è¿½åŠ ï¼ˆä¸€åº¦ã ã‘ï¼‰
              if (newResponse && 
                  newResponse.trim() !== '' && 
                  newResponse !== '{}' && 
                  newResponse !== '[]' && 
                  newResponse !== 'null' && 
                  newResponse !== 'undefined' &&
                  newResponse !== '[object Object]' &&
                  newResponse !== '[object Array]' &&
                  !/^\s*\{\s*\}\s*$/.test(newResponse) &&
                  !/^\s*\[\s*\]\s*$/.test(newResponse) &&
                  !newResponse.includes('[object Object]') &&
                  !newResponse.includes('[object Array]') &&
                  !newResponse.includes('{}') &&
                  !newResponse.includes('[]') &&
                  !hasAddedMessage &&
                  !shouldHideInAudioMode &&
                  chunk.length > 0) { // ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                
                hasAddedMessage = true
                
                // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ ã‚’éåŒæœŸã§å®Ÿè¡Œï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ããŸã‚ï¼‰
                setTimeout(() => {
                  setMessages(prevMessages => {
                    // æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    const existingMessage = prevMessages.find(msg => msg.id === assistantMessageId)
                    
                    if (existingMessage) {
                      // æ—¢å­˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
                      return prevMessages.map(msg => 
                        msg.id === assistantMessageId 
                          ? { ...msg, content: newResponse }
                          : msg
                      )
                    } else {
                      // æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                      return [...prevMessages, { ...assistantMessage, content: newResponse }]
                    }
                  })
                }, 0)
              }
              
              return newResponse
            })
          },
          (fullResponse: string) => {
            // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã®å‡¦ç†
            console.log('âœ… AI streaming completed')
            
            // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            console.log('ğŸ” Streaming completion check:', {
              fullResponse: fullResponse?.substring(0, 50) + (fullResponse && fullResponse.length > 50 ? '...' : ''),
              isEmpty: !fullResponse || fullResponse.trim() === '',
              isObject: fullResponse === '{}',
              isArray: fullResponse === '[]',
              isNull: fullResponse === 'null',
              isUndefined: fullResponse === 'undefined',
              hasWhitespaceOnly: /^\s*$/.test(fullResponse),
              hasWhitespaceObject: /^\s*\{\s*\}\s*$/.test(fullResponse),
              hasWhitespaceArray: /^\s*\[\s*\]\s*$/.test(fullResponse)
            })
            
            // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’éè¡¨ç¤ºã«ã—ã¦TTSå‡¦ç†ã®ã¿ã‚’å®Ÿè¡Œ
            const shouldHideInAudioMode = audioEnabled && generationSettings.audio.enabled
            
            // ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
            if (!fullResponse || 
                fullResponse.trim() === '' || 
                fullResponse.length === 0 ||
                fullResponse === '{}' ||
                fullResponse === '[]' ||
                fullResponse === 'null' ||
                fullResponse === 'undefined' ||
                fullResponse === '[object Object]' ||
                fullResponse === '[object Array]' ||
                fullResponse.replace(/\s/g, '').length === 0 ||
                /^\s*\{\s*\}\s*$/.test(fullResponse) ||
                /^\s*\[\s*\]\s*$/.test(fullResponse) ||
                fullResponse.includes('[object Object]') ||
                fullResponse.includes('[object Array]') ||
                fullResponse.includes('{}') ||
                fullResponse.includes('[]') ||
                shouldHideInAudioMode) {
              if (shouldHideInAudioMode) {
                console.log('ğŸµ Audio mode active - removing text response for TTS processing')
              } else {
                console.log('ğŸ—‘ï¸ Removing empty/invalid response message')
              }
              setMessages(prev => prev.filter(msg => msg.id !== assistantMessageId))
            } else {
              // ç©ºã§ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã®ã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
              console.log('âœ… Updating message with response:', fullResponse.substring(0, 50) + '...')
              setMessages(prev => 
                prev.map(msg => 
                  msg.id === assistantMessageId 
                    ? { ...msg, content: fullResponse }
                    : msg
                )
              )
            }
            setStreamingResponse('')
            setIsTyping(false) // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã«isTypingã‚’falseã«è¨­å®š
            console.log('ğŸ”„ Streaming completed: setting isGenerating to false')
            setIsGenerating(false)
            
            // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºå®Ÿã«å‰Šé™¤
            setMessages(prev => prev.filter(msg => 
              msg.content && 
              msg.content.trim() !== '' && 
              msg.content.length > 0 &&
              msg.content !== ' ' &&
              msg.content !== '\n' &&
              msg.content !== '\t' &&
              msg.content !== 'Chat Response Generation' &&
              msg.content !== 'Audio Mode Active' &&
              msg.content !== '{}' &&
              msg.content !== '[]' &&
              msg.content !== 'null' &&
              msg.content !== 'undefined' &&
              msg.content !== '[object Object]' &&
              msg.content !== '[object Array]' &&
              !/^\s*$/.test(msg.content) &&
              msg.content.replace(/\s/g, '').length > 0 &&
              !/^\s*\{\s*\}\s*$/.test(msg.content) &&
              !/^\s*\[\s*\]\s*$/.test(msg.content) &&
              !msg.content.includes('[object Object]') &&
              !msg.content.includes('[object Array]') &&
              !msg.content.includes('{}') &&
              !msg.content.includes('[]')
            ))
            
            // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ
            if (factCheckingSettings.enabled && factCheckingSettings.autoCheck && fullResponse) {
              // éåŒæœŸã§ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
              performFactCheck(fullResponse).then(factCheckResult => {
                if (factCheckResult && !factCheckResult.isFactual) {
                  // ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã§å•é¡ŒãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                  const warningMessage: ChatMessageProps = {
                    id: `fact-check-${Date.now()}`,
                    content: `**Fact Check Warning**: This response may contain inaccuracies.\n\n**Issues Found**:\n${factCheckResult.issues.map(issue => `â€¢ ${issue}`).join('\n')}\n\n**Confidence**: ${factCheckResult.confidence}%\n\n**Explanation**: ${factCheckResult.explanation}`,
                    role: 'assistant',
                    metadata: {
                      title: 'Fact Check Warning'
                    }
                  }
                  setMessages(prev => [...prev, warningMessage])
                }
              }).catch(error => {
                console.error('Fact check failed:', error)
              })
            }
            
            // TTSå‡¦ç†ã‚’å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½ä»˜ãï¼‰
            if (audioEnabled && generationSettings.audio.enabled) {
              processTTSWithTextExtraction(content)
            }
            
            // ã‚¿ã‚¹ã‚¯å®Œäº†
            if (taskId) {
              completeTask(taskId, true)
            }
          },
          (error: Error) => {
            // ã‚¨ãƒ©ãƒ¼å‡¦ç†
            console.error('âŒ Streaming error:', error)
            
            // AudioãŒæœ‰åŠ¹ãªå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚è¡¨ç¤ºã—ãªã„
            const shouldHideInAudioMode = audioEnabled && generationSettings.audio.enabled
            
            if (!shouldHideInAudioMode) {
              setMessages(prev => 
                prev.map(msg => 
                  msg.id === assistantMessageId 
                    ? { ...msg, content: `âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${error.message}` }
                    : msg
                )
              )
            } else {
              console.log('ğŸµ Audio mode active - hiding error message for TTS processing')
              // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
              setMessages(prev => prev.filter(msg => msg.id !== assistantMessageId))
            }
            
            setStreamingResponse('')
            setLastError(error.message)
            setIsTyping(false) // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚isTypingã‚’falseã«è¨­å®š
            console.log('ğŸ”„ Streaming error: setting isGenerating to false')
            setIsGenerating(false)
            
            // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºå®Ÿã«å‰Šé™¤
            setMessages(prev => prev.filter(msg => 
              msg.content && 
              msg.content.trim() !== '' && 
              msg.content.length > 0 &&
              msg.content !== ' ' &&
              msg.content !== '\n' &&
              msg.content !== '\t' &&
              msg.content !== 'Chat Response Generation' &&
              msg.content !== 'Audio Mode Active' &&
              msg.content !== '{}' &&
              msg.content !== '[]' &&
              msg.content !== 'null' &&
              msg.content !== 'undefined' &&
              msg.content !== '[object Object]' &&
              msg.content !== '[object Array]' &&
              !/^\s*$/.test(msg.content) &&
              msg.content.replace(/\s/g, '').length > 0 &&
              !/^\s*\{\s*\}\s*$/.test(msg.content) &&
              !/^\s*\[\s*\]\s*$/.test(msg.content) &&
              !msg.content.includes('[object Object]') &&
              !msg.content.includes('[object Array]') &&
              !msg.content.includes('{}') &&
              !msg.content.includes('[]')
            ))
            
            // ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼
            if (taskId) {
              completeTask(taskId, false)
            }
          }
        )
      } else {
        // Fallback to mock response
        const mockResponse = `I understand you're asking about: "${content}". However, I need to be configured with an AI provider to provide meaningful responses. Please configure an AI provider in the settings.`
        
        const assistantMessage: ChatMessageProps = {
          id: (Date.now() + 1).toString(),
          content: mockResponse,
          role: 'assistant',
          metadata: {
            title: 'Chat Response Generation'
          }
        }
        setMessages(prev => [...prev, assistantMessage])
        setLastError('No AI provider configured')
        setIsGenerating(false)
      }
    } catch (error) {
      console.error('Error in handleRegularChatRequest:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred'
      setLastError(errorMessage)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      const errorResponse: ChatMessageProps = {
        id: (Date.now() + 1).toString(),
        content: `âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${errorMessage}`,
        role: 'assistant',
        metadata: {
          title: 'Chat Response Generation'
        }
      }
      setMessages(prev => [...prev, errorResponse])
      setIsGenerating(false)
    } finally {
      console.log('ğŸ”„ Finally block: setting isTyping to false, isGenerating to false and loadingState to idle')
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // YouTubeå‹•ç”»ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
  const handleYouTubeVideoRequest = async (content: string, contexts?: Array<{id: string, type: string, name: string, value?: string}>) => {
    try {
      // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
      if (audioEnabled && generationSettings.audio.enabled) {
        // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¿½åŠ ã—ãªã„
        return
      }

      if (aiSDKService.isProviderConfigured()) {
        // ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’å–å¾—
        const currentConfig = aiSDKService.getCurrentConfig()
        if (!currentConfig) {
          throw new Error('No provider configured')
        }

        // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®API Keyã‚’å–å¾—
        const apiKey = providerApiKeys[currentConfig.providerId]
        if (!apiKey) {
          throw new Error(`API Key not found for provider: ${currentConfig.providerId}`)
        }

        // API Keyã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å†è¨­å®š
        await aiSDKService.configureProvider({
          ...currentConfig,
          apiKey: apiKey
        })

        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        const assistantMessageId = (Date.now() + 1).toString()
        const assistantMessage: ChatMessageProps = {
          id: assistantMessageId,
          content: '',
          role: 'assistant',
        }
        
        // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«è¿½åŠ ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºç”¨ï¼‰
        setMessages(prev => [...prev, assistantMessage])

        // GeminiFileServiceã‚’åˆæœŸåŒ–
        const geminiFileService = new GeminiFileService()
        await geminiFileService.configure(apiKey, 'gemini-1.5-flash')

        // YouTubeå‹•ç”»ã«ã¤ã„ã¦ãƒãƒ£ãƒƒãƒˆ
        const question = content.trim() || 'ã“ã®å‹•ç”»ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„'
        const chatResponse = await geminiFileService.chatAboutVideo(content.trim(), question)

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
        const fullResponse = chatResponse.text
        setMessages(prev => 
          prev.map(msg => 
            msg.id === assistantMessageId 
              ? { ...msg, content: fullResponse }
              : msg
          )
        )
        setStreamingResponse('')
        setIsGenerating(false)
        
        // TTSå‡¦ç†ã¯æœ€å¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯å®Ÿè¡Œã—ãªã„

      } else {
        // Fallback to mock response
        const mockResponse = `I understand you want to analyze a YouTube video: "${content}". However, I need to be configured with an AI provider to provide meaningful responses. Please configure an AI provider in the settings.`
        
        const assistantMessage: ChatMessageProps = {
          id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          content: mockResponse,
          role: 'assistant',
        }
        setMessages(prev => [...prev, assistantMessage])
        
        // TTSå‡¦ç†ã¯æœ€å¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯å®Ÿè¡Œã—ãªã„
        
        setLastError('No AI provider configured')
        setIsGenerating(false)
      }
    } catch (error) {
      console.error('Error in handleYouTubeVideoRequest:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred'
      setLastError(errorMessage)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      const errorResponse: ChatMessageProps = {
        id: (Date.now() + 1).toString(),
        content: `âŒ YouTubeå‹•ç”»åˆ†æã‚¨ãƒ©ãƒ¼: ${errorMessage}`,
        role: 'assistant',
      }
      setMessages(prev => [...prev, errorResponse])
      setStreamingResponse('')
      setIsGenerating(false)
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  // ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
  const handleFileChatRequest = async (content: string, contexts?: Array<{id: string, type: string, name: string, value?: string}>, files?: File[], apiKey?: string) => {
    if (!files || files.length === 0 || !apiKey) {
      throw new Error('Files and API key are required for file chat')
    }

    // AudioãŒã‚ªãƒ³ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
    if (audioEnabled && generationSettings.audio.enabled) {
      // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¿½åŠ ã—ãªã„
      return
    }

    try {
      // GeminiFileServiceã‚’åˆæœŸåŒ–
      const geminiFileService = new GeminiFileService()
      await geminiFileService.configure(apiKey, 'gemini-1.5-flash')

      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
      const assistantMessageId = (Date.now() + 1).toString()
      const assistantMessage: ChatMessageProps = {
        id: assistantMessageId,
        content: '',
        role: 'assistant',
      }
      
      // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å³åº§ã«è¿½åŠ ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºç”¨ï¼‰
      setMessages(prev => [...prev, assistantMessage])

      // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
      const uploadPromises = files.map(async (file, index) => {
        try {
          const fileType = file.type.startsWith('video/') ? 'ğŸ¥' : 
                          file.type.startsWith('image/') ? 'ğŸ–¼ï¸' : 
                          file.type.startsWith('audio/') ? 'ğŸµ' : 'ğŸ“„'
          
          // å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
          if (file.type.startsWith('video/')) {
            const fileSizeMB = file.size / 1024 / 1024
            if (fileSizeMB > 15) {
            }
          }
          
          // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
          if (file.type.startsWith('audio/')) {
            const fileSizeMB = file.size / 1024 / 1024
            if (fileSizeMB > 15) {
            }
          }
          
          // Fileã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          const uploadResponse = await geminiFileService.uploadFileObject(file, file.name)
          
          console.log(`File uploaded successfully: ${file.name}`)
          return uploadResponse.file.uri
        } catch (error) {
          console.error(`Failed to upload file ${file.name}:`, error)
          
          // å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
          if (file.type.startsWith('video/')) {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error'
            if (errorMessage.includes('å‹•ç”»ãŒé•·ã™ãã¾ã™')) {
              throw new Error(`å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ãŒé•·ã™ãã¾ã™ã€‚æœ€å¤§60ç§’ã¾ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚`)
            } else if (errorMessage.includes('å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™')) {
              throw new Error(`å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ãŒå¤§ãã™ãã¾ã™ã€‚æœ€å¤§50MBã¾ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚`)
            } else if (errorMessage.includes('å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ')) {
              throw new Error(`å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹ã‹ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`)
            }
          }
          
          // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
          if (file.type.startsWith('audio/')) {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error'
            if (errorMessage.includes('éŸ³å£°ãŒé•·ã™ãã¾ã™')) {
              throw new Error(`éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ãŒé•·ã™ãã¾ã™ã€‚æœ€å¤§30åˆ†ã¾ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚`)
            } else if (errorMessage.includes('éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™')) {
              throw new Error(`éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ãŒå¤§ãã™ãã¾ã™ã€‚æœ€å¤§50MBã¾ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚`)
            } else if (errorMessage.includes('éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ')) {
              throw new Error(`éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« "${file.name}" ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹ã‹ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`)
            }
          }
          
          throw new Error(`Failed to upload file ${file.name}: ${error instanceof Error ? error.message : 'Unknown error'}`)
        }
      })

      const fileUris = await Promise.all(uploadPromises)
      
      const question = content.trim() || 'ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„'
      
      let chatResponse
      if (files.length === 1) {
        // å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        const file = files[0]
        
        if (file.type.startsWith('video/')) {
          // å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
          chatResponse = await geminiFileService.chatAboutVideo(fileUris[0], question)
        } else if (file.type.startsWith('audio/')) {
          // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
          chatResponse = await geminiFileService.chatAboutAudio(fileUris[0], question)
        } else {
          // ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«
          chatResponse = await geminiFileService.chatAboutFile(fileUris[0], question)
        }
      } else {
        // è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼ˆç¾åœ¨ã¯æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ï¼‰
        console.log(`Multiple files detected (${files.length}), processing first file: ${files[0].name}`)
        const firstFile = files[0]
        
        if (firstFile.type.startsWith('video/')) {
          chatResponse = await geminiFileService.chatAboutVideo(fileUris[0], question)
        } else if (firstFile.type.startsWith('audio/')) {
          chatResponse = await geminiFileService.chatAboutAudio(fileUris[0], question)
        } else {
          chatResponse = await geminiFileService.chatAboutFile(fileUris[0], question)
        }
      }

      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
      const fullResponse = chatResponse.text
      setMessages(prev => 
        prev.map(msg => 
          msg.id === assistantMessageId 
            ? { ...msg, content: fullResponse }
            : msg
        )
      )
      setStreamingResponse('')
      setIsGenerating(false)
      
      // TTSå‡¦ç†ã¯æœ€å¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯å®Ÿè¡Œã—ãªã„

    } catch (error) {
      console.error('Error in handleFileChatRequest:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred'
      setLastError(errorMessage)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      const errorResponse: ChatMessageProps = {
        id: (Date.now() + 1).toString(),
        content: `âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${errorMessage}`,
        role: 'assistant',
      }
      setMessages(prev => [...prev, errorResponse])
      setStreamingResponse('')
      setIsGenerating(false)
    } finally {
      setIsTyping(false)
      setIsGenerating(false)
      setLoadingState('idle')
    }
  }

  const handleFilesAdded = (files: File[]) => {
    setSelectedFiles(prev => [...prev, ...files])
  }

  const handleTranscriptionComplete = (text: string) => {
    // æ–‡å­—èµ·ã“ã—çµæœã‚’ãƒãƒ£ãƒƒãƒˆã«é€ä¿¡
    handleSendMessage(`éŸ³å£°ã®æ–‡å­—èµ·ã“ã—çµæœ:\n\n${text}`)
  }

  const handleAttachFiles = () => {
    // This function is no longer needed as we use direct file selection
  }

  const handleCancelSequentialThinking = () => {
    setShowSequentialThinking(false)
    setCurrentPlan(null)
    setIsTyping(false)
    setIsAgentMode(false)
  }

  const handleProviderSelect = async (config: AIProviderConfig) => {
    try {
      setLastError(null)
      
      await aiSDKService.configureProvider(config)
      setCurrentProviderConfig(config)
      
      console.log(`Provider configured: ${config.providerId} with model: ${config.modelId}`)
    } catch (error) {
      console.error('Failed to configure provider:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error'
      setLastError(errorMessage)
    }
  }

  // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡
  const [currentSelectedModel, setCurrentSelectedModel] = useState<string | null>(null)

  // ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—å‡¦ç†é–¢æ•°
  const handleModelDownloadProgress = (progress: any, modelId?: string) => {
    if (progress && typeof progress === 'object') {
      console.log('ğŸ“Š Received progress:', progress)
      console.log('ğŸ“Š Progress keys:', Object.keys(progress))
      console.log('ğŸ“Š actualModelId:', progress.actualModelId)
      console.log('ğŸ“Š originalModelId:', progress.originalModelId)
      console.log('ğŸ“Š modelId:', progress.modelId)
      console.log('ğŸ“Š message:', progress.message)
      console.log('ğŸ“Š Input modelId parameter:', modelId)
      
      // é€²æ—æƒ…å ±ã®æ§‹é€ ã‚’çµ±ä¸€ï¼ˆollama-service.tsã‹ã‚‰ã®æ–°ã—ã„å½¢å¼ã«å¯¾å¿œï¼‰
      const status = progress.status || (progress.message?.includes('completed') ? 'completed' : 'downloading')
      const progressValue = progress.progress || progress.percentage || 0
      const downloaded = progress.downloaded || 0
      const total = progress.total || 0
      
      console.log(`ğŸ“Š Parsed progress: status=${status}, progress=${progressValue}, downloaded=${downloaded}, total=${total}`)
      
      // ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡ºï¼ˆproviderId:modelId ã®å½¢å¼ã‹ã‚‰ modelId ã®ã¿ã‚’å–å¾—ï¼‰
      let modelName = 'Unknown Model'
      const targetModelId = modelId || currentSelectedModel
      if (targetModelId) {
        const parts = targetModelId.split(':')
        const modelIdPart = parts.length > 1 ? parts[1] : targetModelId
        
        // å®Ÿéš›ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        // progress.modelId ã¾ãŸã¯ progress.message ã‹ã‚‰å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡º
        let actualModelName = modelIdPart
        
        // ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‹ã‚‰å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡º
        if (progress.actualModelId) {
          // LLM Managerã‹ã‚‰æä¾›ã•ã‚ŒãŸå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDã‚’ä½¿ç”¨
          const actualModelId = progress.actualModelId
          
          // å®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰è¡¨ç¤ºç”¨ãƒ¢ãƒ‡ãƒ«åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
          const actualModelMapping: { [key: string]: string } = {
            'unsloth/Qwen3-0.6B-GGUF': 'qwen3:0.6b',
            'unsloth/Qwen3-1.7B-GGUF': 'qwen3:1.7b',
            'unsloth/Qwen3-4B-Instruct-2507-GGUF': 'qwen3:4b',
            'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF': 'qwen3:8b',
            'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF': 'qwen3:30b',
            'unsloth/gpt-oss-20b-GGUF': 'gpt-oss-20b',
            'TheBloke/Llama-2-7B-Chat-GGUF': 'llama-2-7b-chat',
            'TheBloke/Llama-2-13B-Chat-GGUF': 'llama-2-13b-chat',
            'TheBloke/Mistral-7B-Instruct-v0.2-GGUF': 'mistral-7b-instruct',
            'TheBloke/Qwen2-7B-Instruct-GGUF': 'qwen2-7b-instruct',
            'TheBloke/CodeLlama-7B-Instruct-GGUF': 'codellama-7b-instruct',
            'TheBloke/Gemma-2-9B-IT-GGUF': 'gemma-2-9b-it'
          }
          
          actualModelName = actualModelMapping[actualModelId] || actualModelId
        } else if (progress.modelId) {
          // Ollamaãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€modelIdãŒå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å
          actualModelName = progress.modelId
        } else if (progress.message) {
          // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡º
          const message = progress.message
          
          // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…ã®å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰è¡¨ç¤ºç”¨ãƒ¢ãƒ‡ãƒ«åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
          const messageModelMapping: { [key: string]: string } = {
            'unsloth/Qwen3-0.6B-GGUF': 'qwen3:0.6b',
            'unsloth/Qwen3-1.7B-GGUF': 'qwen3:1.7b',
            'unsloth/Qwen3-4B-Instruct-2507-GGUF': 'qwen3:4b',
            'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF': 'qwen3:8b',
            'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF': 'qwen3:30b',
            'unsloth/gpt-oss-20b-GGUF': 'gpt-oss-20b',
            'TheBloke/Llama-2-7B-Chat-GGUF': 'llama-2-7b-chat',
            'TheBloke/Llama-2-13B-Chat-GGUF': 'llama-2-13b-chat',
            'TheBloke/Mistral-7B-Instruct-v0.2-GGUF': 'mistral-7b-instruct',
            'TheBloke/Qwen2-7B-Instruct-GGUF': 'qwen2-7b-instruct',
            'TheBloke/CodeLlama-7B-Instruct-GGUF': 'codellama-7b-instruct',
            'TheBloke/Gemma-2-9B-IT-GGUF': 'gemma-2-9b-it'
          }
          
          // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…ã§ãƒãƒƒãƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«IDã‚’æ¤œç´¢
          for (const [modelId, displayName] of Object.entries(messageModelMapping)) {
            if (message.includes(modelId)) {
              actualModelName = displayName
              break
            }
          }
          
          // ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®ãƒ¢ãƒ‡ãƒ«åã‚’ä½¿ç”¨
          if (!actualModelName) {
            actualModelName = modelIdPart
          }
        }
        
        // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ­ã‚°å‡ºåŠ›
        console.log(`ğŸ“Š Debug - actualModelName: ${actualModelName}, modelIdPart: ${modelIdPart}, progress.modelId: ${progress.modelId}, progress.message: ${progress.message}`)
        
        // ãƒ¢ãƒ‡ãƒ«åã‚’ã‚ˆã‚Šè©³ç´°ã«è¡¨ç¤º
        const modelNameMapping: { [key: string]: string } = {
          'qwen3': 'Qwen3 4B Instruct',
          'qwen3:0.6b': 'Qwen3 0.6B',
          'qwen3-0.6b': 'Qwen3 0.6B',
          'qwen3:1.7b': 'Qwen3 1.7B',
          'qwen3-1.7b': 'Qwen3 1.7B',
          'qwen3-4b': 'Qwen3 4B Instruct',
          'qwen3-8b': 'Qwen3 8B',
          'qwen3-30b': 'Qwen3 30B Instruct',
          'gpt-oss-20b': 'GPT-OSS-20B',
          'gpt-oss-20b-GGUF': 'GPT-OSS-20B',
          'gemma3:1b': 'Gemma3 1B',
          'gemma-2-9b-it': 'Gemma 2 9B IT',
          'llama-2-7b-chat': 'Llama 2 7B Chat',
          'llama-2-13b-chat': 'Llama 2 13B Chat',
          'mistral-7b-instruct': 'Mistral 7B Instruct',
          'qwen2-7b-instruct': 'Qwen2 7B Instruct',
          'codellama-7b-instruct': 'CodeLlama 7B Instruct'
        }
        
        modelName = modelNameMapping[actualModelName] || actualModelName
      }
      
      console.log(`ğŸ“Š Setting model download state: ${modelName}, progress: ${progressValue}%, downloaded: ${downloaded}, total: ${total}`)
      
      const newState = {
        isDownloading: status === 'starting' || status === 'downloading' || status === 'verifying',
        modelName: modelName,
        progress: progressValue,
        status: status,
        downloadedBytes: downloaded,
        totalBytes: total
      }
      
      console.log(`ğŸ“Š New model download state:`, newState)
      setModelDownloadState(newState)
      
      // modelDownloadStateãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€downloadProgressã‚’ã‚¯ãƒªã‚¢
      if (newState.isDownloading) {
        setDownloadProgress(null)
      }
      
      // ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å°‘ã—é…å»¶ã—ã¦ã‹ã‚‰çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      if (status === 'completed' || status === 'error') {
        setTimeout(() => {
          setModelDownloadState({
            isDownloading: false,
            modelName: '',
            progress: 0,
            status: 'starting',
            downloadedBytes: undefined,
            totalBytes: undefined
          })
        }, 3000) // 3ç§’å¾Œã«ãƒªã‚»ãƒƒãƒˆ
      }
    }
  }

  // ãƒ¢ãƒ‡ãƒ«é¸æŠãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  const handleModelSelect = async (modelId: string) => {
    try {
      setLastError(null)
      
      if (modelId === 'auto') {
        // Auto mode: åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠ
        console.log('Auto mode selected - finding best available model...')
        await autoConfigureProvider()
        return
      }

      // TTSãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
      if (modelId.startsWith('tts:')) {
        const ttsModelId = modelId.replace('tts:', '')
        console.log(`TTS model selected: ${ttsModelId}`)
        
        // TTSãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦é©åˆ‡ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ±ºå®š
        let provider = 'google' // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if (ttsModelId.startsWith('inworld-tts')) {
          provider = 'inworld'
        } else if (ttsModelId.startsWith('gpt-')) {
          provider = 'openai'
        } else if (ttsModelId.startsWith('gemini-')) {
          provider = 'google'
        }
        
        // TTSãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¨­å®šç”»é¢ã§æœ‰åŠ¹ã«ã™ã‚‹
        setGenerationSettings((prev: any) => {
          const newSettings = {
            ...prev,
            audio: {
              ...prev.audio,
              provider: provider, // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ›´æ–°
              models: {
                ...prev.audio.models,
                [ttsModelId]: {
                  ...prev.audio.models[ttsModelId],
                  enabled: true
                }
              }
            }
          }
          
          // ä»–ã®TTSãƒ¢ãƒ‡ãƒ«ã‚’ç„¡åŠ¹ã«ã™ã‚‹
          Object.keys(newSettings.audio.models).forEach(key => {
            if (key !== ttsModelId) {
              newSettings.audio.models[key].enabled = false
            }
          })
          
          return newSettings
        })
        
        // ç¾åœ¨ã®TTSãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
        setCurrentTTSModel(ttsModelId)
        localStorage.setItem('armis_tts_model', ttsModelId)
        
        console.log(`TTS model activated: ${ttsModelId} with provider: ${provider}`)
        return
      }

      // STTãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
      if (modelId.startsWith('stt:')) {
        const sttModelId = modelId.replace('stt:', '')
        console.log(`STT model selected: ${sttModelId}`)
        
        // STTè¨­å®šã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—
        const sttService = STTSettingsService.getInstance()
        
        // é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æœ‰åŠ¹ã«ã™ã‚‹
        sttService.toggleModel(sttModelId)
        
        // ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç„¡åŠ¹ã«ã™ã‚‹
        const settings = sttService.getSettings()
        Object.keys(settings.models).forEach(key => {
          if (key !== sttModelId) {
            settings.models[key].enabled = false
          }
        })
        
        // è¨­å®šã‚’ä¿å­˜
        sttService.updateSettings({ models: settings.models })
        
        // ç¾åœ¨ã®STTãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
        setCurrentSTTModel(sttModelId)
        localStorage.setItem('armis_stt_model', sttModelId)
        
        console.log(`STT model activated: ${sttModelId}`)
        return
      }

      // modelIdã®å½¢å¼: "providerId:modelId" (modelIdã«:ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹)
      const firstColonIndex = modelId.indexOf(':')
      if (firstColonIndex === -1) {
        console.error('Invalid model ID format:', modelId)
        return
      }
      
      const providerId = modelId.substring(0, firstColonIndex)
      const selectedModelId = modelId.substring(firstColonIndex + 1)
      
      if (!providerId || !selectedModelId) {
        console.error('Invalid model ID format:', modelId)
        return
      }

      // ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
      setCurrentSelectedModel(modelId)

      // Ollamaã¾ãŸã¯LlamaCppãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨
      if (providerId === 'ollama' || providerId === 'llama-cpp') {
        if (!llmManager) {
          setLastError('LLM Manager not available')
          return
        }

        try {
          setIsDownloading(true)
          setDownloadProgress({ status: 'checking', message: 'Checking model availability...' })

          // ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          await llmManager.switchToModel(selectedModelId, (progress) => {
            console.log('ğŸ”„ LLM Manager progress callback received:', progress)
            
            // ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé–‹å§‹ã•ã‚ŒãŸã‚‰ã€ŒChecking model availability...ã€ã‚’éè¡¨ç¤ºã«ã™ã‚‹
            if (progress && progress.status === 'downloading') {
              setDownloadProgress(null)
            }
            // é€²è¡ŒçŠ¶æ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿æ›´æ–°ï¼ˆcheckingçŠ¶æ…‹ã®é‡è¤‡ã‚’é˜²ãï¼‰
            if (progress && (progress.status === 'starting' || progress.status === 'downloading' || progress.status === 'verifying' || progress.status === 'completed' || progress.status === 'error')) {
              console.log('ğŸ“Š Calling handleModelDownloadProgress with:', progress)
              handleModelDownloadProgress(progress, modelId)
            }
            console.log('Download progress:', progress)
            console.log('Progress status:', progress?.status)
            console.log('Progress message:', progress?.message)
          })

          setDownloadProgress({ status: 'completed', message: 'Model ready!' })
          console.log(`Model selected and ready: ${providerId}:${selectedModelId}`)
        } catch (error) {
          console.error('Failed to switch to model:', error)
          setLastError(`Failed to switch to model: ${error}`)
        } finally {
          setIsDownloading(false)
          setDownloadProgress(null)
        }
        return
      }

      // é€šå¸¸ã®APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆ
      const apiKey = providerApiKeys[providerId]
      if (!apiKey) {
        setLastError(`API Key not found for provider: ${providerId}`)
        return
      }

      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‚’ä½œæˆ
      const config: AIProviderConfig = {
        providerId,
        modelId: selectedModelId,
        apiKey
      }

      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¨­å®š
      await aiSDKService.configureProvider(config)
      setCurrentProviderConfig(config)
      
      console.log(`Model selected: ${providerId}:${selectedModelId}`)
    } catch (error) {
      console.error('Failed to select model:', error)
      const errorMessage = error instanceof Error ? error.message : 'Unknown error'
      setLastError(errorMessage)
    }
  }

  return (
    <div 
      className={cn("h-full flex flex-col bg-background", className)}
      onDragOver={handleDragOver}
      onDragLeave={handleDragLeave}
      onDrop={handleDrop}
    >
      {/* Header with Settings and CLI Buttons */}
      <div className="flex items-center justify-between p-4 border-b border-border">
        <div className="flex items-center gap-3">
          {isAgentMode && (
            <div className="flex items-center gap-1 text-blue-500">
              <Brain className="w-4 h-4" />
              <span className="text-sm">Agent Mode</span>
            </div>
          )}
        </div>
        <div className="flex items-center gap-2">
          <Button
            onClick={() => setShowCLI(!showCLI)}
            variant={showCLI ? "default" : "ghost"}
            size="sm"
            className="flex items-center justify-center w-8 h-8 p-0"
            title={showCLI ? 'Hide CLI' : 'Show CLI'}
          >
            <TerminalIcon className="w-4 h-4" />
          </Button>
          <Button
            onClick={() => setShowSettings(true)}
            variant="outline"
            size="sm"
            className="flex items-center gap-2"
          >
            <Settings className="w-4 h-4" />
            Settings
          </Button>
        </div>
      </div>

      {/* Error Display */}
      {lastError && (
        <div className="p-3 bg-red-50 border-b border-red-200">
          <div className="flex items-center gap-2 text-red-700 text-sm">
            <AlertCircle className="w-4 h-4" />
            <span>{lastError}</span>
            <Button
              variant="ghost"
              size="sm"
              onClick={() => setLastError(null)}
              className="ml-auto h-6 px-2 text-xs"
            >
              Dismiss
            </Button>
          </div>
        </div>
      )}

      {/* Download Progress Display - Only show when modelDownloadState is not active */}
      {isDownloading && downloadProgress && !modelDownloadState.isDownloading && (
        <div className="p-3">
          <div className="flex items-center gap-2 text-white text-sm">
            <ShimmerText className="text-white">
              {downloadProgress.message}
            </ShimmerText>
            {downloadProgress.status === 'downloading' && downloadProgress.progress && (
              <span className="text-xs text-white">
                {downloadProgress.progress.percentage ? `${downloadProgress.progress.percentage.toFixed(1)}%` : ''}
              </span>
            )}
          </div>
        </div>
      )}

      {/* Tool Calls Display */}
      {currentToolCalls.length > 0 && (
        <div className="p-3 bg-blue-50 border-b border-blue-200">
          <div className="flex items-center gap-2 text-blue-700 text-sm">
            <Wrench className="w-4 h-4" />
            <span>Using tools: {currentToolCalls.map(tc => tc.name).join(', ')}</span>
          </div>
        </div>
      )}

      {/* Sequential Thinking Panel */}
      {showSequentialThinking && (
        <div className="p-4 border-b border-border">
          <SequentialThinkingPanel
            plan={currentPlan}
            onCancel={handleCancelSequentialThinking}
          />
        </div>
      )}

      {/* Message display area */}
      <ScrollArea className="flex-1 p-4" ref={scrollAreaRef}>
        <div className="space-y-2">
          {messages
            .filter((message) => {
              // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
              console.log('ğŸ” Message filter check:', {
                id: message.id,
                content: message.content?.substring(0, 30) + (message.content && message.content.length > 30 ? '...' : ''),
                contentLength: message.content?.length,
                isEmpty: !message.content || message.content.trim() === '',
                isObject: message.content === '{}',
                isArray: message.content === '[]',
                isNull: message.content === 'null',
                isUndefined: message.content === 'undefined',
                hasWhitespaceOnly: /^\s*$/.test(message.content),
                hasWhitespaceObject: /^\s*\{\s*\}\s*$/.test(message.content),
                hasWhitespaceArray: /^\s*\[\s*\]\s*$/.test(message.content)
              })
              
              // ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„ç„¡åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤ºã«ã™ã‚‹ï¼ˆå‹•ç”»ãŒã‚ã‚‹å ´åˆã¯é™¤å¤–ï¼‰
              if ((!message || 
                  !message.content || 
                  message.content.trim() === '' || 
                  message.content.length === 0 ||
                  message.content === undefined ||
                  message.content === null ||
                  typeof message.content !== 'string' ||
                  message.content === 'Chat Response Generation' ||
                  message.content === ' ' ||
                  message.content === '\n' ||
                  message.content === '\t' ||
                  message.content === '{}' ||
                  message.content === '[]' ||
                  message.content === 'null' ||
                  message.content === 'undefined' ||
                  message.content === '[object Object]' ||
                  message.content === '[object Array]' ||
                  message.id === 'streaming' ||
                  /^\s*$/.test(message.content) ||
                  message.content.replace(/\s/g, '').length === 0 ||
                  message.content === 'Audio Mode Active' ||
                  // ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„é…åˆ—ã®æ–‡å­—åˆ—è¡¨ç¾ã‚’ãƒã‚§ãƒƒã‚¯
                  /^\s*\{\s*\}\s*$/.test(message.content) ||
                  /^\s*\[\s*\]\s*$/.test(message.content) ||
                  // ã‚ˆã‚Šå³å¯†ãªç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
                  message.content.includes('[object Object]') ||
                  message.content.includes('[object Array]') ||
                  message.content.includes('{}') ||
                  message.content.includes('[]')) && 
                  !message.videoUrl) {
                console.log('âŒ Filtering out message:', message.id, message.content)
                return false
              }
              
              console.log('âœ… Keeping message:', message.id, message.content)
              return true
            })
            .map((message) => (
              <ChatMessage 
                key={message.id} 
                {...message} 
                useEnhancedPreview={useEnhancedPreview}
                onEdit={handleMessageEdit}
                onCancelEdit={handleMessageEditCancel}
                audioEnabled={audioEnabled}
              />
            ))}
          
          {/* Agent Info Display - Hidden for Router Agents */}
          {/* {agentInfo && agentInfo.type && currentAnalysis && !isTTSProcessing && (
            <AgentInfoDisplay
              agentType={agentInfo.type}
              confidence={agentInfo.confidence}
              reasoning={agentInfo.reasoning}
              complexity={currentAnalysis.complexity}
            />
          )} */}
          {isTyping && !streamingResponse && (
                          <>
                {console.log('ğŸ” isTyping state:', { isTyping, loadingState, streamingResponse })}
                {loadingState === 'text' ? (
                  <JumpingDots className="text-blue-500" />
                ) : (
                  <div className="flex items-center gap-3 p-4 rounded-lg border-[0.5px] border-[#2B2B2B]">
                    <div className="flex items-center gap-2">
                      {loadingState === 'media' ? (
                        <>
                          <CircleSpinner size="sm" className="text-blue-500" />
                          <span className="text-sm text-muted-foreground">Generating...</span>
                        </>
                      ) : (
                        <>
                          <CircleSpinner size="sm" className="text-blue-500" />
                          <span className="text-sm text-muted-foreground">Thinking...</span>
                        </>
                      )}
                    </div>
                  </div>
                )}
              </>
          )}
          {streamingResponse && 
           streamingResponse.trim() !== '' && 
           streamingResponse.length > 0 && 
           streamingResponse !== '{}' && 
           streamingResponse !== '[]' && 
           streamingResponse !== 'null' && 
           streamingResponse !== 'undefined' &&
           !/^\s*\{\s*\}\s*$/.test(streamingResponse) &&
           !/^\s*\[\s*\]\s*$/.test(streamingResponse) && (
            <ChatMessage
              id="streaming"
              content={streamingResponse}
              role="assistant"
              isStreaming={isTyping}
              audioEnabled={audioEnabled}
            />
          )}

          {/* ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¡¨ç¤ºï¼ˆShimmer Effectï¼‰ */}
          {modelDownloadState.isDownloading && (
            <ModelDownloadShimmer
              modelName={modelDownloadState.modelName}
              progress={modelDownloadState.progress}
              status={modelDownloadState.status}
              downloadedBytes={modelDownloadState.downloadedBytes}
              totalBytes={modelDownloadState.totalBytes}
              className="animate-in slide-in-from-bottom-2 duration-300"
            />
          )}

          {/* STTå®Ÿè¡Œä¸­è¡¨ç¤ºï¼ˆShimmer Effectï¼‰ */}
          {isSTTTranscribing && (
            <div className="flex items-center p-3 animate-in slide-in-from-bottom-2 duration-300">
              <ShimmerText className="text-sm font-medium text-blue-600 dark:text-blue-400">
                Transcribing audio file...
              </ShimmerText>
              {sttFileName && (
                <span className="text-xs text-muted-foreground ml-2">
                  {sttFileName}
                </span>
              )}
            </div>
          )}

          {/* ã‚¿ã‚¹ã‚¯å®Ÿè¡Œè¡¨ç¤ºï¼ˆShimmer Effectï¼‰ */}
          {activeTasks
            .filter(task => !task.title.includes('Chat Response Generation'))
            .filter(task => task.status === 'running' || task.status === 'pending')
            .map((task) => (
            <TaskShimmerDisplay
              key={task.id}
              task={task}
              className="animate-in slide-in-from-bottom-2 duration-300"
            />
          ))}

          {/* ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆè¡¨ç¤º */}
          {activeFileCreations.length > 0 && (
            <div className="space-y-3 mt-4">
              <div className="flex items-center justify-between">
                <h3 className="text-sm font-medium text-muted-foreground">
                  ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­ ({activeFileCreations.length})
                </h3>
                <Button
                  variant="ghost"
                  size="sm"
                  onClick={clearCompletedFileCreations}
                  className="text-xs"
                >
                  å®Œäº†æ¸ˆã¿ã‚’ã‚¯ãƒªã‚¢
                </Button>
              </div>
              {activeFileCreations.map((fileCreation) => (
                <FileCreationLoader
                  key={fileCreation.id}
                  fileInfo={fileCreation}
                  className="animate-in slide-in-from-bottom-2 duration-300"
                />
              ))}
            </div>
          )}
        </div>
      </ScrollArea>

      {/* Input area */}
      <div className="border-t p-4">
        {/* æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º */}
        {selectedFiles.length > 0 && (
          <div className="mb-3 flex flex-wrap gap-2">
            {selectedFiles.map((file, index) => (
              <div key={index} className="flex items-center space-x-2 bg-muted rounded-lg p-2">
                {useEnhancedPreview ? (
                  <EnhancedFilePreview 
                    file={file} 
                    onRemove={() => removeAttachment(index)}
                    showPreview={true}
                    googleService={geminiFileService}
                    onTranscriptionComplete={handleTranscriptionComplete}
                  />
                ) : (
                  <EnhancedFilePreview 
                    file={file}
                    googleService={geminiFileService}
                    onTranscriptionComplete={handleTranscriptionComplete}
                  />
                )}
                {!useEnhancedPreview && (
                  <Button
                    type="button"
                    variant="ghost"
                    size="sm"
                    onClick={() => removeAttachment(index)}
                    className="h-6 w-6 p-0"
                  >
                    <X className="h-3 w-3" />
                  </Button>
                )}
              </div>
            ))}
          </div>
        )}

        {/* Embedded CLI */}
        {showCLI && (
          <div className="mb-3">
            <EmbeddedCLI
              onClose={() => setShowCLI(false)}
              isMinimized={isCLIMinimized}
              onToggleMinimize={() => setIsCLIMinimized(!isCLIMinimized)}
              fontSettings={cliFontSettings}
            />
          </div>
        )}

        <PromptInputBox
          onSend={handleSendMessage}
          onAttachFiles={handleFilesAdded}
          onStop={handleStopGeneration}
          disabled={isTyping}
          placeholder=""
          modelSettings={modelSettings}
          onModelSettingsChange={handleModelSettingsChange}
          onModelSelect={handleModelSelect}
          providerApiKeys={providerApiKeys}
          loadingState={loadingState}
          selectedFiles={selectedFiles}
          currentProviderConfig={currentProviderConfig}
          isGenerating={isGenerating}
          webSearchEnabled={webSearchEnabled}
          onWebSearchToggle={handleWebSearchToggle}
          createImageEnabled={createImageEnabled}
          onCreateImageToggle={handleCreateImageToggle}
          audioEnabled={audioEnabled}
          onAudioToggle={handleAudioToggle}
          videoEnabled={videoEnabled}
          onVideoToggle={handleVideoToggle}
          onGetCurrentMessage={getCurrentMessage}
          generationSettings={generationSettings}
          onSTTStatusChange={(isTranscribing, fileName) => {
            setIsSTTTranscribing(isTranscribing)
            setSttFileName(fileName || '')
          }}
        />
      </div>

      {/* ãƒ‰ãƒ©ãƒƒã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ */}
      <AnimatePresence>
        {isDragging && (
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            className="absolute inset-0 bg-primary/10 border-2 border-dashed border-primary rounded-lg flex items-center justify-center z-50"
          >
            <div className="text-center">
              <Paperclip className="h-8 w-8 mx-auto mb-2" />
              <p className="text-sm font-medium">ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</p>
            </div>
          </motion.div>
        )}
      </AnimatePresence>



      {/* Settings Modal */}
      {showSettings && (
        <ArmisSettings
          onClose={() => setShowSettings(false)}
          onProviderSelect={handleProviderSelect}
                  currentModelSettings={modelSettings}
        onModelSettingsChange={handleModelSettingsChange}
          providerApiKeys={providerApiKeys}
          onProviderApiKeysChange={setProviderApiKeys}
          llmManager={llmManager}
          generationSettings={generationSettings}
          onGenerationSettingsChange={setGenerationSettings}
          onCliFontSettingsChange={handleCliFontSettingsChange}
          currentCliFontSettings={cliFontSettings}
          currentTheme={theme}
          onThemeChange={setTheme}
          factCheckingSettings={factCheckingSettings}
          onFactCheckingSettingsChange={handleFactCheckingSettingsChange}
        />
      )}


    </div>
  )
}
