import { checkEnvironment } from './llama-cpp-text-extraction-example'

/**
 * llama.cppÁí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ
 * 
 * „Åì„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅØ„ÄÅllama.cppÊú¨ÊñáÊäΩÂá∫Ê©üËÉΩ„ÅåÂãï‰Ωú„Åô„ÇãÁí∞Â¢É„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ
 */

async function main() {
  console.log('üîç LlamaCpp Environment Check\n')
  
  // 1. Áí∞Â¢É„ÉÅ„Çß„ÉÉ„ÇØ
  const env = checkEnvironment()
  
  if (!env.isNode) {
    console.log('‚ùå LlamaCpp functionality requires Node.js environment')
    console.log('   This feature is not available in browser environment')
    return
  }
  
  console.log('‚úÖ Node.js environment detected')
  
  // 2. ÂøÖË¶Å„Å™„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆÁ¢∫Ë™ç
  console.log('\nüì¶ Checking required packages...')
  
  try {
    // node-llama-cpp„ÅÆÁ¢∫Ë™ç
    const nodeLlamaCpp = await import('node-llama-cpp')
    console.log('‚úÖ node-llama-cpp package available')
  } catch (error) {
    console.log('‚ùå node-llama-cpp package not found')
    console.log('   Install with: npm install node-llama-cpp@3')
    return
  }
  
  try {
    // @langchain/community„ÅÆÁ¢∫Ë™ç
    const { ChatLlamaCpp } = await import('@langchain/community/chat_models/llama_cpp')
    console.log('‚úÖ @langchain/community package available')
  } catch (error) {
    console.log('‚ùå @langchain/community package not found')
    console.log('   Install with: npm install @langchain/community')
    return
  }
  
  // 3. „É¢„Éá„É´„Éï„Ç°„Ç§„É´„ÅÆÁ¢∫Ë™ç
  console.log('\nüìÅ Checking model files...')
  
  try {
    const fs = await import('fs')
    const path = await import('path')
    
    const modelsDir = './models'
    if (!fs.existsSync(modelsDir)) {
      console.log('‚ùå Models directory not found')
      console.log('   Create directory: mkdir models')
      console.log('   Download GGUF model files to ./models/')
      return
    }
    
    console.log('‚úÖ Models directory found')
    
    const files = fs.readdirSync(modelsDir)
    const ggufFiles = files.filter(file => file.endsWith('.gguf'))
    
    if (ggufFiles.length === 0) {
      console.log('‚ùå No GGUF model files found')
      console.log('   Download GGUF model files to ./models/')
      console.log('   Recommended models:')
      console.log('     - llama-2-7b-chat.gguf')
      console.log('     - llama-2-13b-chat.gguf')
      console.log('     - mistral-7b-instruct-v0.2.gguf')
      console.log('     - qwen2-7b-instruct.gguf')
      return
    }
    
    console.log('‚úÖ GGUF model files found:')
    ggufFiles.forEach(file => {
      const filePath = path.join(modelsDir, file)
      const stats = fs.statSync(filePath)
      const sizeMB = Math.round(stats.size / (1024 * 1024))
      console.log(`   - ${file} (${sizeMB} MB)`)
    })
    
  } catch (error) {
    console.log('‚ùå Error checking model files:', error)
    return
  }
  
  // 4. „Ç∑„Çπ„ÉÜ„É†Ë¶Å‰ª∂„ÅÆÁ¢∫Ë™ç
  console.log('\nüíª Checking system requirements...')
  
  try {
    const os = await import('os')
    
    const totalMemory = Math.round(os.totalmem() / (1024 * 1024 * 1024))
    const freeMemory = Math.round(os.freemem() / (1024 * 1024 * 1024))
    const cpuCores = os.cpus().length
    
    console.log(`‚úÖ System Info:`)
    console.log(`   Total Memory: ${totalMemory} GB`)
    console.log(`   Free Memory: ${freeMemory} GB`)
    console.log(`   CPU Cores: ${cpuCores}`)
    
    if (totalMemory < 8) {
      console.log('‚ö†Ô∏è  Warning: Less than 8GB RAM detected')
      console.log('   LlamaCpp may run slowly or fail with large models')
    }
    
    if (freeMemory < 4) {
      console.log('‚ö†Ô∏è  Warning: Less than 4GB free RAM detected')
      console.log('   Consider closing other applications')
    }
    
  } catch (error) {
    console.log('‚ùå Error checking system requirements:', error)
  }
  
  // 5. Êé®Â•®Ë®≠ÂÆö
  console.log('\n‚öôÔ∏è  Recommended Configuration:')
  console.log('   For 7B models:')
  console.log('     - threads: 4-8')
  console.log('     - contextSize: 4096')
  console.log('     - gpuLayers: 0 (CPU only)')
  console.log('')
  console.log('   For 13B+ models:')
  console.log('     - threads: 8-16')
  console.log('     - contextSize: 8192')
  console.log('     - gpuLayers: 0 (CPU only)')
  console.log('     - Consider GPU acceleration if available')
  
  console.log('\n‚úÖ Environment check completed!')
  console.log('   LlamaCpp text extraction should work in this environment')
}

// „É°„Ç§„É≥ÂÆüË°å
main().catch(console.error)
