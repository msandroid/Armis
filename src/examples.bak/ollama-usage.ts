import { OllamaService } from '@/services/llm/ollama-service'
import { LLMManager } from '@/services/llm/llm-manager'

// Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹
export async function basicOllamaUsage() {
  console.log('=== Ollama Basic Usage Example ===')

  // 1. Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ï¼ˆgemma3:1bãŒè‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼‰
  const ollamaService = new OllamaService({
    defaultModel: 'gemma3:1b',
    baseUrl: 'http://localhost:11434',
    timeout: 30000
  })

  try {
    // 2. ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ï¼ˆgemma3:1bãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
    console.log('ğŸ”„ Initializing Ollama service...')
    await ollamaService.initialize()
    console.log('âœ… Ollama service initialized successfully')

    // 3. åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º
    const models = await ollamaService.listModels()
    console.log('ğŸ“‹ Available models:', models.map(m => m.name))

    // 4. åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    const response = await ollamaService.generate(
      'ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚',
      {
        options: {
          temperature: 0.7,
          num_predict: 100
        }
      }
    )
    console.log('ğŸ¤– Generated response:', response.response)

    // 5. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
    console.log('ğŸ”„ Streaming generation...')
    await ollamaService.generateStream(
      'æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦ç°¡å˜ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',
      {
        options: {
          temperature: 0.8,
          num_predict: 150
        }
      },
      (chunk) => {
        process.stdout.write(chunk.response)
      }
    )

    // 6. ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ã®ç”Ÿæˆ
    const chatResponse = await ollamaService.chat([
      { role: 'system', content: 'ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚' },
      { role: 'user', content: 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚' }
    ])
    console.log('ğŸ’¬ Chat response:', chatResponse.response)

  } catch (error) {
    console.error('âŒ Error:', error)
  }
}

// ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã®ä¾‹
export async function modelManagementExample() {
  console.log('=== Ollama Model Management Example ===')

  const ollamaService = new OllamaService()

  try {
    await ollamaService.initialize()

    // 1. æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
    console.log('ğŸ“¥ Pulling new model with progress tracking...')
    await ollamaService.pullModelWithProgress(
      'llama3.1:8b',
      (progress, message) => {
        console.log(`ğŸ“Š ${message}`)
      }
    )
    console.log('âœ… Model pulled successfully')

    // 2. ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
    await ollamaService.setDefaultModel('llama3.1:8b')
    console.log('ğŸ”„ Model switched to llama3.1:8b')

    // 3. ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    const currentModel = ollamaService.getDefaultModel()
    console.log('ğŸ“‹ Current model:', currentModel)

    // 4. ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    const modelInfo = await ollamaService.getModelInfo('llama3.1:8b')
    if (modelInfo) {
      console.log('ğŸ“Š Model info:', {
        name: modelInfo.name,
        size: modelInfo.size,
        modified: modelInfo.modified_at
      })
    }

  } catch (error) {
    console.error('âŒ Error:', error)
  }
}

// LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã¨ã®çµ±åˆä¾‹
export async function llmManagerIntegrationExample() {
  console.log('=== LLM Manager Integration Example ===')

  // LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
  const llmManager = new LLMManager({
    modelPath: './models/llama-2-7b-chat.gguf',
    contextSize: 4096,
    temperature: 0.7,
    topP: 0.9,
    topK: 40
  })

  try {
    // 1. LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    await llmManager.initialize()
    console.log('âœ… LLM Manager initialized')

    // 2. Ollamaã«åˆ‡ã‚Šæ›¿ãˆ
    await llmManager.switchToOllama()
    console.log('ğŸ”„ Switched to Ollama')

    // 3. åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º
    const models = await llmManager.listOllamaModels()
    console.log('ğŸ“‹ Available Ollama models:', models.map(m => m.name))

    // 4. ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
    await llmManager.setOllamaModel('gemma3:4b')
    console.log('ğŸ”„ Model switched to gemma3:4b')

    // 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†
    const response = await llmManager.processUserRequest(
      'AIã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚'
    )
    console.log('ğŸ¤– Response:', response)

    // 6. ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã®å–å¾—
    const stats = llmManager.getSystemStats()
    console.log('ğŸ“Š System stats:', {
      llmService: stats.llmService,
      ollamaModel: stats.ollamaModel,
      agents: stats.agents.length
    })

  } catch (error) {
    console.error('âŒ Error:', error)
  }
}

// é«˜åº¦ãªä½¿ç”¨ä¾‹
export async function advancedOllamaUsage() {
  console.log('=== Advanced Ollama Usage Example ===')

  const ollamaService = new OllamaService()

  try {
    await ollamaService.initialize()

    // 1. è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã§ã®æ¯”è¼ƒ
    const models = ['gemma3:1b', 'gemma3:4b', 'llama3.1:8b']
    
    for (const model of models) {
      console.log(`\nğŸ” Testing model: ${model}`)
      
      await ollamaService.setDefaultModel(model)
      
      const startTime = Date.now()
      const response = await ollamaService.generate(
        'ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼šAIæŠ€è¡“ã¯æ€¥é€Ÿã«ç™ºå±•ã—ã¦ãŠã‚Šã€ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤§ããå¤‰ãˆã¦ã„ã¾ã™ã€‚',
        {
          options: {
            temperature: 0.3,
            num_predict: 100
          }
        }
      )
      const endTime = Date.now()
      
      console.log(`â±ï¸  Response time: ${endTime - startTime}ms`)
      console.log(`ğŸ“ Response: ${response.response}`)
      console.log(`ğŸ“Š Tokens: ${response.eval_count || 'N/A'}`)
    }

    // 2. ã‚«ã‚¹ã‚¿ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    const customResponse = await ollamaService.generate(
      'ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚',
      {
        system: 'ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹å°‚é–€ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚å®¢è¦³çš„ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚',
        options: {
          temperature: 0.5,
          num_predict: 200
        }
      }
    )
    console.log('\nğŸ“° Custom system prompt response:', customResponse.response)

  } catch (error) {
    console.error('âŒ Error:', error)
  }
}

// ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ä¾‹
export async function errorHandlingExample() {
  console.log('=== Error Handling Example ===')

  const ollamaService = new OllamaService()

  try {
    // 1. OllamaãŒèµ·å‹•ã—ã¦ã„ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    await ollamaService.initialize()
  } catch (error) {
    if (error instanceof Error && error.message.includes('not running')) {
      console.log('âš ï¸  Ollama is not running. Please start Ollama first.')
      console.log('ğŸ’¡ Run: ollama serve')
      return
    }
    throw error
  }

  try {
    // 2. å­˜åœ¨ã—ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆ
    await ollamaService.setDefaultModel('non-existent-model')
  } catch (error) {
    console.log('âš ï¸  Model not found, attempting to pull...')
    try {
      await ollamaService.pullModel('non-existent-model')
    } catch (pullError) {
      console.log('âŒ Failed to pull model:', pullError)
    }
  }

  try {
    // 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
    const response = await ollamaService.generate('Test message', {
      options: {
        num_predict: 10
      }
    })
    console.log('âœ… Success:', response.response)
  } catch (error) {
    if (error instanceof Error && error.message.includes('timeout')) {
      console.log('âš ï¸  Request timed out. Please check your network connection.')
    } else {
      console.log('âŒ Unexpected error:', error)
    }
  }
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
export async function runOllamaExamples() {
  console.log('ğŸš€ Starting Ollama Examples...\n')

  try {
    await basicOllamaUsage()
    console.log('\n' + '='.repeat(50) + '\n')
    
    await modelManagementExample()
    console.log('\n' + '='.repeat(50) + '\n')
    
    await llmManagerIntegrationExample()
    console.log('\n' + '='.repeat(50) + '\n')
    
    await advancedOllamaUsage()
    console.log('\n' + '='.repeat(50) + '\n')
    
    await errorHandlingExample()
    
    console.log('\nâœ… All examples completed successfully!')
  } catch (error) {
    console.error('âŒ Example execution failed:', error)
  }
}

// å€‹åˆ¥å®Ÿè¡Œç”¨ã®é–¢æ•°
export async function runBasicExample() {
  await basicOllamaUsage()
}

export async function runModelManagementExample() {
  await modelManagementExample()
}

export async function runIntegrationExample() {
  await llmManagerIntegrationExample()
}

export async function runAdvancedExample() {
  await advancedOllamaUsage()
}

export async function runErrorHandlingExample() {
  await errorHandlingExample()
}
