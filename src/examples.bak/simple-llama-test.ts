import { getLlama } from 'node-llama-cpp'

async function testSimpleLlama() {
  console.log('ğŸš€ Testing Simple Llama Setup...\n')

  try {
    // 1. Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
    console.log('ğŸ“¥ Creating Llama instance...')
    const llama = await getLlama()
    console.log('âœ… Llama instance created\n')

    // 2. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º
    console.log('ğŸ’» System Information:')
    console.log('Build Type:', llama.buildType)
    console.log('CPU Math Cores:', llama.cpuMathCores)
    console.log('Max Threads:', llama.maxThreads)
    console.log('GPU Support:', llama.supportsGpuOffloading)
    console.log('')

    // 3. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    console.log('ğŸ“ Loading model...')
    const modelPath = './models/unsloth/gpt-oss-20b-GGUF/gpt-oss-20b-F16.gguf'
    
    // ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    const fs = await import('fs')
    if (!fs.existsSync(modelPath)) {
      console.error(`âŒ Model file not found: ${modelPath}`)
      return
    }
    
    console.log('âœ… Model file exists')
    
    // ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
    try {
      const model = await llama.loadModel(modelPath)
      console.log('âœ… Model loaded successfully')
      console.log('Model type:', typeof model)
      
      // ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç¢ºèª
      const methods = Object.getOwnPropertyNames(Object.getPrototypeOf(model))
      console.log('Available methods:', methods.slice(0, 10)) // æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
      
    } catch (error) {
      console.error('âŒ Failed to load model:', error)
      console.error('Error details:', error instanceof Error ? error.message : error)
    }

  } catch (error) {
    console.error('âŒ Test failed:', error)
    console.error('Error details:', error instanceof Error ? error.message : error)
  }
}

// ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
testSimpleLlama().catch(console.error)
