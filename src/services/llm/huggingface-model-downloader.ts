export interface HuggingFaceModel {
  id: string
  name: string
  description: string
  downloads: number
  likes: number
  tags: string[]
  model_type: string
  size: number
  format: string
  quantization: string
}

export interface DownloadProgress {
  modelId: string
  downloaded: number
  total: number
  percentage: number
  speed: number
  eta: number
  status: 'downloading' | 'completed' | 'error'
  error?: string
}

export interface ModelDownloadOptions {
  modelId: string
  targetPath?: string
  onProgress?: (progress: DownloadProgress) => void
  onComplete?: (modelPath: string) => void
  onError?: (error: string) => void
}

export class HuggingFaceModelDownloader {
  private baseUrl = 'https://huggingface.co/api'
  private modelsCache: HuggingFaceModel[] = []
  private downloadQueue: Map<string, AbortController> = new Map()

  async searchLlamaCppModels(query: string = '', limit: number = 50): Promise<HuggingFaceModel[]> {
    try {
      console.log(`ğŸ” Searching for LlamaCpp models with query: "${query}"`)
      
      // ã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œç´¢ã®ãŸã‚ã€è¤‡æ•°ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
      const searchQueries = [
        query,
        query.replace('-', ' '),
        query.replace('_', ' '),
        query.toLowerCase(),
        query.toUpperCase()
      ]
      
      // å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«IDã®å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«åéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦æ¤œç´¢
      if (query.includes('/')) {
        const modelName = query.split('/').pop() || query
        searchQueries.push(
          modelName,
          modelName.replace('-', ' '),
          modelName.replace('_', ' '),
          modelName.toLowerCase(),
          modelName.toUpperCase()
        )
      }
      
      let allModels: HuggingFaceModel[] = []
      
      for (const searchQuery of searchQueries) {
        try {
          // Search for GGUF models with llama.cpp tag
          const searchParams = new URLSearchParams({
            search: searchQuery,
            filter: 'other:llama.cpp',
            sort: 'downloads',
            direction: '-1',
            limit: limit.toString()
          })

          const response = await fetch(`${this.baseUrl}/models?${searchParams}`)
          
          if (!response.ok) {
            continue // ã“ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã‚’è©¦ã™
          }

          const data = await response.json()
          const models: HuggingFaceModel[] = []

          for (const model of data) {
            // Filter for GGUF models
            const hasGGUF = model.siblings?.some((sibling: any) => 
              sibling.rfilename.endsWith('.gguf')
            )

            if (hasGGUF) {
              const ggufFiles = model.siblings?.filter((sibling: any) => 
                sibling.rfilename.endsWith('.gguf')
              ) || []

              // Get the largest GGUF file (usually the main model)
              const mainFile = ggufFiles.reduce((largest: any, current: any) => 
                (current.size || 0) > (largest.size || 0) ? current : largest
              )

              models.push({
                id: model.id,
                name: model.modelId || model.id,
                description: model.description || 'No description available',
                downloads: model.downloads || 0,
                likes: model.likes || 0,
                tags: model.tags || [],
                model_type: model.model_type || 'unknown',
                size: mainFile?.size || 0,
                format: 'GGUF',
                quantization: this.extractQuantization(mainFile?.rfilename || '')
              })
            }
          }
          
          allModels = allModels.concat(models)
        } catch (error) {
          console.warn(`Warning: Search query "${searchQuery}" failed:`, error)
          continue
        }
      }
      
      // é‡è¤‡ã‚’é™¤å»ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ã§ã‚½ãƒ¼ãƒˆ
      const uniqueModels = allModels.filter((model, index, self) => 
        index === self.findIndex(m => m.id === model.id)
      ).sort((a, b) => b.downloads - a.downloads)

      // ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
      if (uniqueModels.length === 0) {
        console.log(`âš ï¸ No models found for query "${query}", adding fallback models`)
        const fallbackModels = this.generateFallbackModels(query)
        uniqueModels.push(...fallbackModels)
      }

      this.modelsCache = uniqueModels
      console.log(`âœ… Found ${uniqueModels.length} unique LlamaCpp models`)
      return uniqueModels

    } catch (error) {
      console.error('âŒ Error searching models:', error)
      throw error
    }
  }

  async getModelDetails(modelId: string): Promise<HuggingFaceModel | null> {
    try {
      const response = await fetch(`${this.baseUrl}/models/${modelId}`)
      
      if (!response.ok) {
        throw new Error(`Failed to fetch model details: ${response.statusText}`)
      }

      const model = await response.json()
      
      // Find GGUF files
      const ggufFiles = model.siblings?.filter((sibling: any) => 
        sibling.rfilename.endsWith('.gguf')
      ) || []

      if (ggufFiles.length === 0) {
        return null
      }

      const mainFile = ggufFiles.reduce((largest: any, current: any) => 
        (current.size || 0) > (largest.size || 0) ? current : largest
      )

      return {
        id: model.id,
        name: model.modelId || model.id,
        description: model.description || 'No description available',
        downloads: model.downloads || 0,
        likes: model.likes || 0,
        tags: model.tags || [],
        model_type: model.model_type || 'unknown',
        size: mainFile?.size || 0,
        format: 'GGUF',
        quantization: this.extractQuantization(mainFile?.rfilename || '')
      }

    } catch (error) {
      console.error('âŒ Error fetching model details:', error)
      return null
    }
  }

  async downloadModel(options: ModelDownloadOptions): Promise<string> {
    const { modelId, targetPath, onProgress, onComplete, onError } = options

    try {
      console.log(`ğŸš€ ===== HUGGING FACE: MODEL DOWNLOAD STARTED =====`)
      console.log(`ğŸ“¥ Model: ${modelId}`)
      console.log(`â° Start time: ${new Date().toLocaleString()}`)
      console.log(`ğŸ”— Base URL: ${this.baseUrl}`)
      console.log(`ğŸ”§ Service: Hugging Face`)
      console.log(`===================================================`)

      // Get model details to find the GGUF file
      const modelDetails = await this.getModelDetails(modelId)
      if (!modelDetails) {
        throw new Error('Model not found or no GGUF files available')
      }

      // Find the main GGUF file
      const response = await fetch(`${this.baseUrl}/models/${modelId}`)
      const model = await response.json()
      
      const ggufFiles = model.siblings?.filter((sibling: any) => 
        sibling.rfilename.endsWith('.gguf')
      ) || []

      if (ggufFiles.length === 0) {
        throw new Error('No GGUF files found for this model')
      }

      // é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã®å„ªå…ˆé †ä½ã‚’å®šç¾©
      const quantizationPriority = ['Q4_K_M', 'Q5_K_M', 'Q4_K_S', 'Q5_K_S', 'Q8_0', 'Q4_0', 'Q5_0']
      
      // å„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
      let mainFile = null
      for (const quant of quantizationPriority) {
        mainFile = ggufFiles.find((file: any) => 
          file.rfilename.includes(quant)
        )
        if (mainFile) {
          console.log(`âœ… Selected ${quant} quantization: ${mainFile.rfilename}`)
          break
        }
      }
      
      // è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¤§ã‚µã‚¤ã‚ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
      if (!mainFile) {
        mainFile = ggufFiles.reduce((largest: any, current: any) => 
          (current.size || 0) > (largest.size || 0) ? current : largest
        )
        console.log(`âš ï¸ No preferred quantization found, using largest file: ${mainFile.rfilename}`)
      }

      const downloadUrl = `https://huggingface.co/${modelId}/resolve/main/${mainFile.rfilename}`
      const fileName = mainFile.rfilename
      const fileSize = mainFile.size
      
      // Create proper directory structure for the model
      const modelDir = modelId.replace('/', '/')
      const modelName = modelId.split('/').pop() || modelId
      const finalTargetPath = targetPath || `./models/${modelDir}/${fileName}`
      
      console.log(`ğŸ“ Model directory: ${modelDir}`)
      console.log(`ğŸ“ Model name: ${modelName}`)
      console.log(`ğŸ“ File: ${fileName}`)
      console.log(`ğŸ“Š Size: ${(fileSize / 1024 / 1024).toFixed(2)} MB`)
      console.log(`ğŸ”§ Quantization: ${this.extractQuantization(fileName)}`)
      console.log(`ğŸ”— Download URL: ${downloadUrl}`)
      console.log(`ğŸ“‚ Target path: ${finalTargetPath}`)

      // Create abort controller for this download
      const abortController = new AbortController()
      this.downloadQueue.set(modelId, abortController)

      // Check if we're in Electron environment
      const isElectron = typeof window !== 'undefined' && (window as any).electronAPI

      if (isElectron) {
        // Use Electron IPC for file download
        return await this.downloadWithElectron(
          modelId,
          downloadUrl,
          fileName,
          fileSize,
          finalTargetPath,
          onProgress,
          onComplete,
          onError,
          abortController
        )
      } else {
        // Fallback to browser download (simulated)
        return await this.downloadInBrowser(
          modelId,
          downloadUrl,
          fileName,
          fileSize,
          finalTargetPath,
          onProgress,
          onComplete,
          onError,
          abortController
        )
      }

    } catch (error) {
      console.error('âŒ Download failed:', error)
      
      // Remove from download queue
      this.downloadQueue.delete(modelId)

      const errorProgress: DownloadProgress = {
        modelId,
        downloaded: 0,
        total: 0,
        percentage: 0,
        speed: 0,
        eta: 0,
        status: 'error',
        error: error instanceof Error ? error.message : 'Unknown error'
      }

      onProgress?.(errorProgress)
      onError?.(error instanceof Error ? error.message : 'Unknown error')
      
      throw error
    }
  }

  private async downloadWithElectron(
    modelId: string,
    downloadUrl: string,
    fileName: string,
    fileSize: number,
    targetPath: string,
    onProgress?: (progress: DownloadProgress) => void,
    onComplete?: (modelPath: string) => void,
    onError?: (error: string) => void,
    abortController?: AbortController
  ): Promise<string> {
    try {
      // Use Electron's IPC to download file
      const electronAPI = (window as any).electronAPI
      
      if (!electronAPI?.downloadFile) {
        throw new Error('Electron download API not available')
      }

      // Set up progress listener
      if (onProgress) {
        electronAPI.onDownloadProgress((progress: any) => {
          if (progress.modelId === modelId) {
            const downloadProgress: DownloadProgress = {
              modelId,
              downloaded: progress.downloaded,
              total: progress.total,
              percentage: progress.percentage,
              speed: progress.speed,
              eta: progress.eta,
              status: progress.status,
              error: progress.error
            }
            
            // è©³ç´°ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
            const downloadedMB = (progress.downloaded / 1024 / 1024).toFixed(2)
            const totalMB = (progress.total / 1024 / 1024).toFixed(2)
            const speedMBps = progress.speed ? (progress.speed / 1024 / 1024).toFixed(2) : '0'
            const etaMinutes = progress.eta ? Math.round(progress.eta / 60) : 0
            
            console.log(`ğŸ“Š [Electron] ${modelId}: ${progress.percentage}% (${downloadedMB}MB/${totalMB}MB) - ${speedMBps}MB/s - ETA: ${etaMinutes}min`)
            
            onProgress(downloadProgress)
          }
        })
      }

      // Start download through Electron
      const result = await electronAPI.downloadFile({
        modelId,
        url: downloadUrl,
        targetPath,
        fileName,
        fileSize
      })

      if (result.success) {
        // Remove from download queue
        this.downloadQueue.delete(modelId)

        const finalProgress: DownloadProgress = {
          modelId,
          downloaded: fileSize,
          total: fileSize,
          percentage: 100,
          speed: 0,
          eta: 0,
          status: 'completed'
        }

        onProgress?.(finalProgress)
        onComplete?.(result.path)

        console.log(`ğŸ‰ ===== HUGGING FACE: DOWNLOAD COMPLETED =====`)
        console.log(`âœ… Download completed: ${modelId}`)
        console.log(`ğŸ“ File: ${fileName}`)
        console.log(`ğŸ“‚ Path: ${result.path}`)
        console.log(`â° End time: ${new Date().toLocaleString()}`)
        console.log(`ğŸ“Š Final size: ${(fileSize / 1024 / 1024).toFixed(2)} MB`)
        console.log(`================================================`)
        return result.path
      } else {
        throw new Error(result.error || 'Download failed')
      }

    } catch (error) {
      console.error(`ğŸ’¥ ===== HUGGING FACE: DOWNLOAD FAILED =====`)
      console.error(`âŒ Electron download failed: ${error}`)
      console.error(`ğŸ“¥ Model: ${modelId}`)
      console.error(`ğŸ“ File: ${fileName}`)
      console.error(`â° Time: ${new Date().toLocaleString()}`)
      console.error(`==============================================`)
      throw error
    }
  }

  private async downloadInBrowser(
    modelId: string,
    downloadUrl: string,
    fileName: string,
    fileSize: number,
    targetPath: string,
    onProgress?: (progress: DownloadProgress) => void,
    onComplete?: (modelPath: string) => void,
    onError?: (error: string) => void,
    abortController?: AbortController
  ): Promise<string> {
    try {
      // Start download
      const downloadResponse = await fetch(downloadUrl, {
        signal: abortController?.signal
      })

      if (!downloadResponse.ok) {
        throw new Error(`Download failed: ${downloadResponse.statusText}`)
      }

      const reader = downloadResponse.body?.getReader()
      if (!reader) {
        throw new Error('Failed to get response reader')
      }

      let downloadedBytes = 0
      const chunks: Uint8Array[] = []
      const startTime = Date.now()

      // Read the stream
      while (true) {
        const { done, value } = await reader.read()
        
        if (done) break
        
        chunks.push(value)
        downloadedBytes += value.length

        // Calculate progress
        const percentage = (downloadedBytes / fileSize) * 100
        const elapsedTime = (Date.now() - startTime) / 1000
        const speed = downloadedBytes / elapsedTime // bytes per second
        const remainingBytes = fileSize - downloadedBytes
        const eta = remainingBytes / speed // seconds

        const progress: DownloadProgress = {
          modelId,
          downloaded: downloadedBytes,
          total: fileSize,
          percentage: Math.round(percentage * 100) / 100,
          speed: Math.round(speed),
          eta: Math.round(eta),
          status: 'downloading'
        }

        // è©³ç´°ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
        const downloadedMB = (downloadedBytes / 1024 / 1024).toFixed(2)
        const totalMB = (fileSize / 1024 / 1024).toFixed(2)
        const speedMBps = (speed / 1024 / 1024).toFixed(2)
        const etaMinutes = Math.round(eta / 60)
        
        console.log(`ğŸ“Š [Browser] ${modelId}: ${Math.round(percentage * 100) / 100}% (${downloadedMB}MB/${totalMB}MB) - ${speedMBps}MB/s - ETA: ${etaMinutes}min`)
        
        onProgress?.(progress)
      }

      // Combine chunks and create blob
      const blob = new Blob(chunks as BlobPart[])
      
      // In browser environment, we can't save to filesystem directly
      // So we'll create a download link
      const url = URL.createObjectURL(blob)
      const a = document.createElement('a')
      a.href = url
      a.download = fileName
      document.body.appendChild(a)
      a.click()
      document.body.removeChild(a)
      URL.revokeObjectURL(url)

      // Remove from download queue
      this.downloadQueue.delete(modelId)

      const finalProgress: DownloadProgress = {
        modelId,
        downloaded: fileSize,
        total: fileSize,
        percentage: 100,
        speed: 0,
        eta: 0,
        status: 'completed'
      }

      onProgress?.(finalProgress)
      onComplete?.(targetPath)

      console.log(`ğŸ‰ ===== HUGGING FACE: BROWSER DOWNLOAD COMPLETED =====`)
      console.log(`âœ… Download completed: ${modelId}`)
      console.log(`ğŸ“ File: ${fileName}`)
      console.log(`ğŸ“‚ Path: ${targetPath}`)
      console.log(`â° End time: ${new Date().toLocaleString()}`)
      console.log(`ğŸ“Š Final size: ${(fileSize / 1024 / 1024).toFixed(2)} MB`)
      console.log(`========================================================`)
      return targetPath

    } catch (error) {
      console.error(`ğŸ’¥ ===== HUGGING FACE: BROWSER DOWNLOAD FAILED =====`)
      console.error(`âŒ Browser download failed: ${error}`)
      console.error(`ğŸ“¥ Model: ${modelId}`)
      console.error(`ğŸ“ File: ${fileName}`)
      console.error(`â° Time: ${new Date().toLocaleString()}`)
      console.error(`====================================================`)
      throw error
    }
  }

  async cancelDownload(modelId: string): Promise<void> {
    const abortController = this.downloadQueue.get(modelId)
    if (abortController) {
      abortController.abort()
      this.downloadQueue.delete(modelId)
      console.log(`âŒ Download cancelled for: ${modelId}`)
    }
  }

  isDownloading(modelId: string): boolean {
    return this.downloadQueue.has(modelId)
  }

  getDownloadingModels(): string[] {
    return Array.from(this.downloadQueue.keys())
  }

  private extractQuantization(filename: string): string {
    // ã‚ˆã‚Šè©³ç´°ãªé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã®èªè­˜
    if (filename.includes('Q4_K_M')) return 'Q4_K_M'
    if (filename.includes('Q4_K_S')) return 'Q4_K_S'
    if (filename.includes('Q5_K_M')) return 'Q5_K_M'
    if (filename.includes('Q5_K_S')) return 'Q5_K_S'
    if (filename.includes('Q6_K')) return 'Q6_K'
    if (filename.includes('Q8_0')) return 'Q8_0'
    if (filename.includes('Q4_0')) return 'Q4_0'
    if (filename.includes('Q4_1')) return 'Q4_1'
    if (filename.includes('Q5_0')) return 'Q5_0'
    if (filename.includes('Q5_1')) return 'Q5_1'
    if (filename.includes('f16')) return 'F16'
    if (filename.includes('f32')) return 'F32'
    return 'Unknown'
  }

  formatFileSize(bytes: number): string {
    const sizes = ['Bytes', 'KB', 'MB', 'GB']
    if (bytes === 0) return '0 Bytes'
    const i = Math.floor(Math.log(bytes) / Math.log(1024))
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i]
  }

  formatSpeed(bytesPerSecond: number): string {
    return this.formatFileSize(bytesPerSecond) + '/s'
  }

  formatTime(seconds: number): string {
    if (seconds < 60) return `${Math.round(seconds)}s`
    if (seconds < 3600) return `${Math.round(seconds / 60)}m ${Math.round(seconds % 60)}s`
    const hours = Math.floor(seconds / 3600)
    const minutes = Math.round((seconds % 3600) / 60)
    return `${hours}h ${minutes}m`
  }

  private generateFallbackModels(query: string): HuggingFaceModel[] {
    const fallbackModels: HuggingFaceModel[] = []
    
    // qwen3é–¢é€£ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
    if (query.toLowerCase().includes('qwen3')) {
      fallbackModels.push(
        {
          id: 'unsloth/Qwen3-0.6B-GGUF',
          name: 'Qwen3-0.6B-GGUF',
          description: 'Qwen3 0.6B model optimized with Unsloth, GGUF format for ultra-lightweight inference',
          downloads: 34452,
          likes: 74,
          tags: ['gguf', 'qwen3', 'lightweight'],
          model_type: 'qwen3',
          size: 0,
          format: 'GGUF',
          quantization: 'Q4_K_M'
        },
        {
          id: 'unsloth/Qwen3-1.7B-GGUF',
          name: 'Qwen3-1.7B-GGUF',
          description: 'Qwen3 1.7B model optimized with Unsloth, GGUF format for lightweight inference',
          downloads: 31364,
          likes: 41,
          tags: ['gguf', 'qwen3', 'lightweight'],
          model_type: 'qwen3',
          size: 0,
          format: 'GGUF',
          quantization: 'Q4_K_M'
        },
        {
          id: 'unsloth/Qwen3-4B-Instruct-2507-GGUF',
          name: 'Qwen3-4B-Instruct-2507-GGUF',
          description: 'Qwen3 4B Instruct model optimized with Unsloth, GGUF format for local inference',
          downloads: 67000,
          likes: 47,
          tags: ['gguf', 'qwen3', 'instruct'],
          model_type: 'qwen3',
          size: 0,
          format: 'GGUF',
          quantization: 'Q4_K_M'
        },
        {
          id: 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF',
          name: 'DeepSeek-R1-0528-Qwen3-8B-GGUF',
          description: 'Qwen3 8B model optimized with Unsloth, GGUF format',
          downloads: 256000,
          likes: 295,
          tags: ['gguf', 'qwen3', 'deepseek'],
          model_type: 'qwen3',
          size: 0,
          format: 'GGUF',
          quantization: 'Q4_K_M'
        },
        {
          id: 'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF',
          name: 'Qwen3-30B-A3B-Instruct-2507-GGUF',
          description: 'Qwen3 30B Instruct model optimized with Unsloth, GGUF format',
          downloads: 263000,
          likes: 209,
          tags: ['gguf', 'qwen3', 'instruct'],
          model_type: 'qwen3',
          size: 0,
          format: 'GGUF',
          quantization: 'Q4_K_M'
        }
      )
    }
    
    // ãã®ä»–ã®ä¸€èˆ¬çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
    fallbackModels.push(
      {
        id: 'unsloth/gpt-oss-20b-GGUF',
        name: 'GPT-OSS-20B-GGUF',
        description: 'OpenAI GPT-OSS-20B model for powerful reasoning and agentic tasks',
        downloads: 658000,
        likes: 336,
        tags: ['gguf', 'gpt', 'oss'],
        model_type: 'gpt',
        size: 0,
        format: 'GGUF',
        quantization: 'Q4_K_M'
      },
      {
        id: 'TheBloke/Llama-2-7B-Chat-GGUF',
        name: 'Llama-2-7B-Chat-GGUF',
        description: 'Meta Llama 2 7B Chat model in GGUF format',
        downloads: 1000000,
        likes: 500,
        tags: ['gguf', 'llama2', 'chat'],
        model_type: 'llama',
        size: 0,
        format: 'GGUF',
        quantization: 'Q4_K_M'
      }
    )
    
    return fallbackModels
  }

  // ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ç¢ºèª
  private async verifyDownloadStructure(modelId: string, targetPath: string): Promise<void> {
    try {
      console.log(`ğŸ” Verifying download structure for: ${modelId}`)
      
      // Electronç’°å¢ƒã§ã¯å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª
      if (typeof window !== 'undefined' && (window as any).electronAPI) {
        const result = await (window as any).electronAPI.verifyFile({ path: targetPath })
        if (result.exists) {
          console.log(`âœ… Download verified: ${targetPath}`)
        } else {
          console.warn(`âš ï¸ Download verification failed: ${targetPath}`)
        }
      } else {
        // ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã§ã¯æ§‹é€ ã®ææ¡ˆã®ã¿
        const modelDir = this.generateModelDirectory(modelId)
        console.log(`ğŸ“ Expected directory structure: ${modelDir}`)
        console.log(`ğŸ“‚ Expected file path: ${targetPath}`)
      }
    } catch (error) {
      console.warn(`âš ï¸ Download verification failed:`, error)
    }
  }

  private generateModelDirectory(modelId: string): string {
    const modelName = modelId.split('/').pop() || modelId
    return `./models/${modelName}`
  }
}
