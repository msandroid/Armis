import { LlamaService } from './llama-service'
import { OllamaService } from './ollama-service'
import { LlamaCppService } from './llama-cpp-service'
import { HuggingFaceModelDownloader, HuggingFaceModel, DownloadProgress } from './huggingface-model-downloader'
import { SequentialThinkingAgent } from '@/services/agent/sequential-thinking-agent'
import { VectorDatabaseService } from '@/services/vector/vector-database'
import { ArmisTools } from '@/services/tools/armis-tools'
import { GeminiFileService } from './gemini-file-service'
import { GeminiFileTools } from '@/services/tools/gemini-file-tools'
import { GeminiImageService } from './gemini-image-service'
import { LLMConfig, RouterAgentConfig, AgentResponse } from '@/types/llm'
import { AVAILABLE_PROVIDERS } from '@/types/ai-sdk'
import { RouterAgent } from '@/services/agent/router-agent'
import { LangChainRouterAgent } from '@/services/agent/langchain-router-agent'
import {
  GeneralAgent,
  CodeAssistantAgent,
  FileProcessorAgent,
  DataAnalyzerAgent,
  CreativeWriterAgent,
  SequentialThinkingAgentWrapper
} from '@/services/agent/specialized-agents-fixed'
import { ImageGenerationAgent } from '@/services/agent/image-generation-agent'

export class LLMManager {
  private llamaService: LlamaService
  private ollamaService: OllamaService
  private llamaCppService: LlamaCppService
  private huggingFaceDownloader: HuggingFaceModelDownloader
  private vectorDB: VectorDatabaseService
  private agent: SequentialThinkingAgent
  private tools: ArmisTools
  private geminiFileService: GeminiFileService
  private geminiFileTools: GeminiFileTools
  private geminiImageService: GeminiImageService
  private routerAgent: RouterAgent
  private isInitialized = false
  private isInitializing = false // åˆæœŸåŒ–ä¸­ã®é‡è¤‡ã‚’é˜²ããƒ•ãƒ©ã‚°
  private useOllama: boolean = true // Default to Ollama
  private useLlamaCpp: boolean = false // New flag for LlamaCpp

  constructor(config: LLMConfig) {
    this.llamaService = new LlamaService(config)
    this.ollamaService = new OllamaService({
      defaultModel: 'gemma3:1b',
      baseUrl: 'http://localhost:11434'
    })
    this.llamaCppService = new LlamaCppService({
      modelPath: config.modelPath || './models/gpt-oss-20b-GGUF.gguf',
      temperature: config.temperature,
      maxTokens: config.contextSize,
      topP: config.topP,
      topK: config.topK,
      contextSize: config.contextSize,
      threads: 4,
      gpuLayers: 0,
      verbose: false
    })
    this.huggingFaceDownloader = new HuggingFaceModelDownloader()
    this.vectorDB = new VectorDatabaseService()
    this.agent = new SequentialThinkingAgent(this.llamaService)
    this.tools = new ArmisTools(this.vectorDB)
    this.geminiFileService = new GeminiFileService()
    this.geminiFileTools = new GeminiFileTools(this.geminiFileService)
    this.geminiImageService = new GeminiImageService()
    
    // Initialize Router Agent with default configuration
    const routerConfig: RouterAgentConfig = {
      defaultAgent: 'general',
      confidenceThreshold: 0.7,
      enableFallback: true,
      maxRetries: 3
    }
    this.routerAgent = new RouterAgent(this.llamaService, routerConfig, this.vectorDB)
  }

  async initialize(): Promise<void> {
    // æ—¢ã«åˆæœŸåŒ–ä¸­ã¾ãŸã¯åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
    if (this.isInitializing || this.isInitialized) {
      console.log('ğŸ”„ LLM Manager is already initializing or initialized')
      return
    }

    this.isInitializing = true
    
    try {
      console.log('ğŸš€ Initializing LLM Manager...')
      
      // æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
      try {
        await this.migrateExistingModels()
      } catch (error) {
        console.warn('âš ï¸ Model migration failed, continuing with initialization:', error)
      }
      
      // Initialize vector database
      await this.vectorDB.initialize()
      
      // Initialize LLM service based on preference
      if (this.useLlamaCpp) {
        try {
          console.log('ğŸ”„ Attempting to initialize LlamaCpp service...')
          await this.llamaCppService.initialize()
          console.log('âœ… LlamaCpp service initialized successfully')
        } catch (error) {
          console.warn('âš ï¸ LlamaCpp initialization failed, falling back to Ollama:', error)
          this.useLlamaCpp = false
          this.useOllama = true
          try {
            console.log('ğŸ”„ Attempting to initialize Ollama service...')
            await this.ollamaService.initialize()
            console.log('âœ… Ollama service initialized successfully')
          } catch (ollamaError) {
            console.warn('âš ï¸ Ollama initialization failed, falling back to Llama service:', ollamaError)
            await this.llamaService.initialize()
            this.useOllama = false
          }
        }
      } else if (this.useOllama) {
        try {
          console.log('ğŸ”„ Attempting to initialize Ollama service...')
          await this.ollamaService.initialize()
          console.log('âœ… Ollama service initialized successfully')
        } catch (error) {
          console.warn('âš ï¸ Ollama initialization failed, falling back to Llama service:', error)
          await this.llamaService.initialize()
          this.useOllama = false
        }
      } else {
        await this.llamaService.initialize()
      }
      
      // Register tools with agent
      const allTools = this.tools.getAllTools()
      const geminiFileTools = this.geminiFileTools.getAllTools()
      allTools.forEach(tool => this.agent.registerTool(tool))
      geminiFileTools.forEach(tool => this.agent.registerTool(tool))
      
      // Initialize and register specialized agents with router
      await this.initializeRouterAgents()
      
      this.isInitialized = true
      console.log('âœ… LLM Manager initialized successfully')
      
      // Log current configuration
      const stats = this.getSystemStats()
      console.log(`ğŸ“Š Current configuration: ${stats.llmService}${stats.ollamaModel ? ` (${stats.ollamaModel})` : ''}`)
    } catch (error) {
      console.error('âŒ Failed to initialize LLM Manager:', error)
      throw error
    } finally {
      this.isInitializing = false
    }
  }

  private async initializeRouterAgents(): Promise<void> {
    console.log('Initializing Router Agents...')
    
    // Get the appropriate LLM service
    const llmService = this.useOllama ? this.ollamaService : this.llamaService
    
    // Create specialized agents
    const generalAgent = new GeneralAgent(llmService)
    const codeAssistantAgent = new CodeAssistantAgent(llmService)
    const fileProcessorAgent = new FileProcessorAgent(llmService)
    const dataAnalyzerAgent = new DataAnalyzerAgent(llmService)
    const creativeWriterAgent = new CreativeWriterAgent(llmService)
    const sequentialThinkingWrapper = new SequentialThinkingAgentWrapper(llmService)
    const imageGenerationAgent = new ImageGenerationAgent(llmService)
    
    // Register all agents with router
    this.routerAgent.registerAgent(generalAgent)
    this.routerAgent.registerAgent(codeAssistantAgent)
    this.routerAgent.registerAgent(fileProcessorAgent)
    this.routerAgent.registerAgent(dataAnalyzerAgent)
    this.routerAgent.registerAgent(creativeWriterAgent)
    this.routerAgent.registerAgent(sequentialThinkingWrapper)
    this.routerAgent.registerAgent(imageGenerationAgent)
    
    console.log('Router Agents initialized successfully')
  }

  async processUserRequest(userInput: string, context?: Record<string, any>): Promise<any> {
    if (!this.isInitialized) {
      throw new Error('LLM Manager not initialized. Call initialize() first.')
    }

    try {
      // Use router agent to determine and execute the best agent
      return await this.routerAgent.routeAndExecute(userInput, context)
    } catch (error) {
      console.error('Error processing user request:', error)
      throw error
    }
  }

  // Legacy method for backward compatibility
  async processUserRequestLegacy(userInput: string, context?: Record<string, any>): Promise<any> {
    if (!this.isInitialized) {
      throw new Error('LLM Manager not initialized. Call initialize() first.')
    }

    try {
      // ç›´æ¥LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
      const llmService = this.getLLMService()
      
      if (this.useOllama) {
        // OllamaServiceã®å ´åˆ
        const response = await (llmService as OllamaService).generate(userInput)
        console.log('Ollama response:', response)
        console.log('Ollama response type:', typeof response)
        console.log('Ollama response keys:', response ? Object.keys(response) : 'null/undefined')
        
        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£å¸¸ã‹ãƒã‚§ãƒƒã‚¯
        if (response && response.response) {
          const responseText = response.response
          // ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
          const cleanResponse = responseText && 
            responseText.trim() !== '' && 
            responseText !== '{}' && 
            responseText !== '[]' && 
            responseText !== 'null' && 
            responseText !== 'undefined' &&
            !/^\s*\{\s*\}\s*$/.test(responseText) &&
            !/^\s*\[\s*\]\s*$/.test(responseText) 
            ? responseText 
            : ''
          
          return {
            content: cleanResponse,
            response: cleanResponse,
            success: true
          }
        } else if (response && typeof response === 'object' && 'response' in response) {
          // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã‚’ç¢ºèª
          const responseText = response.response || ''
          // ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
          const cleanResponse = responseText && 
            responseText.trim() !== '' && 
            responseText !== '{}' && 
            responseText !== '[]' && 
            responseText !== 'null' && 
            responseText !== 'undefined' &&
            !/^\s*\{\s*\}\s*$/.test(responseText) &&
            !/^\s*\[\s*\]\s*$/.test(responseText) 
            ? responseText 
            : ''
          
          return {
            content: cleanResponse,
            response: cleanResponse,
            success: true
          }
        } else {
          console.error('Invalid Ollama response structure:', response)
          throw new Error('Invalid response from Ollama service')
        }
      } else {
        // LlamaServiceã®å ´åˆ
        const response = await (llmService as LlamaService).generateResponse(userInput)
        return {
          content: response.text,
          response: response.text,
          success: true
        }
      }
    } catch (error) {
      console.error('Error processing user request:', error)
      throw error
    }
  }

  async addUserPreference(userId: string, action: string, context: Record<string, any>, result: any): Promise<void> {
    if (!this.isInitialized) {
      throw new Error('LLM Manager not initialized.')
    }

    await this.vectorDB.addUserPreference(userId, action, context, result)
  }

  async findSimilarWorkflows(userId: string, context: Record<string, any>): Promise<any[]> {
    if (!this.isInitialized) {
      throw new Error('LLM Manager not initialized.')
    }

    return await this.vectorDB.findSimilarWorkflows(userId, context)
  }

  isReady(): boolean {
    return this.isInitialized
  }

  getLLMService(): LlamaService | OllamaService {
    return this.useOllama ? this.ollamaService : this.llamaService
  }

  getOllamaService(): OllamaService {
    return this.ollamaService
  }

  getLlamaService(): LlamaService {
    return this.llamaService
  }

  getLlamaCppService(): LlamaCppService {
    return this.llamaCppService
  }

  getAgent(): SequentialThinkingAgent {
    return this.agent
  }

  getRouterAgent(): RouterAgent {
    return this.routerAgent
  }

  getVectorDB(): VectorDatabaseService {
    return this.vectorDB
  }

  getGeminiFileService(): GeminiFileService {
    return this.geminiFileService
  }

  getGeminiFileTools(): GeminiFileTools {
    return this.geminiFileTools
  }

  // Router Agent specific methods
  async routeAndExecute(userInput: string, context?: Record<string, any>): Promise<AgentResponse> {
    if (!this.isInitialized) {
      throw new Error('LLM Manager not initialized.')
    }

    return await this.routerAgent.routeAndExecute(userInput, context)
  }

  updateRouterConfig(newConfig: Partial<RouterAgentConfig>): void {
    this.routerAgent.updateConfig(newConfig)
  }

  getRouterConfig(): RouterAgentConfig {
    return this.routerAgent.getConfig()
  }

  getAvailableAgents(): any[] {
    return this.routerAgent.getAllAgents()
  }

  getRoutingHistory(): any[] {
    return this.routerAgent.getRoutingHistory()
  }

  // æ–°ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰: é«˜åº¦ãªã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
  getMultiAgentMemory(): any {
    return this.routerAgent.getMultiAgentMemory()
  }

  getAdvancedRAG(): any {
    return this.routerAgent.getAdvancedRAG()
  }

  // ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã®å–å¾—
  getMemoryStats(): any {
    return this.routerAgent.getMemoryStats()
  }

  // RAGçµ±è¨ˆã®å–å¾—
  getRAGStats(): any {
    return this.routerAgent.getRAGStats()
  }

  // ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
  async addToSharedMemory(content: string, sourceAgent: string, context?: Record<string, any>, tags?: string[]): Promise<string> {
    const memory = this.routerAgent.getMultiAgentMemory()
    return await memory.addToSharedMemory(content, sourceAgent, context || {}, tags || [])
  }

  async searchSharedMemory(query: string, limit?: number, minImportance?: number): Promise<any[]> {
    const memory = this.routerAgent.getMultiAgentMemory()
    return await memory.searchSharedMemory(query, limit || 10, minImportance || 0.3)
  }

  async addToLongTermMemory(content: string, sourceAgent: string, context?: Record<string, any>, tags?: string[]): Promise<string> {
    const memory = this.routerAgent.getMultiAgentMemory()
    return await memory.addToLongTermMemory(content, sourceAgent, context || {}, tags || [])
  }

  // RAGç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
  async hybridSearch(query: string, context?: Record<string, any>): Promise<any[]> {
    const rag = this.routerAgent.getAdvancedRAG()
    return await rag.hybridSearch(query, context)
  }

  async contextualSearch(query: string, context: Record<string, any>): Promise<any[]> {
    const rag = this.routerAgent.getAdvancedRAG()
    return await rag.contextualSearch(query, context)
  }

  async multiSourceIntegration(sources: any[]): Promise<any> {
    const rag = this.routerAgent.getAdvancedRAG()
    return await rag.multiSourceIntegration(sources)
  }

  // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
  updateContext(newContext: Record<string, any>, trigger?: string): void {
    const memory = this.routerAgent.getMultiAgentMemory()
    memory.updateContext(newContext, trigger || 'manual')
  }

  getCurrentContext(): Record<string, any> {
    const memory = this.routerAgent.getMultiAgentMemory()
    return memory.getCurrentContext()
  }

  getContextHistory(limit?: number): any[] {
    const memory = this.routerAgent.getMultiAgentMemory()
    return memory.getContextHistory(limit || 10)
  }

  // ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã®å–å¾—
  getSystemStats(): {
    memory: any
    rag: any
    agents: any[]
    routingHistory: any[]
    llmService: string
    ollamaModel?: string
    llamaCppModel?: string
  } {
    return {
      memory: this.getMemoryStats(),
      rag: this.getRAGStats(),
      agents: this.getAvailableAgents(),
      routingHistory: this.getRoutingHistory(),
      llmService: this.useLlamaCpp ? 'LlamaCpp' : this.useOllama ? 'Ollama' : 'Llama',
      ollamaModel: this.useOllama ? this.ollamaService.getDefaultModel() : undefined,
      llamaCppModel: this.useLlamaCpp ? this.llamaCppService.getModelInfo()?.name : undefined
    }
  }

  // Ollamaå›ºæœ‰ã®ãƒ¡ã‚½ãƒƒãƒ‰
  async switchToOllama(): Promise<void> {
    if (!this.useOllama) {
      await this.ollamaService.initialize()
      this.useOllama = true
      console.log('Switched to Ollama service')
    }
  }

  async switchToLlama(): Promise<void> {
    if (this.useOllama) {
      await this.llamaService.initialize()
      this.useOllama = false
      console.log('Switched to Llama service')
    }
  }

  async setOllamaModel(modelName: string): Promise<void> {
    if (this.useOllama) {
      await this.ollamaService.setDefaultModel(modelName)
    } else {
      throw new Error('Ollama is not currently active. Call switchToOllama() first.')
    }
  }

  async listOllamaModels(): Promise<any[]> {
    return await this.ollamaService.listModels()
  }

  async pullOllamaModel(modelName: string): Promise<void> {
    await this.ollamaService.pullModel(modelName)
  }

  isUsingOllama(): boolean {
    return this.useOllama
  }

  // LlamaCppå›ºæœ‰ã®ãƒ¡ã‚½ãƒƒãƒ‰
  async switchToLlamaCpp(): Promise<void> {
    if (!this.useLlamaCpp) {
      await this.llamaCppService.initialize()
      this.useLlamaCpp = true
      this.useOllama = false
      console.log('Switched to LlamaCpp service')
    }
  }

  async loadLlamaCppModel(modelPath: string): Promise<void> {
    if (this.useLlamaCpp) {
      await this.llamaCppService.loadModel(modelPath)
    } else {
      throw new Error('LlamaCpp is not currently active. Call switchToLlamaCpp() first.')
    }
  }

  async getAvailableLlamaCppModels(): Promise<any[]> {
    return await this.llamaCppService.getAvailableModels()
  }

  async updateLlamaCppConfig(config: any): Promise<void> {
    if (this.useLlamaCpp) {
      this.llamaCppService.updateConfig(config)
    } else {
      throw new Error('LlamaCpp is not currently active. Call switchToLlamaCpp() first.')
    }
  }

  isUsingLlamaCpp(): boolean {
    return this.useLlamaCpp
  }

  // Hugging Faceãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰
  async searchHuggingFaceModels(query: string = '', limit: number = 50): Promise<HuggingFaceModel[]> {
    return await this.huggingFaceDownloader.searchLlamaCppModels(query, limit)
  }

  async getHuggingFaceModelDetails(modelId: string): Promise<HuggingFaceModel | null> {
    return await this.huggingFaceDownloader.getModelDetails(modelId)
  }

  async downloadHuggingFaceModel(
    modelId: string,
    onProgress?: (progress: DownloadProgress) => void,
    onComplete?: (modelPath: string) => void,
    onError?: (error: string) => void
  ): Promise<string> {
    return await this.huggingFaceDownloader.downloadModel({
      modelId,
      onProgress,
      onComplete,
      onError
    })
  }

  async cancelHuggingFaceDownload(modelId: string): Promise<void> {
    await this.huggingFaceDownloader.cancelDownload(modelId)
  }

  isHuggingFaceDownloading(modelId: string): boolean {
    return this.huggingFaceDownloader.isDownloading(modelId)
  }

  getHuggingFaceDownloadingModels(): string[] {
    return this.huggingFaceDownloader.getDownloadingModels()
  }

  // ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—
  getActiveService(): 'LlamaCpp' | 'Ollama' | 'Llama' {
    if (this.useLlamaCpp) return 'LlamaCpp'
    if (this.useOllama) return 'Ollama'
    return 'Llama'
  }

  // ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚µãƒ¼ãƒ“ã‚¹ã§å¿œç­”ç”Ÿæˆ
  async generateResponse(prompt: string): Promise<any> {
    if (this.useLlamaCpp) {
      return await this.llamaCppService.generateResponse(prompt)
    } else if (this.useOllama) {
      return await this.ollamaService.generate(prompt)
    } else {
      return await this.llamaService.generateResponse(prompt)
    }
  }

  // Gemini Image Serviceé–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰
  async configureGeminiImageService(apiKey: string, projectId?: string, location?: string): Promise<void> {
    try {
      await this.geminiImageService.configure(apiKey, projectId, location)
      console.log('âœ… Gemini Image Service configured successfully')
    } catch (error) {
      console.error('âŒ Failed to configure Gemini Image Service:', error)
      throw error
    }
  }

  async generateImage(request: any): Promise<any> {
    if (!this.geminiImageService) {
      throw new Error('Gemini Image Service not configured. Call configureGeminiImageService() first.')
    }
    return await this.geminiImageService.generateImage(request)
  }

  async generateMultipleImages(request: any, count: number = 1): Promise<any> {
    if (!this.geminiImageService) {
      throw new Error('Gemini Image Service not configured. Call configureGeminiImageService() first.')
    }
    return await this.geminiImageService.generateMultipleImages(request, count)
  }

  getAvailableImageModels(): Record<string, any> {
    if (!this.geminiImageService) {
      throw new Error('Gemini Image Service not configured.')
    }
    return this.geminiImageService.getAvailableModels()
  }

  getImageModelInfo(modelName: string): any {
    if (!this.geminiImageService) {
      throw new Error('Gemini Image Service not configured.')
    }
    return this.geminiImageService.getModelInfo(modelName)
  }

  // ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’å–å¾—
  getModelProviderInfo(modelId: string): { providerId: string; modelId: string } {
    console.log(`ğŸ” Getting provider info for model: ${modelId}`)
    
    // AVAILABLE_PROVIDERSã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    for (const provider of AVAILABLE_PROVIDERS) {
      const model = provider.models.find(m => m.id === modelId)
      if (model) {
        console.log(`âœ… Found model ${modelId} in provider ${provider.id}`)
        return { providerId: provider.id, modelId: model.id }
      }
    }
    
    console.log(`âŒ Model ${modelId} not found in direct search, checking mappings...`)
    
    // ç‰¹æ®Šãªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆqwen3ã‚·ãƒªãƒ¼ã‚ºï¼‰
    const modelMappings: { [key: string]: string } = {
      // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å‰Šé™¤ã—ã€æ˜ç¤ºçš„ãªãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿æ®‹ã™
      'qwen3-0.6b': 'qwen3:0.6b',
      'qwen3-1.7b': 'qwen3:1.7b',
      'qwen3-4b': 'qwen3:4b',
      'qwen3-8b': 'qwen3:8b',
      'qwen3-14b': 'qwen3:14b',
      'qwen3-30b': 'qwen3:30b',
      'qwen3-32b': 'qwen3:32b',
      // Llama.cppç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°
      'qwen3-0.6b-gguf': 'qwen3-0.6b-gguf',
      'qwen3-1.7b-gguf': 'qwen3-1.7b-gguf',
      'qwen3-4b-gguf': 'qwen3-4b-gguf',
      'qwen3-8b-gguf': 'qwen3-8b-gguf',
      'qwen3-30b-gguf': 'qwen3-30b-gguf',
      // Hugging Faceç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°
      'qwen3-0.6b-hf': 'qwen3-0.6b-hf',
      'qwen3-1.7b-hf': 'qwen3-1.7b-hf',
      'qwen3-4b-hf': 'qwen3-4b-hf',
      'qwen3-8b-hf': 'qwen3-8b-hf',
      'qwen3-30b-hf': 'qwen3-30b-hf'
    }
    
    const mappedModelId = modelMappings[modelId]
    if (mappedModelId) {
      console.log(`ğŸ” Found mapping: ${modelId} â†’ ${mappedModelId}`)
      // ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«IDã§å†æ¤œç´¢
      for (const provider of AVAILABLE_PROVIDERS) {
        const model = provider.models.find(m => m.id === mappedModelId)
        if (model) {
          console.log(`âœ… Found mapped model ${mappedModelId} in provider ${provider.id}`)
          return { providerId: provider.id, modelId: mappedModelId }
        }
      }
      console.log(`âŒ Mapped model ${mappedModelId} not found in any provider`)
    } else {
      console.log(`âŒ No mapping found for model ${modelId}`)
    }
    
    // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šåˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
    console.log(`ğŸ” Available providers:`, AVAILABLE_PROVIDERS.map(p => ({ id: p.id, name: p.name, modelCount: p.models.length })))
    console.log(`ğŸ” Ollama models:`, AVAILABLE_PROVIDERS.find(p => p.id === 'ollama')?.models.map(m => m.id) || [])
    
    // ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§llama-cppã¨ã—ã¦æ‰±ã†
    console.warn(`Model ${modelId} not found in AVAILABLE_PROVIDERS, defaulting to llama-cpp`)
    return { providerId: 'llama-cpp', modelId }
  }



  // ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
  async ensureModelAvailable(modelId: string, onProgress?: (progress: any) => void): Promise<boolean> {
    try {
      console.log(`ğŸ” Checking if model ${modelId} is available...`)
      
      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’å–å¾—
      const { providerId, modelId: mappedModelId } = this.getModelProviderInfo(modelId)
      console.log(`ğŸ” Model ${modelId} is mapped to provider: ${providerId}`)
      console.log(`ğŸ” Mapped model ID: ${mappedModelId}`)
      
      // Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆ
      if (providerId === 'ollama') {
        // æ—¢ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«IDã‚’ä½¿ç”¨
        const actualModelId = mappedModelId
        const availableModels = await this.listOllamaModels()
        console.log(`ğŸ” Available Ollama models:`, availableModels.map((m: any) => m.name))
        console.log(`ğŸ” Looking for model: ${actualModelId}`)
        
        const modelExists = availableModels.some((model: any) => 
          model.name === actualModelId || model.name.includes(actualModelId)
        )
        
        console.log(`ğŸ” Model exists: ${modelExists}`)
        
        if (!modelExists) {
          console.log(`ğŸ“¥ Model ${actualModelId} not found in Ollama, downloading...`)
          console.log(`ğŸ” Original model ID: ${modelId}`)
          console.log(`ğŸ” Mapped model ID: ${actualModelId}`)
          await this.pullOllamaModelWithProgress(actualModelId, onProgress)
          return true
        }
        
        console.log(`âœ… Model ${modelId} is already available in Ollama`)
        return true
      }
      
      // LlamaCppãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆ
      if (providerId === 'llama-cpp') {
        console.log(`ğŸ” Checking LlamaCpp for model: ${modelId}`)
        
        // ã¾ãšã€ãƒ­ãƒ¼ã‚«ãƒ«ã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        const availableModels = await this.getAvailableLlamaCppModels()
        const localModel = availableModels.find((model: any) => {
          const modelNameLower = model.name.toLowerCase()
          const modelPathLower = model.path.toLowerCase()
          const searchIdLower = modelId.toLowerCase()
          
          // ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã€éƒ¨åˆ†ä¸€è‡´ã‚’å¼·åŒ–ï¼‰
          return modelNameLower.includes(searchIdLower) || 
                 modelPathLower.includes(searchIdLower) ||
                 modelNameLower.includes(searchIdLower.replace(':', '')) ||
                 modelPathLower.includes(searchIdLower.replace(':', ''))
        })
        
        if (localModel) {
          console.log(`âœ… Model ${modelId} found locally: ${localModel.path}`)
          return true
        }
        
        // ãƒ­ãƒ¼ã‚«ãƒ«ã«ãªã„å ´åˆã¯Hugging Faceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        console.log(`ğŸ“¥ Model ${modelId} not found locally, searching on Hugging Face...`)
        
        // ãƒ¢ãƒ‡ãƒ«åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆHugging Faceç”¨ï¼‰
        const modelNameMapping: { [key: string]: string } = {
          'gpt-oss-20b-gguf': 'unsloth/gpt-oss-20b-GGUF',
          'gpt-oss-20b-GGUF': 'unsloth/gpt-oss-20b-GGUF',
          'gpt-oss-20b': 'unsloth/gpt-oss-20b-GGUF',
          'llama-2-7b-chat': 'TheBloke/Llama-2-7B-Chat-GGUF',
          'llama-2-13b-chat': 'TheBloke/Llama-2-13B-Chat-GGUF',
          'mistral-7b-instruct': 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF',
          'qwen2-7b-instruct': 'TheBloke/Qwen2-7B-Instruct-GGUF',
          // Qwen2.5-Omni ãƒãƒƒãƒ”ãƒ³ã‚°
          'qwen2.5-omni-3b-gguf': 'unsloth/Qwen2.5-Omni-3B-GGUF',
          'qwen2.5-omni-3b': 'unsloth/Qwen2.5-Omni-3B-GGUF',
          'qwen2.5-omni-7b-gguf': 'unsloth/Qwen2.5-Omni-7B-GGUF',
          'qwen2.5-omni-7b': 'unsloth/Qwen2.5-Omni-7B-GGUF',
          // Qwen3 ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆLlama.cppç”¨ï¼‰
          'qwen3': 'unsloth/Qwen3-4B-Instruct-2507-GGUF',
          'qwen3:0.6b': 'unsloth/Qwen3-0.6B-GGUF',
          'qwen3-0.6b': 'unsloth/Qwen3-0.6B-GGUF',
          'qwen3-0.6b-gguf': 'unsloth/Qwen3-0.6B-GGUF',
          'qwen3-1.7b': 'unsloth/Qwen3-1.7B-GGUF',
          'qwen3-1.7b-gguf': 'unsloth/Qwen3-1.7B-GGUF',
          'qwen3-4b': 'unsloth/Qwen3-4B-Instruct-2507-GGUF',
          'qwen3-4b-gguf': 'unsloth/Qwen3-4B-Instruct-2507-GGUF',
          'qwen3-4b-instruct': 'unsloth/Qwen3-4B-Instruct-2507-GGUF',
          'qwen3-8b': 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF',
          'qwen3-8b-gguf': 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF',
          'qwen3-8b-instruct': 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF',
          'qwen3-30b': 'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF',
          'qwen3-30b-gguf': 'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF',
          'qwen3-30b-instruct': 'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF',
          'gemma-2-9b-it': 'TheBloke/Gemma-2-9B-IT-GGUF',
          'codellama-7b-instruct': 'TheBloke/CodeLlama-7B-Instruct-GGUF'
        }
        
        const searchQuery = modelNameMapping[modelId] || modelId
        console.log(`ğŸ” Searching for model: ${searchQuery} on Hugging Face...`)
        
        // Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        const models = await this.searchHuggingFaceModels(searchQuery, 5)
        if (models.length > 0) {
          const bestModel = models[0] // æœ€ã‚‚äººæ°—ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
          console.log(`ğŸ“¥ Found model: ${bestModel.id}, downloading...`)
          await this.downloadHuggingFaceModel(
            bestModel.id,
            (progress) => {
              console.log(`ğŸ“Š Hugging Face download progress:`, progress)
              if (onProgress) {
                // å®Ÿéš›ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã«è¿½åŠ 
                const enhancedProgress = {
                  ...progress,
                  actualModelId: bestModel.id,
                  originalModelId: modelId
                }
                onProgress(enhancedProgress)
              }
            },
            (modelPath) => console.log(`âœ… Model downloaded: ${modelPath}`),
            (error) => console.error(`âŒ Download failed: ${error}`)
          )
          return true
        } else {
          throw new Error(`No models found for ${modelId} on Hugging Face`)
        }
      }
      
      // ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆHugging Faceã€OpenAIç­‰ï¼‰ã®å ´åˆã¯æ—¢ã«åˆ©ç”¨å¯èƒ½ã¨ã¿ãªã™
      console.log(`âœ… Model ${modelId} is available for provider ${providerId}`)
      return true
    } catch (error) {
      console.error(`âŒ Failed to ensure model availability: ${error}`)
      throw error
    }
  }

  // Ollamaãƒ¢ãƒ‡ãƒ«ã®é€²æ—ä»˜ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  async pullOllamaModelWithProgress(modelName: string, onProgress?: (progress: any) => void): Promise<void> {
    try {
      console.log(`ğŸš€ ===== LLM MANAGER: MODEL DOWNLOAD STARTED =====`)
      console.log(`ğŸ“¥ Model: ${modelName}`)
      console.log(`â° Start time: ${new Date().toLocaleString()}`)
      console.log(`ğŸ”§ Service: Ollama`)
      console.log(`================================================`)
      
      await this.ollamaService.pullModelWithProgress(modelName, (progressData) => {
        if (onProgress) {
          onProgress(progressData)
        }
        // è©³ç´°ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
        console.log(`ğŸ“Š [LLM Manager] ${progressData.message}`)
      }).catch(async (error) => {
        console.error(`ğŸ’¥ LLM Manager: Download failed for ${modelName}:`, error)
        
        // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ”¹å–„
        const errorMessage = error instanceof Error 
          ? error.message 
          : 'Unknown download error'
        
        if (onProgress) {
          onProgress({ 
            status: 'error', 
            message: `Download failed: ${errorMessage}`,
            progress: 0
          })
        }
        
        throw error
      })
      
      if (onProgress) {
        onProgress({ status: 'completed', message: `Download completed: ${modelName}` })
      }
      
      console.log(`ğŸ‰ ===== LLM MANAGER: DOWNLOAD COMPLETED =====`)
      console.log(`âœ… Ollama model downloaded successfully: ${modelName}`)
      console.log(`â° End time: ${new Date().toLocaleString()}`)
      console.log(`==============================================`)
    } catch (error) {
      console.error(`ğŸ’¥ ===== LLM MANAGER: DOWNLOAD FAILED =====`)
      console.error(`âŒ Failed to download Ollama model: ${error}`)
      console.error(`ğŸ“¥ Model: ${modelName}`)
      console.error(`â° Time: ${new Date().toLocaleString()}`)
      console.error(`============================================`)
      if (onProgress) {
        onProgress({ status: 'error', message: `Download failed: ${error}` })
      }
      throw error
    }
  }

  // ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  async switchToModel(modelId: string, onProgress?: (progress: any) => void): Promise<void> {
    try {
      console.log(`ğŸ”„ Switching to model: ${modelId}`)
      
      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’å–å¾—
      const { providerId, modelId: mappedModelId } = this.getModelProviderInfo(modelId)
      
      // ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
      await this.ensureModelAvailable(modelId, onProgress)
      
      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆ
      if (providerId === 'ollama') {
        // Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆ
        this.useOllama = true
        this.useLlamaCpp = false
        await this.ollamaService.setModel(mappedModelId)
        console.log(`âœ… Switched to Ollama model: ${mappedModelId}`)
      } else if (providerId === 'llama-cpp') {
        // LlamaCppãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å ´åˆ
        this.useOllama = false
        this.useLlamaCpp = true
        
        // LlamaCppã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        const availableModels = await this.getAvailableLlamaCppModels()
        
        // ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ¢ãƒ‡ãƒ«æ¤œç´¢ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã€éƒ¨åˆ†ä¸€è‡´ã‚’å¼·åŒ–ï¼‰
        const targetModel = availableModels.find((model: any) => {
          const modelNameLower = model.name.toLowerCase()
          const modelPathLower = model.path.toLowerCase()
          const searchIdLower = modelId.toLowerCase()
          
          // å®Œå…¨ä¸€è‡´ã€éƒ¨åˆ†ä¸€è‡´ã€ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
          return modelNameLower.includes(searchIdLower) || 
                 modelPathLower.includes(searchIdLower) ||
                 modelNameLower.includes(searchIdLower.replace(':', '')) ||
                 modelPathLower.includes(searchIdLower.replace(':', ''))
        })
        
        if (targetModel) {
          await this.loadLlamaCppModel(targetModel.path)
          console.log(`âœ… Switched to LlamaCpp model: ${targetModel.path}`)
        } else {
          console.warn(`âš ï¸ Model ${modelId} not found in available LlamaCpp models`)
          console.log(`ğŸ” Available models:`, availableModels.map(m => ({ name: m.name, path: m.path })))
        }
      } else {
        // ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆHugging Faceã€OpenAIç­‰ï¼‰ã®å ´åˆã¯ç¾åœ¨ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¶­æŒ
        console.log(`âœ… Using model ${modelId} with provider ${providerId}`)
      }
      
      console.log(`âœ… Successfully switched to model: ${modelId}`)
    } catch (error) {
      console.error(`âŒ Failed to switch to model: ${error}`)
      throw error
    }
  }

  // æ—¢å­˜ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«ç§»å‹•
  async migrateExistingModels(): Promise<void> {
    try {
      console.log('ğŸ”„ Migrating existing models to new directory structure...')
      
      const availableModels = await this.getAvailableLlamaCppModels()
      
      for (const model of availableModels) {
        const fileName = model.path.split('/').pop() || model.path.split('\\').pop() || ''
        
        // Qwen3ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
        if (fileName.toLowerCase().includes('qwen3')) {
          const modelId = this.detectModelIdFromFileName(fileName)
          if (modelId) {
            const newPath = `./models/${modelId}/${fileName}`
            console.log(`ğŸ“ Migrating ${fileName} to ${newPath}`)
            
            // å®Ÿéš›ã®ç§»å‹•å‡¦ç†ã¯Electronç’°å¢ƒã§ã®ã¿å®Ÿè¡Œ
            if (typeof window !== 'undefined' && (window as any).electronAPI) {
              try {
                await (window as any).electronAPI.moveFile({
                  oldPath: model.path,
                  newPath: newPath
                })
                console.log(`âœ… Successfully migrated ${fileName}`)
              } catch (error) {
                console.warn(`âš ï¸ Failed to migrate ${fileName}:`, error)
              }
            }
          }
        }
      }
      
      console.log('âœ… Model migration completed')
    } catch (error) {
      console.error('âŒ Model migration failed:', error)
    }
  }

  // ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’æ¤œå‡º
  private detectModelIdFromFileName(fileName: string): string | null {
    const fileNameLower = fileName.toLowerCase()
    
    if (fileNameLower.includes('qwen3-0.6b')) {
      return 'unsloth/Qwen3-0.6B-GGUF'
    } else if (fileNameLower.includes('qwen3-1.7b')) {
      return 'unsloth/Qwen3-1.7B-GGUF'
    } else if (fileNameLower.includes('qwen3-4b')) {
      return 'unsloth/Qwen3-4B-Instruct-2507-GGUF'
    } else if (fileNameLower.includes('qwen3-8b')) {
      return 'unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF'
    } else if (fileNameLower.includes('qwen3-30b')) {
      return 'unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF'
    }
    
    return null
  }
}
